---
title: "**Nextflow Development - Metadata Proprogation**"
output:
  html_document:
    toc: false
    toc_float: false
from: markdown+emoji
---

::: callout-tip
### Objectives{.unlisted}
- Gain an understanding of how to use nf-core modules in a workflow script
- Manipulate and proprogate sample metadata throughout the workflow
- Create a custom nf-core module
:::


## **6.1 Samplesheet parsing**

In the `./nf-core-customrnaseq/main.nf` script, the `PIPELINE_INITIALISATION` subworkflow created by default from the nf-core template will output a channel that contains the parsed `--input` samplesheet. This channel is then input into `NFCORE_CUSTOMRNASEQ`, which launches our analysis workflow containing the newly included modules `salmon/quant` and `salmon/quant`. 

```default
...

include { PIPELINE_INITIALISATION } from './subworkflows/local/utils_nfcore_customrnaseq_pipeline'

...

workflow {

    main:

    ...

    //
    // WORKFLOW: Run main workflow
    //
    NFCORE_CUSTOMRNASEQ (
        PIPELINE_INITIALISATION.out.samplesheet
    )

    ...

}
```

How does the `PIPELINE_INITIALISATION` parse the samplesheet?

::: {.callout-note collapse="true"}
### `./nf-core-customrnaseq/subworkflows/local/utils_nfcore_customrnaseq_pipeline/main.nf`
```default
//
// Create channel from input file provided through params.input
//
workflow PIPELINE_INITIALISATION {

    take:
    version           // boolean: Display version and exit
    validate_params   // boolean: Boolean whether to validate parameters against the schema at runtime
    monochrome_logs   // boolean: Do not use coloured log outputs
    nextflow_cli_args //   array: List of positional nextflow CLI args
    outdir            //  string: The output directory where the results will be saved
    input             //  string: Path to input samplesheet

    main:

    ...

    Channel
        .fromList(samplesheetToList(params.input, "${projectDir}/assets/schema_input.json"))
        .map {
            meta, fastq_1, fastq_2 ->
                if (!fastq_2) {
                    return [ meta.id, meta + [ single_end:true ], [ fastq_1 ] ]
                } else {
                    return [ meta.id, meta + [ single_end:false ], [ fastq_1, fastq_2 ] ]
                }
        }
        .groupTuple()
        .map { samplesheet ->
            validateInputSamplesheet(samplesheet)
        }
        .map {
            meta, fastqs ->
                return [ meta, fastqs.flatten() ]
        }
        .set { ch_samplesheet }

    emit:
    samplesheet = ch_samplesheet
    versions    = ch_versions
}

```
:::

The important pieces of information that we will explore further in this section are:

1. The use of a `schema_input.json` to validate the samplesheet metadata
2. The use of `.map { }` and `.groupTuple()` functions to manipulate sample metadata

### **6.1.1 Default samplesheet channel**

The samplesheet is automatically parsed, resulting in a channel that contains all relevant datta specified in the `--input`. What does this channel contain?

Open the analysis workflow file `workflows/customrnaseq.nf`. Use the `.view()` function inside the `workflow` scope to view the `ch_samplesheet` that has been input to the pipeline:

```default
workflow CUSTOMRNASEQ {

    take:
    ch_samplesheet // channel: samplesheet read in from --input
    main:

    ch_samplesheet.view()

    ch_versions = Channel.empty()
    ch_multiqc_files = Channel.empty()

    ...
}
```

Now, rerun the pipeline, ensuring `-resume` is specified in the `nextflow run` command. **Note** ebsure you are no longer inside your pipeline folder. 

::: {.callout-important}
**TO DO**

Change CONTAINER
:::

```default
nextflow run ./nf-core-customrnaseq/main.nf -resume -profile CONTAINER --input ./samplesheet.csv --outdir output
```

The channel should have the following structure:

```default
[[id:gut, single_end:false], [/.../data/gut_1.fastq.gz, /.../data/gut_2.fastq.gz]]
[[id:liver, single_end:false], [/.../data/liver_1.fastq.gz, /.../data/liver_2.fastq.gz]]
[[id:lung, single_end:false], [/.../data/lung_1.fastq.gz, /.../data/lung_2.fastq.gz]]
```

This channel contains three elements, one for each sample type. The first element is a tuple, where the first element is a `list` that represents the sample metadata. This metadata contains the sample name, stored as `id`, and if the sample is single-ded, stored as `single_end`. The second element in this tuple contain the paths to the input FASTQ files. 

Let's see how this relates to our samplesheet:

```default
sample,fastq_1,fastq_2
gut,/.../data/gut_1.fastq.gz,/.../data/gut_2.fastq.gz
liver,/.../data/liver_1.fastq.gz,/.../data/liver_2.fastq.gz
lung,/.../data/lung_1.fastq.gz,/s.../data/lung_2.fastq.gz
```

Notice that the value under the `sample` column has been assigned as `id` in the channel metadata. File paths in the `fastq_1` and `fastq_2` have been added as the second element in the tuple, which represents the read paths. 

This is defined inside the `assets/schema_input.json` file. In this file, each "property" represents a column that can be present inside the `--input` samplesheet. Any required columns are also specified, as the `"required"` item. 

:::{.callout-note collapse="true"}
## assets/schema_input.json
```default
{
    "$schema": "https://json-schema.org/draft/2020-12/schema",
    "$id": "https://raw.githubusercontent.com/nf-core/customrnaseq/main/assets/schema_input.json",
    "title": "nf-core/customrnaseq pipeline - params.input schema",
    "description": "Schema for the file provided with params.input",
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "sample": {
                "type": "string",
                "pattern": "^\\S+$",
                "errorMessage": "Sample name must be provided and cannot contain spaces",
                "meta": ["id"]
            },
            "fastq_1": {
                "type": "string",
                "format": "file-path",
                "exists": true,
                "pattern": "^\\S+\\.f(ast)?q\\.gz$",
                "errorMessage": "FastQ file for reads 1 must be provided, cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'"
            },
            "fastq_2": {
                "type": "string",
                "format": "file-path",
                "exists": true,
                "pattern": "^\\S+\\.f(ast)?q\\.gz$",
                "errorMessage": "FastQ file for reads 2 cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'"
            }
        },
        "required": ["sample", "fastq_1"]
    }
}
```
:::

Inside the `"sample"` property, the `"meta"` has been set to `["id"]`. This is the value in the channel metadata that the sample name will be assigned to. For example, if the following was specified: `"meta": ["name"]`

The parsed channel would have the following structure, where `id` is replaced with `name`:

```default
[[name:gut, single_end:false], [/.../data/gut_1.fastq.gz, /.../data/gut_2.fastq.gz]]
[[name:liver, single_end:false], [/.../data/liver_1.fastq.gz, /.../data/liver_2.fastq.gz]]
[[name:lung, single_end:false], [/.../data/lung_1.fastq.gz, /.../data/lung_2.fastq.gz]]
```

Therefore, if you wish to specify an additional column in the sampleshet (ie. adding sample metadata), the `schema_input.json` should also be changed to allow for this. We will investigate this later in the session. 

:::{.callout-tip}
Many existing nf-core nodules rely on the input metadata having **at least** the `id` value -- it is not recommended to change this name from the default. 
:::


### **6.1.2 Input channels to an nf-core module**



## 7.1 **Metadata Parsing**
We have covered a few different methods of metadata parsing.


### **7.1.1 First Pass: `.fromFilePairs`**

A first pass attempt at pulling these files into Nextflow might use the fromFilePairs method:
```default
workflow {
    Channel.fromFilePairs("/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/reads/*/*_R{1,2}.fastq.gz")
    .view
}
```
Nextflow will pull out the first part of the fastq filename and returned us a channel of tuple elements where the first element is the filename-derived ID and the second element is a list of two fastq files.

The id is stored as a simple string. We'd like to move to using a map of key-value pairs because we have more than one piece of metadata to track. In this example, we have sample, replicate, tumor/normal, and treatment. We could add extra elements to the tuple, but this changes the 'cardinality' of the elements in the channel and adding extra elements would require updating all downstream processes. A map is a single object and is passed through Nextflow channels as one value, so adding extra metadata fields will not require us to change the cardinality of the downstream processes.

There are a couple of different ways we can pull out the metadata

We can use the tokenize method to split our id. To sanity-check, I just pipe the result directly into the view operator.
```default
workflow {
    Channel.fromFilePairs("/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/reads/*/*_R{1,2}.fastq.gz")
    .map { id, reads ->
        tokens = id.tokenize("_")
    }
    .view
}
```

If we are confident about the stability of the naming scheme, we can destructure the list returned by tokenize and assign them to variables directly:
```default
map { id, reads ->
    (sample, replicate, type) = id.tokenize("_")
    meta = [sample:sample, replicate:replicate, type:type]
    [meta, reads]
}
```

::: callout-note
```default
Make sure that you're using a tuple with parentheses e.g. (one, two) rather than a List e.g. [one, two]
```
:::

If we move back to the previous method, but decided that the 'rep' prefix on the replicate should be removed, we can use regular expressions to simply "subtract" pieces of a string. Here we remove a 'rep' prefix from the replicate variable if the prefix is present:

```default
map { id, reads ->
    (sample, replicate, type) = id.tokenize("_")
    replicate -= ~/^rep/
    meta = [sample:sample, replicate:replicate, type:type]
    [meta, reads]
}
```

By setting up our the "meta", in our tuple with the format above, allows us to access the values in "sample" throughout our modules/configs as `${meta.sample}`.

## **Second Parse: `.splitCsv`**
We have briefly touched on `.splitCsv` in the first week.

As a quick overview

Assuming we have the samplesheet
```default
sample_name,fastq1,fastq2
gut_sample,/.../training/nf-training/data/ggal/gut_1.fq,/.../training/nf-training/data/ggal/gut_2.fq
liver_sample,/.../training/nf-training/data/ggal/liver_1.fq,/.../training/nf-training/data/ggal/liver_2.fq
lung_sample,/.../training/nf-training/data/ggal/lung_1.fq,/.../training/nf-training/data/ggal/lung_2.fq
```

We can set up a workflow to read in these files as:

```default
params.reads = "/.../rnaseq_samplesheet.csv"

reads_ch = Channel.fromPath(params.reads)
reads_ch.view()
reads_ch = reads_ch.splitCsv(header:true)
reads_ch.view()
```


::: callout-tip
## Challenge{.unlisted}
Using `.splitCsv` and `.map` read in the samplesheet below:
`/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/samplesheet.csv`

Set the meta to contain the following keys from the header `id`, `repeat` and `type`
:::

:::{.callout-caution collapse="true"}
## Solution
```default
params.input = "/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/samplesheet.csv"

ch_sheet = Channel.fromPath(params.input)

ch_sheet.splitCsv(header:true)
    .map {
        it ->
            [[it.id, it.repeat, it.type], it.fastq_1, it.fastq_2]
    }.view()


```
:::

## **7.2 Manipulating Metadata and Channels**
There are a number of use cases where we will be interested in manipulating our metadata and channels.

Here we will look at 2 use cases.

### **7.2.1 Matching input channels**
As we have seen in examples/challenges in the operators section, it is important to ensure that the format of the channels that you provide as inputs match the process definition.

```default
params.reads = "/home/Shared/For_NF_Workshop/training/nf-training/data/ggal/*_{1,2}.fq"

process printNumLines {
    input:
    path(reads)

    output:
    path("*txt")

    script:
    """
    wc -l ${reads}
    """
}

workflow {
    ch_input = Channel.fromFilePairs("$params.reads")
    printNumLines( ch_input )
}
```

As if the format does not match you will see and error similar to below:
```default
[myeung@papr-res-compute204 lesson7.1test]$ nextflow run test.nf 
N E X T F L O W  ~  version 23.04.1
Launching `test.nf` [agitated_faggin] DSL2 - revision: c210080493
[-        ] process > printNumLines -
```
or if using nf-core template

```default
ERROR ~ Error executing process > 'PMCCCGTRC_UMIHYBCAP:UMIHYBCAP:PREPARE_GENOME:BEDTOOLS_SLOP'

Caused by:
  Not a valid path value type: java.util.LinkedHashMap ([id:genome_size])


Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`

 -- Check '.nextflow.log' file for details
```

When encountering these errors there are two methods to correct this:

1. Change the `input` definition in the process
2. Use variations of the channel operators to correct the format of your channel

There are cases where changing the `input` definition is impractical (i.e. when using nf-core modules/subworkflows).

Let's take a look at some select modules.

[`BEDTOOLS_SLOP`](https://github.com/nf-core/modules/blob/master/modules/nf-core/bedtools/slop/main.nf)

[`BEDTOOLS_INTERSECT`](https://github.com/nf-core/modules/blob/master/modules/nf-core/bedtools/intersect/main.nf)


::: callout-tip
## Challenge{.unlisted}
Assuming that you have the following inputs

```default
ch_target = Channel.fromPath("/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals.bed")
ch_bait = Channel.fromPath("/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals2.bed").map { fn -> [ [id: fn.baseName ], fn ] }
ch_sizes = Channel.fromPath("/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/genome.sizes")
```

Write a mini workflow that:

1. Takes the `ch_target` bedfile and extends the bed by 20bp on both sides using `BEDTOOLS_SLOP` (You can use the config definition below as a helper, or write your own as an additional challenge)
2. Take the output from `BEDTOOLS_SLOP` and input this output with the `ch_baits` to `BEDTOOLS_INTERSECT`

HINT: The modules can be imported from this location: `/home/Shared/For_NF_Workshop/training/pmcc-test/modules/nf-core/bedtools`

HINT: You will need need the following operators to achieve this `.map` and `.combine`
:::

::: {.callout-note collapse="true"}
## Config
```default

process {
    withName: 'BEDTOOLS_SLOP' {
        ext.args = "-b 20"
        ext.prefix = "extended.bed"
    }

    withName: 'BEDTOOLS_INTERSECT' {
        ext.prefix = "intersect.bed"
    }
}
:::

:::{.callout-caution collapse="true"}
## **Solution**
```default
include { BEDTOOLS_SLOP } from '/home/Shared/For_NF_Workshop/training/pmcc-test/modules/nf-core/bedtools/slop/main'
include { BEDTOOLS_INTERSECT } from '/home/Shared/For_NF_Workshop/training/pmcc-test/modules/nf-core/bedtools/intersect/main'


ch_target = Channel.fromPath("/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals.bed")
ch_bait = Channel.fromPath("/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals2.bed").map { fn -> [ [id: fn.baseName ], fn ] }
ch_sizes = Channel.fromPath("/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/genome.sizes")

workflow {
    BEDTOOLS_SLOP ( ch_target.map{ fn -> [ [id:fn.baseName], fn ]}, ch_sizes)

    target_bait_bed = BEDTOOLS_SLOP.out.bed.combine( ch_bait )
    BEDTOOLS_INTERSECT( target_bait_bed, ch_sizes.map{ fn -> [ [id: fn.baseName], fn]} )
}
```

```default
nextflow run nfcoretest.nf -profile singularity -c test2.config --outdir nfcoretest
```
:::

## **7.3 Grouping with Metadata**
Earlier we introduced the function `groupTuple`


```default

ch_reads = Channel.fromFilePairs("/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/reads/*/*_R{1,2}.fastq.gz")
    .map { id, reads ->
        (sample, replicate, type) = id.tokenize("_")
        replicate -= ~/^rep/
        meta = [sample:sample, replicate:replicate, type:type]
    [meta, reads]
}

## Assume that we want to drop replicate from the meta and combine fastqs

ch_reads.map {
    meta, reads -> 
        [ meta - meta.subMap('replicate') + [data_type: 'fastq'], reads ]
    }
    .groupTuple().view()
```

