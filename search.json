[
  {
    "objectID": "sessions/2_nf_dev_intro.html",
    "href": "sessions/2_nf_dev_intro.html",
    "title": "Developing bioinformatics workflows with Nextflow",
    "section": "",
    "text": "This workshop is designed to provide participants with a fundamental understanding of developing bioinformatics pipelines using nf-core templates.\n\nCourse Presenters\n\nSong Li, Bioinformatics Core Facility, Peter Mac\nRichard Lupat, Bioinformatics Core Facility, Peter Mac\n\n\n\nCourse Helpers\nTO DO\n\n\nPrerequisites\n\nExperience with command line interface and cluster/slurm\nFamiliarity with the basic concept of workflows\nAttendance in the ‘Introduction to Nextflow and Running nf-core Workflows’ workshop, or an understanding of the Nextflow concepts outlined in the workshop material\n\n\n\nLearning Objectives:\nBy the end of this workshop, participants should be able to:\n\nLearn the structure of nf-core templates\nDevelop a basic Nextflow workflow using nf-core templates\nRe-use and import processes, modules, and sub-workflows into a Nextflow workflow\nCreate custom modules using nf-core tools\nDefine workflow logic within a workflow script\nGain an understanding of pipeline version control\n\n\n\nSet up requirements\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP via Slack.\n\n\nWorkshop schedule\n\n\n\nLesson\nOverview\nTime\n\n\n\n\nSession kick off\nSession kick off: Discuss learning outcomes and finalise setup\n10.00 - 10.10\n\n\nCreating workflows using nf-core templates\nIntroduction to nf-core pipeline template structure and workflow syntax\n10.10 - 10.50\n\n\nBreak\nBreak\n10:50 - 11:00\n\n\nnf-core modules and subworkflows\nInstalling and creating nf-core modules\n11.00 - 12.15\n\n\nLunch Break\nBreak\n12:15 - 13:00\n\n\nMetadata propagation\nWorking with Nextflow-Schema & Metadata\n13.00 - 13.40\n\n\nPipeline version control and additional resources\nPipeline development version control\n13.45 - 14:00\n\n\n\n\n\nCredits and acknowledgement\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, and Nextflow Patterns materials from Nextflow and nf-core."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Nextflow Workshop",
    "section": "",
    "text": "These workshops are designed to provide participants with a foundational understanding of Nextflow and nf-core pipelines. Participants are expected to have prior experience with the command-line interface and working with cluster systems like Slurm. The primary goal of the workshop is to equip researchers with the skills needed to use nextflow and nf-core pipelines for their research data.\n\nCourse Developers & Maintainers\nCollaborative workshop between Peter Mac, Melbourne Bioinformatics, and WEHI\n\nSong Li, Bioinformatics Core Facility, Peter Mac\nRichard Lupat, Bioinformatics Core Facility, Peter Mac\nMiriam Yeung, Cancer Genomics Translational Research Centre, Peter Mac\nSanduni Rajapaksa, Research Computing Facility, Peter Mac\n\n\n\nWorkshop Sessions\n\n\n\nLesson\nOverview\nDate\n\n\n\n\nIntroduction to Nextflow and running nf-core workflows\nIntroduction to Nextflow: Introduce nextflow’s core features and concepts + nf-core\n28th May 2025\n\n\nWorking with nf-core Templates\nIntroduction to developing bioinformatics workflow with Nextflow (Part2)\n29th May 2025\n\n\n\n\n\nCredits and acknowledgement\nThis workshop is adapted from various nextflow training materials, including:\n\nNextflow Training Materials\nCustomising Nf-Core Workshop\nHello Nextflow Workshop"
  },
  {
    "objectID": "workshops/2.1_customise_and_run.html",
    "href": "workshops/2.1_customise_and_run.html",
    "title": "Customising and running nf-core pipelines",
    "section": "",
    "text": "Objectives\n\n\n\n\nRun an nf-core pipeline\nSpecify different parameters to customise the running of an nf-core pipeline\nExamine output files from an nf-core pipeline\nCustomise config files for running an nf-core pipeline\n\n\n\n\n2.1.1. Environment setup\n\n\n\n\n\n\nInstitute specific instructions\n\n\n\n\n\nReminder before starting this session, make sure you follow your institute’s HPC rulebook re: how to run a workflow manager.\nAt Peter Mac HPC, this would mean not running the nextflow pipeline on the login node, load the nextflow & apptainer modules, and set the container cache location (or use the institutional config)\nsrun --pty -p &lt;PARTITION&gt; --mem 8GB --mincpus 2 -t 0-5:00 bash\n\nmodule load nextflow/24.10.5\n\nexport NXF_APPTAINER_CACHEDIR=\"/config/binaries/singularity/containers_devel/nextflow\n\n\n\nBefore we start, please download the test datasets into your home directory\ngit clone --single-branch --branch rnaseq https://github.com/nf-core/test-datasets ~/rnaseq_data\nAnd for the purpose of this workshop, we are going to cache the containers by running a test (with a profile) to save time for our processing later\nexport NXF_APPTAINER_CACHEDIR=\"/home/${USER}/apptainer_cache\"\nnextflow run nf-core/rnaseq -r 3.14.0 -profile test,apptainer --outdir ~/test_rnaseq --max_memory 7.GB --max_cpus 2 \nPlease also create a folder inside our work directory called lesson2.1 and move into it:\nmkdir ./lesson2.1 && cd $_\n\n\n2.1.2. Understanding an nf-core pipeline\nThe following sections of the nf-core documentation can be used to understand what a particular pipeline is doing, to inform your choices about aspects of pipeline-specific customisations. For this section, we will investigate nf-core/rnaseq pipeline.\n\n\n\nDocs\nDescription\nCustomisation level\n\n\n\n\nIntroduction\nWorkflow summary\n\nworkflow\nprocess\n\n\n\nUsage\nInputs and options\n\nworkflow\nprocess\n\n\n\nParameters\nAvailable flags\n\nworkflow\nprocess\ncompute resources\n\n\n\nOutput\nFiles from all processes processes\n\nworkflow\nprocess\ntool\n\n\n\n\n\nPipeline structure\nLooking at the nf-core/rnaseq pipeline structure provided in the introduction, we can see that the developers have:\n\nOrganised the workflow into 5 stages based on the type of work that is being done\nProvided a choice of multiple methods and specified defaults\nProvided a choice of tool for some steps\n\n\nQuiz: Observing the diagram above, which statement is true regarding the choice of alignment and quantification methods provided by the nf-core/rnaseq pipeline?\nA. The pipeline uses a fixed method for read alignment and quantification.\nB. Users can choose between several different methods for read alignment and quantification.\nC. The pipeline always performs read alignment and quantification using STAR or HISAT2.\nD. The choice of alignment and quantification method is determined automatically based on the input data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe correct answer is B. The nf-core/rnaseq pipeline allows users to choose between pseudo-alignment and quantification, or genome-based read alignment and quantification.\n\nA. is incorrect because the pipeline is not limited to a single method.\n\nC. is incorrect because while read alignment and quantification using STAR is the default method, users can also choose the pseudo-alignment.\nD. is also incorrect, as the pipeline only accepts FASTQ files as input, and the choice of alignment and quantification method must be specified by the user.\n\n\n\n\n\n\n\nDefault pipeline usage\nTypically, nf-core pipelines at a minimum require users to specify a sample sheet (--input) detailing the path to your sample data and any relevant metadata. Additionally, if a reference file version is not provided (using the --genome parameter), a default iGenomes one will be used.\nYou can see the recommended (typical) run command and all the parameters available for the nf-core/rnaseq pipeline by running:\nnextflow run nf-core/rnaseq -r 3.14.0 --help \nThe typical or recommended run command for this rnaseq pipeline is provided at the top of the output:\n\nIt outlines a requirement for:\n\n--input: An input samplesheet that contains the data to be processed\n--outdir: A location to store outputs\n--genome: Relevant reference data\n-profile: A software management method\n\n\n\n\n\n\n\nReminder: hyphens matter in Nextflow!\n\n\n\nNextflow-specific parameters use one (-) hyphen, whereas pipeline-specific parameters use two (--). In the typical run command above -profile is a Nextflow parameter, while --input is a Nextflow parameter.\n\n\n\n\n2.1.3. Setting up the run command\nMost of us will need to adjust the default run command for our experiments. Today we’ll be adjusting the typical nf-core/rnaseq run command by:\n\nCreating a samplesheet csv, based on the requirements of the pipeline\nProviding our own reference files\nUsing the apptainer software management profile\nCustomising the execution of some processes\nSpecifying the computing resource limitations of our session (2 CPUs, 8 GB RAM)\n\n\nRequired parameter: --input\nWe will create a samplesheet.csv based on the documentation provided by nf-core/rnaseq.\n\nFor rnaseq, a sample value that specifies the sample name, path to FASTQ files, and strandedness is required.\nSince we are only testing the pipeline in this session, we only need to work with a couple of samples.\nsample,fastq_1,fastq_2,strandedness\nSRR6357070,../rnaseq_data/testdata/GSE110004/SRR6357070_1.fastq.gz,../rnaseq_data/testdata/GSE110004/SRR6357070_2.fastq.gz,forward\nSRR6357071,../rnaseq_data/testdata/GSE110004/SRR6357071_1.fastq.gz,../rnaseq_data/testdata/GSE110004/SRR6357071_2.fastq.gz,forward\n\n\nRequired parameter: --outdir\nMost nf-core pipelines will require user to specified an output directory to dump all the output files to.\nFor this exercise, we will set our output directory to:\n    --outdir ./rnaseq_small_test_outdir\n\n\nRequired input: reference data\nMany nf-core pipelines have a minimum requirement for reference data inputs. The input reference data requirements for this pipeline are provided in the usage documentation:\n\nIn the documentation, we see that the recommended method to provide reference files is to explicitly state them using the --fasta and --gtf parameters. This means we can replace the --genome flag in the typical run command with our own files. To see all available reference file parameters, rerun the pipeline’s help command to view all the available parameters:\nnextflow run nf-core/rnaseq -r 3.14.0 --help\nFrom the Reference genome options parameters, we will provide our own files using:\n  --fasta ../rnaseq_data/reference/genome.fasta\n  --gtf ../rnaseq_data/reference/genes.gtf\n\n\n\nOptional parameters\nNow that we have prepared our input and reference data, we will further customise the typical run command by:\n\nUsing Nextflow’s -profile parameter to specify the apptainer profile\nAdding additional process-specific flags to skip alignment and only use pseudo-aligner\nAdding additional max resource flags to specify the number of CPUs and amount of memory available to the pipeline.\n\nUsing the command line, the following parameters can be set:\n--skip_alignment true\n--pseudo_aligner salmon\nInside the nextflow.config, max_memory and max_cpus have been set to the following:\n    max_memory                 = '128.GB'\n    max_cpus                   = 16\nUsing the command line, this can be changed with:\n--max_memory '6.GB'\n--max_cpus 2\n\n\n2.1.4. Running the pipeline\nPutting together all the input & parameters that we specified above, the final command will contain our software profile, input samplesheet, output directory, reference files, custom pipeline steps, and custom resources to use.\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n    -profile apptainer \\\n    --input samplesheet.csv \\\n    --outdir ./rnaseq_small_test_outdir \\\n    --fasta ../rnaseq_data/reference/genome.fasta \\\n    --gtf ../rnaseq_data/reference/genes.gtf \\\n    --skip_alignment true \\\n    --pseudo_aligner salmon \\\n    --max_memory '6.GB' \\\n    --max_cpus 2\nYou can see how far we’ve customised the typical run command from the original example of :\nnextflow run nf-core/rnaseq --input samplesheet.csv --genome GRCh37 -profile docker\nNow that we have prepared our data and chosen which parameters to apply, run the pipeline using the customised command we created above. Take a look at the stdout printed to the screen. Your workflow configuration and parameter customisations are all documented here. You can use this to confirm if your parameters have been correctly passed to the run command:\n\n\n\nNote that this screenshot is only demonstrating the different sections, not necessary the parameters that we have specified above\n\n\nAs the workflow starts, you will also see a number of processes that are created underneath this. Recall that processes are executed independently and can run in parallel. Nextflow manages the data dependencies between processes, ensuring that each process is executed only when its input data is available, and all of its dependencies have been satisfied.\nTo understand how this is coordinated, consider the STAR_ALIGN process.\n\nNotice a few things:\n\nWe can see which inputs are being processed by looking at the end of the process name\nWhen a process starts it progressively spawns tasks for all inputs to be processed\nTwo TRIMGALORE processes are created, one for each sample in our samplesheet.csv. This process has to complete before STAR_ALIGN begins\nOnce a TRIMGALORE task is completed for a sample, the STAR_ALIGN task for that sample begins\nWhen the STAR_ALIGN process starts, it spawns 2 tasks, one for each sample in our samplesheet\n\nExercise\nWhile we can specify parameters to a pipeline using the command line, this can be messy and result in huge nextflow run commands where the parameters we used is not documented. Recall earlier that a -params-file Nextflow parameter can be used to supply parameters to the pipeline.\nYour task: Create a a parameter file workshop-params.yaml, that contains our customised pipeline parameters. How can this file then be used in the nextflow run command? (Note: Nextflow parameters can’t be supplied inside a parameter file)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe parameter file workshop-params.yaml should contain the following:\ninput: \"./samplesheet.csv\"\noutdir: \"./rnaseq_small_test_outdir\"\nfasta: \"../rnaseq_data/reference/genome.fasta\"\ngtf: \"../rnaseq_data/reference/genes.gtf\"\nskip_alignment: true\npseudo_aligner: \"salmon\"\nmax_memory: \"6.GB\"\nmax_cpus: 2\nNote that here the full path to our reference files is provided. Since -profile is a Nextflow parameter and not a pipeline parameter, it’s not listed in the parameter file.\nTo run the pipeline using this parameter file, the following command can be used:\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n    -profile apptainer \\\n    -params-file ./workshop-params.yaml\n\n\n\n\nExamine the outputs\nOnce your pipeline has completed, you should see this message printed to your terminal:\n-[nf-core/rnaseq] Pipeline completed successfully -\nCompleted at: 27-May-2025 15:25:54\nDuration    : 2m 15s\nCPU hours   : 0.1\nSucceeded   : 20\nThe pipeline ran successfully.\nIn the meantime, list (ls -a) the contents of your directory, and you’ll see new directories (and a hidden directories/files) have been created:\n.  ..  .nextflow  .nextflow.log  .nextflow.log.1  rnaseq_small_test_outdir  samplesheet.csv  work  workshop-params.yaml\nNextflow has created 2 new output directories, work and rnaseq_small_test_outdir in the current directory.\n\nThe work directory:\n\nAs each job is ran, a unique sub-directory is created inside the work directory.\nThese directories house temporary files and various command logs created by a process. This contains all the information required when troubleshooting a failed process.\n\nWe will talk in more detail about pipeline troubleshooting later in the next section.\nThe rnaseq_small_test_outdir directory\n\nAll final outputs will be presented in a directory specified by the --outdir parameter.\nInside this directory, you should have the output files grouped into common tools:\n\n.  ..  fastqc  multiqc  pipeline_info  salmon  trimgalore\n\nExercise\nIn the previous exercise we have been skipping alignment and only do pseudo-aligner. For this final exercise, your task is to run the same sample but through a different path of the nf-core/rnaseq pipeline. We would use a different output directory for this task exercise_rnaseq.\nThe additional setting that we need to aim for:\n\nSkip picard MarkDuplicates step\nSkip bigwig file creation\nSkip stringtie\nSkip dupradar\nNot doing pseudo alignment\nBut keep the alignment step this time\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n    -profile apptainer \\\n    --input samplesheet.csv \\\n    --outdir ./exercise_rnaseq \\\n    --fasta ../rnaseq_data/reference/genome.fasta \\\n    --gtf ../rnaseq_data/reference/genes.gtf \\\n    --skip_pseudo_alignment \\\n    --skip_markduplicates \\\n    --skip_bigwig \\\n    --skip_stringtie \\\n    --skip_dupradar \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 \\\n    -resume\nWith the following message on stdout:\n[c0/9d81c8] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF_FILTER (genome.fasta)                                             [100%] 1 of 1 ✔\n[ea/3069ab] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF2BED (genome.filtered.gtf)                                         [100%] 1 of 1 ✔\n[3a/994844] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:MAKE_TRANSCRIPTS_FASTA (rsem/genome.fasta)                            [100%] 1 of 1 ✔\n[04/d57b99] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:CUSTOM_GETCHROMSIZES (genome.fasta)                                   [100%] 1 of 1 ✔\n[fd/b1edb9] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:STAR_GENOMEGENERATE (genome.fasta)                                    [100%] 1 of 1 ✔\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:CAT_FASTQ                                                                            -\n[dd/9efa41] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (SRR6357071)                                 [100%] 2 of 2 ✔\n[e3/a17aba] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (SRR6357071)                             [100%] 2 of 2 ✔\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_SUBSAMPLE_FQ_SALMON:SALMON_INDEX                                               -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_SUBSAMPLE_FQ_SALMON:FQ_SUBSAMPLE                                               -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_SUBSAMPLE_FQ_SALMON:SALMON_QUANT                                               -\n[2c/d49991] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:STAR_ALIGN (SRR6357071)                                                   [100%] 2 of 2 ✔\n[be/9af502] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:BAM_SORT_STATS_SAMTOOLS:SAMTOOLS_SORT (SRR6357071)                        [100%] 2 of 2 ✔\n[18/579c2a] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:BAM_SORT_STATS_SAMTOOLS:SAMTOOLS_INDEX (SRR6357071)                       [100%] 2 of 2 ✔\n[06/c64a64] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:BAM_SORT_STATS_SAMTOOLS:BAM_STATS_SAMTOOLS:SAMTOOLS_STATS (SRR6357071)    [100%] 2 of 2 ✔\n[56/efc898] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:BAM_SORT_STATS_SAMTOOLS:BAM_STATS_SAMTOOLS:SAMTOOLS_FLAGSTAT (SRR6357071) [100%] 2 of 2 ✔\n[28/44e581] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:BAM_SORT_STATS_SAMTOOLS:BAM_STATS_SAMTOOLS:SAMTOOLS_IDXSTATS (SRR6357071) [100%] 2 of 2 ✔\n[97/95c8d8] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_QUANT (SRR6357071)                                       [100%] 2 of 2 ✔\n[6b/29f71d] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:TX2GENE (genome.filtered.gtf)                                   [100%] 1 of 1 ✔\n[9d/99d8aa] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:TXIMPORT                                                        [100%] 1 of 1 ✔\n[b6/1662ea] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SE_GENE (tx2gene.tsv)                                           [100%] 1 of 1 ✔\n[a8/6e7986] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SE_GENE_LENGTH_SCALED (tx2gene.tsv)                             [100%] 1 of 1 ✔\n[32/955831] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SE_GENE_SCALED (tx2gene.tsv)                                    [100%] 1 of 1 ✔\n[72/c648dd] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SE_TRANSCRIPT (tx2gene.tsv)                                     [100%] 1 of 1 ✔\n[40/6e5a0d] process &gt; NFCORE_RNASEQ:RNASEQ:DESEQ2_QC_STAR_SALMON                                                                [100%] 1 of 1 ✔\n[f8/4f4df4] process &gt; NFCORE_RNASEQ:RNASEQ:SUBREAD_FEATURECOUNTS (SRR6357071)                                                   [100%] 2 of 2 ✔\n[f4/216288] process &gt; NFCORE_RNASEQ:RNASEQ:MULTIQC_CUSTOM_BIOTYPE (SRR6357071)                                                  [100%] 2 of 2 ✔\n[ed/d5fc22] process &gt; NFCORE_RNASEQ:RNASEQ:QUALIMAP_RNASEQ (SRR6357071)                                                         [100%] 2 of 2 ✔\n[1c/c7c8e2] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RSEQC:RSEQC_BAMSTAT (SRR6357071)                                                 [100%] 2 of 2 ✔\n[16/449e9a] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RSEQC:RSEQC_INNERDISTANCE (SRR6357071)                                           [100%] 2 of 2 ✔\n[dc/8d178e] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RSEQC:RSEQC_INFEREXPERIMENT (SRR6357071)                                         [100%] 2 of 2 ✔\n[43/4f82d7] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RSEQC:RSEQC_JUNCTIONANNOTATION (SRR6357071)                                      [100%] 2 of 2 ✔\n[6c/e6dc04] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RSEQC:RSEQC_JUNCTIONSATURATION (SRR6357071)                                      [100%] 2 of 2 ✔\n[a0/2cc99b] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RSEQC:RSEQC_READDISTRIBUTION (SRR6357071)                                        [100%] 2 of 2 ✔\n[b5/2ad65c] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RSEQC:RSEQC_READDUPLICATION (SRR6357071)                                         [100%] 2 of 2 ✔\n[6b/6657bb] process &gt; NFCORE_RNASEQ:RNASEQ:CUSTOM_DUMPSOFTWAREVERSIONS (1)                                                      [100%] 1 of 1 ✔\n[73/528544] process &gt; NFCORE_RNASEQ:RNASEQ:MULTIQC (1)                                                                          [100%] 1 of 1 ✔\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 27-May-2025 17:09:36\nDuration    : 6m 15s\nCPU hours   : 0.2\nSucceeded   : 52\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core pipelines contain default settings and required inputs that can be customised.\nAn nf-core pipeline’s Usage, Output, and Parameters documentation can be used to design a suitable run command.\nParameters can be used to customise the workflow, processes, tools, and compute resources.\n\n\n\n\n\n\n\n\nNext Chapter: Troubleshooting a nextflow pipeline run\n\n\nThis workshop is adapted from various nextflow training materials, including:\n\nNextflow Training Materials\nCustomising Nf-Core Workshop\nHello Nextflow Workshop"
  },
  {
    "objectID": "workshops/5.3_metadata_propagation.html",
    "href": "workshops/5.3_metadata_propagation.html",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of how to use nf-core modules in a workflow script\nManipulate and proprogate sample metadata throughout the workflow\nCreate a custom nf-core module"
  },
  {
    "objectID": "workshops/5.3_metadata_propagation.html#samplesheet-parsing",
    "href": "workshops/5.3_metadata_propagation.html#samplesheet-parsing",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "6.1 Samplesheet parsing",
    "text": "6.1 Samplesheet parsing\nIn the ./nf-core-customrnaseq/main.nf script, the PIPELINE_INITIALISATION subworkflow created by default from the nf-core template will output a channel that contains the parsed --input samplesheet. This channel is then input into NFCORE_CUSTOMRNASEQ, which launches our analysis workflow containing the newly included modules salmon/quant and salmon/quant.\n...\n\ninclude { PIPELINE_INITIALISATION } from './subworkflows/local/utils_nfcore_customrnaseq_pipeline'\n\n...\n\nworkflow {\n\n    main:\n\n    ...\n\n    //\n    // WORKFLOW: Run main workflow\n    //\n    NFCORE_CUSTOMRNASEQ (\n        PIPELINE_INITIALISATION.out.samplesheet\n    )\n\n    ...\n\n}\nHow does the PIPELINE_INITIALISATION parse the samplesheet?\n\n\n\n\n\n\n./nf-core-customrnaseq/subworkflows/local/utils_nfcore_customrnaseq_pipeline/main.nf\n\n\n\n\n\n//\n// Create channel from input file provided through params.input\n//\nworkflow PIPELINE_INITIALISATION {\n\n    take:\n    version           // boolean: Display version and exit\n    validate_params   // boolean: Boolean whether to validate parameters against the schema at runtime\n    monochrome_logs   // boolean: Do not use coloured log outputs\n    nextflow_cli_args //   array: List of positional nextflow CLI args\n    outdir            //  string: The output directory where the results will be saved\n    input             //  string: Path to input samplesheet\n\n    main:\n\n    ...\n\n    Channel\n        .fromList(samplesheetToList(params.input, \"${projectDir}/assets/schema_input.json\"))\n        .map {\n            meta, fastq_1, fastq_2 -&gt;\n                if (!fastq_2) {\n                    return [ meta.id, meta + [ single_end:true ], [ fastq_1 ] ]\n                } else {\n                    return [ meta.id, meta + [ single_end:false ], [ fastq_1, fastq_2 ] ]\n                }\n        }\n        .groupTuple()\n        .map { samplesheet -&gt;\n            validateInputSamplesheet(samplesheet)\n        }\n        .map {\n            meta, fastqs -&gt;\n                return [ meta, fastqs.flatten() ]\n        }\n        .set { ch_samplesheet }\n\n    emit:\n    samplesheet = ch_samplesheet\n    versions    = ch_versions\n}\n\n\n\nThe important pieces of information that we will explore further in this section are:\n\nThe use of a schema_input.json to validate the samplesheet metadata\nThe use of .map { } and .groupTuple() functions to manipulate sample metadata\n\n\n6.1.1 Default samplesheet channel\nThe samplesheet is automatically parsed, resulting in a channel that contains all relevant datta specified in the --input. What does this channel contain?\nOpen the analysis workflow file workflows/customrnaseq.nf. Use the .view() function inside the workflow scope to view the ch_samplesheet that has been input to the pipeline:\nworkflow CUSTOMRNASEQ {\n\n    take:\n    ch_samplesheet // channel: samplesheet read in from --input\n    main:\n\n    ch_samplesheet.view()\n\n    ch_versions = Channel.empty()\n    ch_multiqc_files = Channel.empty()\n\n    ...\n}\nNow, rerun the pipeline, ensuring -resume is specified in the nextflow run command. Note ebsure you are no longer inside your pipeline folder.\nnextflow run ./nf-core-customrnaseq/main.nf -resume -profile apptainer --input ./samplesheet.csv --outdir output\nThe channel should have the following structure:\n[[id:gut, single_end:false], [/.../data/gut_1.fastq.gz, /.../data/gut_2.fastq.gz]]\n[[id:liver, single_end:false], [/.../data/liver_1.fastq.gz, /.../data/liver_2.fastq.gz]]\n[[id:lung, single_end:false], [/.../data/lung_1.fastq.gz, /.../data/lung_2.fastq.gz]]\nThis channel contains three elements, one for each sample type. The first element is a tuple, where the first element is a list that represents the sample metadata. This metadata contains the sample name, stored as id, and if the sample is single-ded, stored as single_end. The second element in this tuple contain the paths to the input FASTQ files.\nLet’s see how this relates to our samplesheet:\nsample,fastq_1,fastq_2\ngut,/.../data/gut_1.fastq.gz,/.../data/gut_2.fastq.gz\nliver,/.../data/liver_1.fastq.gz,/.../data/liver_2.fastq.gz\nlung,/.../data/lung_1.fastq.gz,/s.../data/lung_2.fastq.gz\nNotice that the value under the sample column has been assigned as id in the channel metadata. File paths in the fastq_1 and fastq_2 have been added as the second element in the tuple, which represents the read paths.\nThis is defined inside the assets/schema_input.json file. In this file, each “property” represents a column that can be present inside the --input samplesheet. Any required columns are also specified, as the \"required\" item.\n\n\n\n\n\n\nassets/schema_input.json\n\n\n\n\n\n{\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n    \"$id\": \"https://raw.githubusercontent.com/nf-core/customrnaseq/main/assets/schema_input.json\",\n    \"title\": \"nf-core/customrnaseq pipeline - params.input schema\",\n    \"description\": \"Schema for the file provided with params.input\",\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"sample\": {\n                \"type\": \"string\",\n                \"pattern\": \"^\\\\S+$\",\n                \"errorMessage\": \"Sample name must be provided and cannot contain spaces\",\n                \"meta\": [\"id\"]\n            },\n            \"fastq_1\": {\n                \"type\": \"string\",\n                \"format\": \"file-path\",\n                \"exists\": true,\n                \"pattern\": \"^\\\\S+\\\\.f(ast)?q\\\\.gz$\",\n                \"errorMessage\": \"FastQ file for reads 1 must be provided, cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'\"\n            },\n            \"fastq_2\": {\n                \"type\": \"string\",\n                \"format\": \"file-path\",\n                \"exists\": true,\n                \"pattern\": \"^\\\\S+\\\\.f(ast)?q\\\\.gz$\",\n                \"errorMessage\": \"FastQ file for reads 2 cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'\"\n            }\n        },\n        \"required\": [\"sample\", \"fastq_1\"]\n    }\n}\n\n\n\nInside the \"sample\" property, the \"meta\" has been set to [\"id\"]. This is the value in the channel metadata that the sample name will be assigned to. For example, if the following was specified: \"meta\": [\"name\"]\nThe parsed channel would have the following structure, where id is replaced with name:\n[[name:gut, single_end:false], [/.../data/gut_1.fastq.gz, /.../data/gut_2.fastq.gz]]\n[[name:liver, single_end:false], [/.../data/liver_1.fastq.gz, /.../data/liver_2.fastq.gz]]\n[[name:lung, single_end:false], [/.../data/lung_1.fastq.gz, /.../data/lung_2.fastq.gz]]\nTherefore, if you wish to specify an additional column in the sampleshet (ie. adding sample metadata), the schema_input.json should also be changed to allow for this. We will investigate this later in the session.\n\n\n\n\n\n\nTip\n\n\n\nMany existing nf-core nodules rely on the input metadata having at least the id value – it is not recommended to change this name from the default.\n\n\n\n\n6.1.2 Input channels to an nf-core module\nNow that we know the contents of our parsed samplesheet channel, let’s check what inputs are required to our two processes.\n\nProcess salmon/index\nFrom the salmon/index module GitHub page, we see that the process requires two inputs: a genom_fasta file, and a transcript_fasta file.\nprocess SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n\n    conda \"${moduleDir}/environment.yml\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.10.3--h6dccd9a_2' :\n        'biocontainers/salmon:1.10.3--h6dccd9a_2' }\"\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    ...\n    \n}\nLet’s take a closer look at the main Nextflow script that we use to launch the pipeline. Near the top of the script, it provides an example of how to set parameters. These parameters can be specified to the nextflow run command using a parameter .yaml file, specified with -params-file.\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    GENOME PARAMETER VALUES\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\n// TODO nf-core: Remove this line if you don't need a FASTA file\n//   This is an example of how to use getGenomeAttribute() to fetch parameters\n//   from igenomes.config using `--genome`\nparams.fasta = getGenomeAttribute('fasta')\nIn the template, it provides an example of how to set the fasta parameter that can be passed to the workflow. Since we won’t need this, we can comment it out. Instead, we want to add a genome_fasta and transcript_fasta parameter.\nEdit that code block to the following:\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    GENOME PARAMETER VALUES\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\n// TODO nf-core: Remove this line if you don't need a FASTA file\n//   This is an example of how to use getGenomeAttribute() to fetch parameters\n//   from igenomes.config using `--genome`\n// params.fasta = getGenomeAttribute('fasta')\nparams.genome_fasta = getGenomeAttribute('genome_fasta')\nparams.transcript_fasta = getGenomeAttribute('transcript_fasta')\nIn summary, the getGenomeAttribute will access the genome_fasta or transcript_fasta that’s specified to it via the command line or a parameter file, and re-assign it as a Nextflow parameter that can be accessed in workflows.\nNow, we can create a parameter file params.yaml, and specify the paths to genome_fasta and transcript_fasta:\n\n\n\n\n\n\nImportant\n\n\n\nTO DO Change PATH TO DATA\n\n\ngenome_fasta: \"/.../data/genome.fa\"\ntranscript_fasta: \"/.../transcriptome.fa\"\nSince these parameters are strings, we can convert them into channels using the Channel.fromPath channel factory. Open you analysis workflow script and add the following:\nworkflow CUSTOMRNASEQ {\n\n    take:\n    ch_samplesheet // channel: samplesheet read in from --input\n\n    main:\n\n    // Gather reference files\n    ch_genome_fasta = Channel.fromPath(params.genome_fasta)\n    ch_transcript_fasta = Channel.fromPath(params.transcript_fasta)\n\n    ch_versions = Channel.empty()\n    ch_multiqc_files = Channel.empty()\n\n    ...\n}\n\n\n\n\n\n\nTip\n\n\n\nNextflow parameters are global variables that can be accessed by any script within the pipeline. Therefore, it doesn’t need to be passed from one file/workflow/process to another.\n\n\nThe newly created channels ch_genome_fasta and ch_transcript_fasta match the inputs defined in the SALMON_INDEX process. Suppy these channels as inputs to the SALMON_INDEX, inside workflow CUSTOMRNASEQ { ... }\n    SALMON_INDEX ( \n        ch_genome_fasta,\n        ch_transcript_fasta\n    )\nRerun the pipeline, specifying the params.yaml file in the nextflow run command\nnextflow run ./nf-core-customrnaseq/main.nf -resume -profile apptainer --input ./samplesheet.csv --outdir output -params-file ./params.yaml \n\n\n\n\n\n\nstdout\n\n\n\n\n\n\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `./nf-core-customrnaseq/main.nf` [goofy_cori] DSL2 - revision: 5492b74b7a\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/customrnaseq 1.0.0dev\n------------------------------------------------------\nInput/output options\n  input              : ./samplesheet.csv\n  outdir             : output\n\nGeneric options\n  trace_report_suffix: 2025-05-06_03-22-36\n\nCore Nextflow options\n  runName            : goofy_cori\n  containerEngine    : singularity\n  launchDir          : /scratch/users/sli/workshop/pipeline\n  workDir            : /scratch/users/sli/workshop/pipeline/work\n  projectDir         : /scratch/users/sli/workshop/pipeline/nf-core-customrnaseq\n  userName           : sli\n  profile            : singularity\n  configFiles        : /scratch/users/sli/workshop/pipeline/nf-core-customrnaseq/nextflow.config\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\n* The nf-core framework\n    https://doi.org/10.1038/s41587-020-0439-x\n\n* Software dependencies\n    https://github.com/nf-core/customrnaseq/blob/main/CITATIONS.md\n\nWARN: The following invalid input values have been detected:\n\n* --genome_fasta: /scratch/users/sli/workshop/training/nf4-science/rnaseq/data/genome.fa\n* --transcript_fasta: /scratch/users/sli/workshop/training/nf-training/data/ggal/transcriptome.fa\n\n\nexecutor &gt;  local (1)\n[19/79bd6e] NFCORE_CUSTOMRNASEQ:CUSTOMRNASEQ:FASTQC (liver)       | 3 of 3, cached: 3 ✔\n[5c/05a99e] NFC…ASEQ:CUSTOMRNASEQ:SALMON_INDEX (transcriptome.fa) | 1 of 1 ✔\n[e4/41d893] NFCORE_CUSTOMRNASEQ:CUSTOMRNASEQ:MULTIQC              | 1 of 1 ✔\n-[nf-core/customrnaseq] Pipeline completed successfully-\n\n\n\nHere, the pipeline completed successfully, with the new process SALMON_INDEX completing successfully. However, there are two new warnings, which we will discuss more later:\nWARN: The following invalid input values have been detected:\n\n* --genome_fasta: /scratch/users/sli/workshop/training/nf4-science/rnaseq/data/genome.fa\n* --transcript_fasta: /scratch/users/sli/workshop/training/nf-training/data/ggal/transcriptome.fa\n\n\nProcess salmon/quant\nLet’s repeat the process for salmon/quant. Now that we know the contents of the parsed samplesheet channel, we need to determine if this channel is suitable to be used in our processes.\n[[name:gut, single_end:false], [/.../data/gut_1.fastq.gz, /.../data/gut_2.fastq.gz]]\n[[name:liver, single_end:false], [/.../data/liver_1.fastq.gz, /.../data/liver_2.fastq.gz]]\n[[name:lung, single_end:false], [/.../data/lung_1.fastq.gz, /.../data/lung_2.fastq.gz]]\nFrom the salmon/quant module GitHub page, we see that many inputs are needed in addition to our data.\nprocess SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n\n    conda \"${moduleDir}/environment.yml\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.10.3--h6dccd9a_2' :\n        'biocontainers/salmon:1.10.3--h6dccd9a_2' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    ...\n\n}\n\nThe first input that is required is a tuple of two elements – the first element containing the sample metadata, and the second element containing the paths to the FASTQ files. The default channel parsed from the samplesheet matches this structure, so it can be used as the input.\n\n\n\n\n\n\n\ntag “$meta.id”\n\n\n\nRecall that when a process is exected, the output containing the process name, along with the contents of the tag directive will be printed:\n[6e/28919b] Submitted process &gt; FOO (alpha)\n[d2/1c6175] Submitted process &gt; FOO (gamma)\n[1c/3ef220] Submitted process &gt; FOO (omega)\nIn the first input to the process, the sample metadata is specified – the id contained within this metadata is accessed (ie. the sample name), and used in the tag directive. This directive is typically defined as “$meta.id” for all nf-core processes, so it is recommended that your pipeline contains this id metadata value.\n\n\n\nThe second input to the process is the path to the index file. The SALMON_INDEX output of this process can be used as input to SALMON_QUANT\nThe last two inputs are values that defines how SALMON_QUANT will be ran. For now, let’s define these variables within the workflow block of workflows/customrnaseq.nf:\n\n    def align_mode = false\n    def lib_type = \"A\"\nExercise: Following the steps we used previously to add genome_fasta and transcript_fasta to the workflow, repeat it for gtf. Provide these inputs to the SALMON_QUANT and rerun the workflow, ensuring the new process has been executed.\n\n\n\n\n\n\nImportant\n\n\n\nCHANGE PATH TO DATA\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nInside nf-core-customrnaseq/main.nf, the gtf parameter is added:\n\nparams.gtf = getGenomeAttribute('gtf')\n\nThe path to the gtf file is specified in the params.yaml:\n\ngtf: \"/PATH TO DATA/rna-seq/ref/chr22_with_ERCC92.gtf\"\n\nInside workflows/customrnaseq.nf, convert the parameter to a channel\n\nch_gtf =  Channel.fromPath(params.gtf)\n\nThe inputs to SALMON_QUANT are defined as:\n\nSALMON_QUANT (\n    ch_samplesheet,\n    SALMON_INDEX.out.index,\n    ch_gtf,\n    ch_transcript_fasta,\n    align_mode,\n    lib_type\n)\nNote that the order of inputs matter, and have to match what is declared in the process input definition.\n\nThe new process SALMON_QUANT is present, when rerunning the pipeline\n\nnextflow run ./nf-core-customrnaseq/main.nf -resume -profile singularity --input ./samplesheet.csv --outdir output -params-file ./params.yaml \nexecutor &gt;  local (1)\n[10/4a95a5] NFCORE_CUSTOMRNASEQ:CUSTOMRNASEQ:FASTQC (lung)        | 3 of 3, cached: 3 ✔\n[5c/05a99e] NFC…ASEQ:CUSTOMRNASEQ:SALMON_INDEX (transcriptome.fa) | 1 of 1, cached: 1 ✔\n[be/6fe5d8] NFCORE_CUSTOMRNASEQ:CUSTOMRNASEQ:SALMON_QUANT (gut)   | 1 of 1, cached: 1 ✔\n[2c/bfcac1] NFCORE_CUSTOMRNASEQ:CUSTOMRNASEQ:MULTIQC              | 1 of 1 ✔\n-[nf-core/customrnaseq] Pipeline completed successfully-\n111\n\n\n\n\n\n\n\n\n\nnf-core-customrnaseq/main.nf\n\n\n\n\n\n#!/usr/bin/env nextflow\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    nf-core/customrnaseq\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Github : https://github.com/nf-core/customrnaseq\n    Website: https://nf-co.re/customrnaseq\n    Slack  : https://nfcore.slack.com/channels/customrnaseq\n----------------------------------------------------------------------------------------\n*/\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    IMPORT FUNCTIONS / MODULES / SUBWORKFLOWS / WORKFLOWS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\ninclude { CUSTOMRNASEQ  } from './workflows/customrnaseq'\ninclude { PIPELINE_INITIALISATION } from './subworkflows/local/utils_nfcore_customrnaseq_pipeline'\ninclude { PIPELINE_COMPLETION     } from './subworkflows/local/utils_nfcore_customrnaseq_pipeline'\ninclude { getGenomeAttribute      } from './subworkflows/local/utils_nfcore_customrnaseq_pipeline'\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    GENOME PARAMETER VALUES\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\n// TODO nf-core: Remove this line if you don't need a FASTA file\n//   This is an example of how to use getGenomeAttribute() to fetch parameters\n//   from igenomes.config using `--genome`\n// params.fasta = getGenomeAttribute('fasta')\nparams.genome_fasta = getGenomeAttribute('genome_fasta')\nparams.transcript_fasta = getGenomeAttribute('transcript_fasta')\nparams.gtf = getGenomeAttribute('gtf')\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    NAMED WORKFLOWS FOR PIPELINE\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\n//\n// WORKFLOW: Run main analysis pipeline depending on type of input\n//\nworkflow NFCORE_CUSTOMRNASEQ {\n\n    take:\n    samplesheet // channel: samplesheet read in from --input\n\n    main:\n\n    //\n    // WORKFLOW: Run pipeline\n    //\n    CUSTOMRNASEQ (\n        samplesheet\n    )\n    emit:\n    multiqc_report = CUSTOMRNASEQ.out.multiqc_report // channel: /path/to/multiqc_report.html\n}\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    RUN MAIN WORKFLOW\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\nworkflow {\n\n    main:\n    //\n    // SUBWORKFLOW: Run initialisation tasks\n    //\n    PIPELINE_INITIALISATION (\n        params.version,\n        params.validate_params,\n        params.monochrome_logs,\n        args,\n        params.outdir,\n        params.input\n    )\n\n    //\n    // WORKFLOW: Run main workflow\n    //\n    NFCORE_CUSTOMRNASEQ (\n        PIPELINE_INITIALISATION.out.samplesheet\n    )\n    //\n    // SUBWORKFLOW: Run completion tasks\n    //\n    PIPELINE_COMPLETION (\n        params.email,\n        params.email_on_fail,\n        params.plaintext_email,\n        params.outdir,\n        params.monochrome_logs,\n        params.hook_url,\n        NFCORE_CUSTOMRNASEQ.out.multiqc_report\n    )\n}\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    THE END\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\n\n\n\n\n\n\n\n\nnf-core-customrnaseq/workflows/customrnaseq.nf\n\n\n\n\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    IMPORT MODULES / SUBWORKFLOWS / FUNCTIONS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\ninclude { FASTQC                 } from '../modules/nf-core/fastqc/main'\ninclude { MULTIQC                } from '../modules/nf-core/multiqc/main'\ninclude { paramsSummaryMap       } from 'plugin/nf-schema'\ninclude { paramsSummaryMultiqc   } from '../subworkflows/nf-core/utils_nfcore_pipeline'\ninclude { softwareVersionsToYAML } from '../subworkflows/nf-core/utils_nfcore_pipeline'\ninclude { methodsDescriptionText } from '../subworkflows/local/utils_nfcore_customrnaseq_pipeline'\n\ninclude { SALMON_INDEX } from '../modules/nf-core/salmon/index/main' \ninclude { SALMON_QUANT } from '../modules/nf-core/salmon/quant/main'    \n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    RUN MAIN WORKFLOW\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\nworkflow CUSTOMRNASEQ {\n\n    take:\n    ch_samplesheet // channel: samplesheet read in from --input\n\n    main:\n\n    // Gather reference files\n    ch_genome_fasta = Channel.fromPath(params.genome_fasta)\n    ch_transcript_fasta = Channel.fromPath(params.transcript_fasta)\n    ch_gtf =  Channel.fromPath(params.gtf)\n\n    ch_versions = Channel.empty()\n    ch_multiqc_files = Channel.empty()\n\n    //\n    // MODULE: Run FastQC\n    //\n    FASTQC (\n        ch_samplesheet\n    )\n    ch_multiqc_files = ch_multiqc_files.mix(FASTQC.out.zip.collect{it[1]})\n    ch_versions = ch_versions.mix(FASTQC.out.versions.first())\n\n\n    SALMON_INDEX ( \n        ch_genome_fasta,\n        ch_transcript_fasta\n    )\n\n    def align_mode = false\n    def lib_type = \"A\"\n\n    SALMON_QUANT (\n        ch_samplesheet,\n        SALMON_INDEX.out.index,\n        ch_gtf,\n        ch_transcript_fasta,\n        align_mode,\n        lib_type\n    )\n\n\n    //\n    // Collate and save software versions\n    //\n    softwareVersionsToYAML(ch_versions)\n        .collectFile(\n            storeDir: \"${params.outdir}/pipeline_info\",\n            name: 'nf_core_'  +  'customrnaseq_software_'  + 'mqc_'  + 'versions.yml',\n            sort: true,\n            newLine: true\n        ).set { ch_collated_versions }\n\n\n    //\n    // MODULE: MultiQC\n    //\n    ch_multiqc_config        = Channel.fromPath(\n        \"$projectDir/assets/multiqc_config.yml\", checkIfExists: true)\n    ch_multiqc_custom_config = params.multiqc_config ?\n        Channel.fromPath(params.multiqc_config, checkIfExists: true) :\n        Channel.empty()\n    ch_multiqc_logo          = params.multiqc_logo ?\n        Channel.fromPath(params.multiqc_logo, checkIfExists: true) :\n        Channel.empty()\n\n    summary_params      = paramsSummaryMap(\n        workflow, parameters_schema: \"nextflow_schema.json\")\n    ch_workflow_summary = Channel.value(paramsSummaryMultiqc(summary_params))\n    ch_multiqc_files = ch_multiqc_files.mix(\n        ch_workflow_summary.collectFile(name: 'workflow_summary_mqc.yaml'))\n    ch_multiqc_custom_methods_description = params.multiqc_methods_description ?\n        file(params.multiqc_methods_description, checkIfExists: true) :\n        file(\"$projectDir/assets/methods_description_template.yml\", checkIfExists: true)\n    ch_methods_description                = Channel.value(\n        methodsDescriptionText(ch_multiqc_custom_methods_description))\n\n    ch_multiqc_files = ch_multiqc_files.mix(ch_collated_versions)\n    ch_multiqc_files = ch_multiqc_files.mix(\n        ch_methods_description.collectFile(\n            name: 'methods_description_mqc.yaml',\n            sort: true\n        )\n    )\n\n    MULTIQC (\n        ch_multiqc_files.collect(),\n        ch_multiqc_config.toList(),\n        ch_multiqc_custom_config.toList(),\n        ch_multiqc_logo.toList(),\n        [],\n        []\n    )\n\n    emit:multiqc_report = MULTIQC.out.report.toList() // channel: /path/to/multiqc_report.html\n    versions       = ch_versions                 // channel: [ path(versions.yml) ]\n\n}\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    THE END\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/"
  },
  {
    "objectID": "workshops/5.3_metadata_propagation.html#complex-datasets",
    "href": "workshops/5.3_metadata_propagation.html#complex-datasets",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "6.2 Complex datasets",
    "text": "6.2 Complex datasets\nConsider the dataset below:\nHBR_Rep1_read1.fastq.gz\nHBR_Rep1_read2.fastq.gz\nHBR_Rep2_read1.fastq.gz\nHBR_Rep2_read2.fastq.gz\nUHR_Rep1_read1.fastq.gz\nUHR_Rep1_read2.fastq.gz\nUHR_Rep2_read1.fastq.gz\nUHR_Rep2_read2.fastq.gz\nThere are two samples, HBR and UHR, each with two replicates. How can we adjust our samplesheet and pipeline to handle this dataset?\n\n6.2.1 Samplesheet metadata\nThe first step is to add an additional column to the samplesheet. For this dataset, we will add replicate.\n\n\n\n\n\n\nImportant\n\n\n\nTO DO Change path\n\n\nsample,replicate,fastq_1,fastq_2\nERC,1,/.../HBR_Rep1_read1.fastq.gz,/.../HBR_Rep1_read2.fastq.gz\nERC,2,/.../HBR_Rep2_read1.fastq.gz,/.../HBR_Rep2_read2.fastq.gz\nUHR,1,/.../UHR_Rep1_read1.fastq.gz,/.../UHR_Rep1_read2.fastq.gz\nUHR,2,/.../UHR_Rep2_read1.fastq.gz,/.../UHR_Rep2_read2.fastq.gz\nSince this new column was added, the assets/schema_input.json will also need to be changed, specifying a new \"replicate\" property. Currently, there is no nf-core tool that can help add a new property to the schema – this has to be done manually. Add the following as a “property” to assets/schema_input.json:\n            \"replicate\": {\n                \"type\": \"integer\",\n                \"errorMessage\": \"Replicate number must be provided\",\n                \"meta\": [\"rep\"]\n            }\nHere, the important specification is \"meta\", which will result in parsed samplesheet channel to have an additional metadata value rep, which corresponds to the replicate specified in the samplesheet.\nNow, let’s run the pipeline with the new samplesheet, and view() the ch_samplesheet that has been created:\n\n\n\n\n\n\nWarning\n\n\n\n\n\nInside the PIPELINE_INITIALISATION workflow, you will need to change the samplesheet parsing to the following:\nnf-core-customrnaseq/subworkflows/local/utils_nfcore_customrnaseq_pipeline/main.nf:\n\n    //\n    // Create channel from input file provided through params.input\n    //\n\n    Channel\n        .fromList(samplesheetToList(params.input, \"${projectDir}/assets/schema_input.json\"))\n        .map {\n            meta, fastq_1, fastq_2 -&gt;\n                if (!fastq_2) {\n                    return [ meta + [ single_end:true ], [ fastq_1 ] ]\n                } else {\n                    return [ meta + [ single_end:false ], [ fastq_1, fastq_2 ] ]\n                }\n        }\n        // .groupTuple()\n        // .map { samplesheet -&gt;\n        //     validateInputSamplesheet(samplesheet)\n        // }\n        // .map {\n        //     meta, fastqs -&gt;\n        //         return [ meta, fastqs.flatten() ]\n        // }\n        .set { ch_samplesheet }\nHere, the .groupTuple() and downstream functions have been commented out. Within the first map { } funcion, the first element of the array has also been removed.\nWhen the full command is used, this can cause unexpected behaviour, especially when creating new metadata values.\n\n\n\nnextflow run ./nf-core-customrnaseq/main.nf -resume -profile apptainer --input ./samplesheet_replicates.csv --outdir output -params-file ./params.yaml \nAs expected, the new metadata value rep has been added in ch_samplesheet:\n[[id:ERC, rep:1, single_end:false], [/.../HBR_Rep1_read1.fastq.gz, /.../HBR_Rep1_read2.fastq.gz]]\n[[id:ERC, rep:2, single_end:false], [/.../HBR_Rep2_read1.fastq.gz, /.../HBR_Rep2_read2.fastq.gz]]\n[[id:UHR, rep:1, single_end:false], [/.../UHR_Rep1_read1.fastq.gz, /.../UHR_Rep1_read2.fastq.gz]]\n[[id:UHR, rep:2, single_end:false], [/.../UHR_Rep2_read1.fastq.gz, /.../UHR_Rep2_read2.fastq.gz]]\n\n\n6.2.2 Grouping metadata\nNow, what if we wish to concatenate the replicate FASTQ files that belong to one sample together? This can be done using the map { } function.\nLet’s first consider one element channel:\n[[id:ERC, rep:1, single_end:false], [/.../HBR_Rep1_read1.fastq.gz, /.../HBR_Rep1_read2.fastq.gz]]\nThis element is a tuple, where the first list contains the metadata specified in the input samplesheet, and the second list contain the FASTQ files.\nThe map { } function can be used to access different values in the tuple.\nch_samplesheet\n    .map { meta, fqs -&gt;\n        meta.id = \"CHANGED_NAME\"\n        [meta, fqs]\n    }\n    .view()\nFirst, the local variables meta and fqs are declared within the map function. Variables declared before the -&gt; are always local variables that reference the input channel. The fist local variable meta can then be used to access the first element in the ch_input tuple (the metadata list). Similarly, the second local variable fqs can be used to access te second element in the ch_input tuple (the FASTQ list).\nIn the second line of the map function, the id value inside meta has been changed from ERC to \"CHANGED_SAMPLE\".\nIn the last line of the map function, the updated metadata and unchanged FASTQs are returned. The last line of the map function will be the output.\nThese steps are repeated for each element in ch_samplesheet, resulting in the following output:\n[[id:CHANGED_SAMPLE, rep:1, single_end:false], [/.../HBR_Rep1_read1.fastq.gz, /.../HBR_Rep1_read2.fastq.gz]]\n[[id:CHANGED_SAMPLE, rep:2, single_end:false], [/.../HBR_Rep2_read1.fastq.gz, /.../HBR_Rep2_read2.fastq.gz]]\n[[id:CHANGED_SAMPLE, rep:1, single_end:false], [/.../UHR_Rep1_read1.fastq.gz, /.../UHR_Rep1_read2.fastq.gz]]\n[[id:CHANGED_SAMPLE, rep:2, single_end:false], [/.../UHR_Rep2_read1.fastq.gz, /.../UHR_Rep2_read2.fastq.gz]]\nThe map { } function can also be used to remove metadata values. To remove rep from the metadata:\nch_samplesheet\n    .map { meta, fqs -&gt;\n        meta = meta - meta.subMap('rep') \n        [meta, fqs]\n    }\n    .view()\nHere, subMap is used to remove rep from the original metadata. In the last line of the map { } function, the updated metadata is returned\nExercise: What do you think the new outputs would look like? Add the map { } function to your workflow script to confirm.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nrep is removed from the metadata, resulting in only id and single_end keys.\n[[id:ERC, single_end:false], [/.../HBR_Rep1_read1.fastq.gz, /.../HBR_Rep1_read2.fastq.gz]]\n[[id:ERC, single_end:false], [/.../HBR_Rep2_read1.fastq.gz, /.../HBR_Rep2_read2.fastq.gz]]\n[[id:UHR, single_end:false], [/.../UHR_Rep1_read1.fastq.gz, /.../UHR_Rep1_read2.fastq.gz]]\n[[id:UHR, single_end:false], [/.../UHR_Rep2_read1.fastq.gz, /.../UHR_Rep2_read2.fastq.gz]]\nIn the above output, we can see that we now have two samples that have the same metadata, but different FASTQ input files.\n\n\n\nTo group together all FASTQ files containing the same file metadata, groupTuple() can be used.\nExercise: Add groupTuple() after your map { } function and view the results. Does the output match what you expect?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAdding groupTuple() after the map { } function and viewing the result:\n    ch_samplesheet\n        .map { meta, fqs -&gt;\n            meta = meta - meta.subMap('rep') \n            [meta, fqs]\n        }\n        .groupTuple()\n        .view()\nThe following can be seen:\n[[id:ERC, single_end:false], [/.../HBR_Rep1_read1.fastq.gz, /.../HBR_Rep1_read2.fastq.gz], [/.../HBR_Rep2_read1.fastq.gz, /.../HBR_Rep2_read2.fastq.gz]]\n[[id:UHR, single_end:false], [/.../UHR_Rep1_read1.fastq.gz, /.../UHR_Rep1_read2.fastq.gz], [/.../UHR_Rep2_read1.fastq.gz, /.../UHR_Rep2_read2.fastq.gz]]\n\n\n\nIn the output, notice that while all FASTQ files that contain the same metadata are grouped together, they are still nested as [ [s1r1, s1r2], [s2r1, s2r2] ].\nExercise: Using the nf-core operator documentation, can you find a suitable operator that will un-nest the FASTQ files such that the output structure is [ s1r1, s1r2, s2r1, s2r2 ]? Can you use it in combination with a new map { } function to un-nest just the FASTQ files (and not the metadata list)? Test it out by adding it after the groupTuple() operator and view the result.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe following map function structure can be used\n    ch_samplesheet\n        .map { meta, fqs -&gt;\n            meta = meta - meta.subMap('rep') \n            [meta, fqs]\n        }\n        .groupTuple()\n        .map { meta, fqs -&gt; \n            &lt;OPERATOR_COMMAND_HERE&gt;\n            [meta, fqs]\n        }\n        .view()\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe flatten operator can be used to achieve this.\n    ch_samplesheet\n        .map { meta, fqs -&gt;\n            meta = meta - meta.subMap('rep') \n            [meta, fqs]\n        }\n        .groupTuple()\n        .map { meta, fqs -&gt; \n            fqs = fqs.flatten()\n            [meta, fqs]\n        }\n        .view()\nThe FASTQ files have now been flattened:\n[[id:ERC, single_end:false], [/.../HBR_Rep1_read1.fastq.gz, /.../HBR_Rep1_read2.fastq.gz, /.../HBR_Rep2_read1.fastq.gz, /.../HBR_Rep2_read2.fastq.gz]]\n[[id:UHR, single_end:false], [/.../UHR_Rep1_read1.fastq.gz, /.../UHR_Rep1_read2.fastq.gz, /.../UHR_Rep2_read1.fastq.gz, /.../UHR_Rep2_read2.fastq.gz]]\n\n\n\n\n\n6.2.3 Creating a custom nf-core module\nTo concatenate the FASTQ files together, let’s create a custom module. This can be done with nf-core modules create\nWhen prompted, the tool cat can be specified.\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n    There is a new version of nf-core/tools available! (3.2.1)\n\n\nINFO     Repository type: pipeline                                                                                                                                  \nINFO     Press enter to use default values (shown in brackets) or type your own responses. ctrl+click underlined text to open links.                                \nName of tool/subtool: cat\nNotice that nf-core will automatically locate the corresponding bioconda package and singularity or Docker container.\nINFO     Using Bioconda package: 'bioconda::cat=6.0.1'                                                                                                              \nINFO     Using Docker container: 'biocontainers/cat:6.0.1--hdfd78af_0'                                                                                              \nINFO     Using Singularity container: 'https://depot.galaxyproject.org/singularity/cat:6.0.1--hdfd78af_0'    \nProgress through the prompts, setting your GitHub username as the author, and accept any default settings. When this command has completed successfully, a list of created files along with environment files will be displayed.\nINFO     Created component template: 'cat'                                                                             \nINFO     Created following files:                                                                                      \n           modules/local/cat/main.nf                                                                                   \n           modules/local/cat/meta.yml                                                                                  \n           modules/local/cat/environment.yml                                                                           \n           modules/local/cat/tests/main.nf.test    \nOpen the modules/local/cat/main.nf file. Notice that the conda environment file and container repositories have already been included. Some example input and outputs have also been created.\n    conda \"${moduleDir}/environment.yml\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cat:6.0.1--hdfd78af_0':\n        'biocontainers/cat:6.0.1--hdfd78af_0' }\"\nExercise:\n\nRemove the stub block for now\nEdit the input block such that it takes an input of type tuple – the first element in the tuple is the metadata, and the second element are FASTQ files.\nEdit the script block such that all read_1 FASTQ files will be concatenated together, and all read_2 FASTQ files will be concatenated together\nEdit the output block such that an output of type tuple will be emitted as merged from the process – the first element in the tuple is the metadata and the second element contains the FASTQ files\n\n\n\n\n\n\n\nTO DO\n\n\n\nADD SOLUTION\nADD outputs of merging as inputs to existing SALMON"
  },
  {
    "objectID": "workshops/6.1_operators.html",
    "href": "workshops/6.1_operators.html",
    "title": "Nextflow Development - Channel Operators",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of Nextflow channel operators"
  },
  {
    "objectID": "workshops/6.1_operators.html#environment-setup",
    "href": "workshops/6.1_operators.html#environment-setup",
    "title": "Nextflow Development - Channel Operators",
    "section": "Environment Setup",
    "text": "Environment Setup\nSet up an interactive shell to run our Nextflow workflow:\nsrun --pty -p prod_short --mem 8GB --mincpus 2 -t 0-2:00 bash\nLoad the required modules to run Nextflow:\nmodule load nextflow/23.04.1\nmodule load singularity/3.7.3\nSet the singularity cache environment variable:\nexport NXF_SINGULARITY_CACHEDIR=/config/binaries/singularity/containers_devel/nextflow\nSingularity images downloaded by workflow executions will now be stored in this directory.\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session. A complete list of environment variables can be found here.\nThe training data can be cloned from:\ngit clone https://github.com/nextflow-io/training.git"
  },
  {
    "objectID": "workshops/6.1_operators.html#rna-seq-workflow-and-module-files",
    "href": "workshops/6.1_operators.html#rna-seq-workflow-and-module-files",
    "title": "Nextflow Development - Channel Operators",
    "section": "RNA-seq Workflow and Module Files ",
    "text": "RNA-seq Workflow and Module Files \nPreviously, we created three Nextflow files and one config file:\n├── nextflow.config\n├── rnaseq.nf\n├── modules.nf\n└── modules\n    └── trimgalore.nf\n\nrnaseq.nf: main workflow script where parameters are defined and processes were called.\n\n#!/usr/bin/env nextflow\n\nparams.reads = \"/scratch/users/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\nmodules.nf: script containing the majority of modules, including INDEX, QUANTIFICATION, FASTQC, and MULTIQC\n\nprocess INDEX {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nprocess FASTQC {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\nmodules/trimgalore.nf: script inside a modules folder, containing only the TRIMGALORE process\n\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n\n  input:\n    tuple val(sample_id), path(reads)\n  \n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n}\n\nnextflow.config: config file that enables singularity\n\nsingularity {\n  enabled = true\n  autoMounts = true\n  cacheDir = \"/config/binaries/singularity/containers_devel/nextflow\"\n}\nRun the pipeline, specifying --outdir:\n&gt;&gt;&gt; nextflow run rnaseq.nf --outdir output\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [soggy_jennings] DSL2 - revision: 87afc1d98d\nexecutor &gt;  local (16)\n[93/d37ef0] process &gt; INDEX          [100%] 1 of 1 ✔\n[b3/4c4d9c] process &gt; QT (1)         [100%] 3 of 3 ✔\n[d0/173a6e] process &gt; FASTQC_one (3) [100%] 3 of 3 ✔\n[58/0b8af2] process &gt; TRIMGALORE (3) [100%] 3 of 3 ✔\n[c6/def175] process &gt; FASTQC_two (3) [100%] 3 of 3 ✔\n[e0/bcf904] process &gt; MULTIQC (3)    [100%] 3 of 3 ✔"
  },
  {
    "objectID": "workshops/6.1_operators.html#map",
    "href": "workshops/6.1_operators.html#map",
    "title": "Nextflow Development - Channel Operators",
    "section": "6.1.1 map ",
    "text": "6.1.1 map \nThe map operator applies a mapping function to each item in a channel. This function is expressed using the Groovy closure { }.\nChannel\n    .of('hello', 'world')\n    .map { word -&gt; \n        def word_size = word.size()\n        [word, word_size] \n    }\n    .view()\nIn this example, a channel containing the strings hello and world is created.\nInside the map operator, the local variable word is declared, and used to represent each input value that is passed to the function, ie. each element in the channel, hello and world.\nThe map operator ‘loops’ through each element in the channel and assigns that element to the local varialbe word. A new local variable word_size is defined inside the map function, and calculates the length of the string using size(). Finally, a tuple is returned, where the first element is the string represented by the local word variable, and the second element is the length of the string, represented by the local word_size variable.\nOutput:\n[hello, 5]\n[world, 5]\nFor our RNA-seq pipeline, let’s first create separate transcriptome files for each organ: lung.transcriptome.fa, liver.transcriptome.fa, gut.transcriptome.fa\ncp \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\" \"/scratch/users/.../training/nf-training/data/ggal/lung.transcriptome.fa\"\ncp \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\" \"/scratch/users/.../training/nf-training/data/ggal/liver.transcriptome.fa\"\nmv \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\" \"/scratch/users/.../training/nf-training/data/ggal/gut.transcriptome.fa\"\nEnsure transcriptome.fa no longer exists:\n&gt;&gt;&gt; ls /scratch/users/.../training/nf-training/data/ggal/\ngut_1.fq\ngut_2.fq\ngut.transcriptome.fa\nliver_1.fq\nliver_2.fq\nliver.transcriptome.fa\nlung_1.fq\nlung_2.fq\nlung.transcriptome.fa\nExercise\nCurrently in the rnaseq.nf script, we define the transcriptome_file parameter to be a single file.\nparams.transcriptome_file = \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\"\nSet the transcriptome_file parameter to match for all three .fa files using a glob path matcher.\nUse the fromPath channel factory to read in the transcriptome files, and the map operator to create a tuple where the first element is the sample (organ type) of the .fa, and the second element is the path of the .fa file. Assign the final output to be a channel called transcriptome_ch.\nThe getSimpleName() Groovy method can be used extract the sample name from our .fa file, for example:\ndef sample = fasta.getSimpleName()\nUse the view() channel operator to view the transcriptome_ch channel. The expected output:\n[lung, /scratch/users/.../training/nf-training/data/ggal/lung.transcriptome.fa]\n[liver, /scratch/users/.../training/nf-training/data/ggal/liver.transcriptome.fa]\n[gut, /scratch/users/.../training/nf-training/data/ggal/gut.transcriptome.fa]\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe transcriptome_file parameter is defined using *, using glob to match for all three .fa files. The fromPath channel factory is used to read the .fa files, and the map operator is used to create the tuple.\nIn the map function, the variable file was chosen to represent each element that is passed to the function. The function emits a tuple where the first element is the sample name, returned by the getSimpleName() method, and the second element is the .fa file path.\nparams.transcriptome_file = \"/scratch/users/.../nf-training/data/ggal/*.fa\"\n\ntranscriptome_ch = Channel.fromPath(\"$params.transcriptome_file\")\n    .map { fasta -&gt; \n    def sample = fasta.getSimpleName()\n    [sample, fasta]\n    }\n    .view()\n\n\n\n\nChallenge\nModify the INDEX process to match the input structure of transcriptome_ch. Modify the output of INDEX so that a tuple is emitted, where the first elememt is the value of the grouping key, and the second element is the path of the salmon_idx folder.\nIndex the transcriptome_ch using the INDEX process. Emit the output as index_ch.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe input is now defined to be a tuple of two elements, where the first element is the grouping key and the second element is the path of the transcriptome file.\nprocess INDEX {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    tuple val(sample_id), path(transcriptome)\n\n    output:\n    tuple val(sample_id), path(\"salmon_idx\")\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\nInside the workflow block, transcriptome_ch is used as input into the INDEX process. The process outputs are emitted as index_ch\nworkflow {\n  index_ch = INDEX(transcriptome_ch)\n  index_ch.view()\n}\nThe index_ch channel is now a tuple where the first element is the grouping key, and the second element is the path to the salmon index folder.\n&gt;&gt;&gt; nextflow run rnaseq.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [dreamy_linnaeus] DSL2 - revision: b4ec1d02bd\n[21/91088a] process &gt; INDEX (3) [100%] 3 of 3\n[liver, /scratch/users/.../work/06/f0a54ba9191cce9f73f5a97bfb7bea/salmon_idx]\n[lung, /scratch/users/.../work/60/e84b1b1f06c43c8cf69a5c621d5a41/salmon_idx]\n[gut, /scratch/users/.../work/21/91088aafb553cb4b933bc2b3493f33/salmon_idx]\n\n\n\nCopy the new INDEX process into modules.nf. In the workflow block of rnaseq.nf, use transcriptome_ch as the input to the process INDEX."
  },
  {
    "objectID": "workshops/6.1_operators.html#combine",
    "href": "workshops/6.1_operators.html#combine",
    "title": "Nextflow Development - Channel Operators",
    "section": "6.1.2 combine ",
    "text": "6.1.2 combine \nThe combine operator produces the cross product (ie. outer product) combinations of two source channels.\nFor example: The words channel is combined with the numbers channel, emitting a channel where each element of numbers is paired with each element of words.\nnumbers = Channel.of(1, 2, 3)\nwords = Channel.of('hello', 'ciao')\n\nnumbers.combine(words).view()\nOutput:\n[1, hello]\n[2, hello]\n[3, hello]\n[1, ciao]\n[2, ciao]\n[3, ciao]\nThe by option can be used to combine items that share a matching key. This value is zero-based, and represents the index or list of indices for the grouping key. The emitted tuple will consist of multiple elements.\nFor example: source and target are channels consisting of multiple tuples, where the first element of each tuple represents the grouping key. Since indexing is zero-based, by is set to 0 to represent the first element of the tuple.\nsource = Channel.of( [1, 'alpha'], [2, 'beta'] )\ntarget = Channel.of( [1, 'x'], [1, 'y'], [1, 'z'], [2, 'p'], [2, 'q'], [2, 't'] )\n\nsource.combine(target, by: 0).view()\nEach value within the source and target channels are separate elements, resulting in the emitted tuple each containing 3 elements:\n[1, alpha, x]\n[1, alpha, y]\n[1, alpha, z]\n[2, beta, p]\n[2, beta, q]\n[2, beta, t]\nExercise\nIn our RNA-seq pipeline, create a channel quant_inputs_ch that contains the reads_ch combined with the index_ch via a matching key. The emitted channel should contain three elements, where the first element is the grouping key, the second element is the path to the salmon index folder, and the third element is a list of the .fq pairs.\nThe expected output:\n[liver, /scratch/users/.../work/cf/42458b80e050a466d62baf99d0c1cf/salmon_idx, [/scratch/users/.../training/nf-training/data/ggal/liver_1.fq, /scratch/users/.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, /scratch/users/.../work/64/90a77a5f1ed5a0000f6620fd1fab9a/salmon_idx, [/scratch/users/.../training/nf-training/data/ggal/lung_1.fq, /scratch/users/.../training/nf-training/data/ggal/lung_2.fq]]\n[gut, /scratch/users/.../work/37/352b00bfb71156a9250150428ddf1d/salmon_idx, [/scratch/users/.../training/nf-training/data/ggal/gut_1.fq, /scratch/users/.../training/nf-training/data/ggal/gut_2.fq]]\nUse quant_inputs_ch as the input for the QT process within the workflow block.\nModify the process such that the input will be a tuple consisting of three elements, where the first element is the grouping key, the second element is the salmon index and the third element is the list of .fq reads. Also modify the output of the QT process to emit a tuple of two elements, where the first element is the grouping key and the second element is the $sample_id folder. Emit the process output as quant_ch in the workflow block.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe reads_ch is combined with the index_ch using the combine channel operator with by: 0, and is assigned to the channel quant_inputs_ch. The new quant_inputs_ch channel is input into the QT process.\nworkflow {\n  index_ch = INDEX(transcriptome_ch)\n\n  quant_inputs_ch = index_ch.combine(reads_ch, by: 0)\n  quant_ch = QT(quant_inputs_ch)\n}\nIn te QT process, the input has been modified to be a tuple of three elements - the first element is the grouping key, the second element is the path to the salmon index, and the third element is the list of .fq reads.\nprocess QUANTIFICATION {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    tuple val(sample_id), path(salmon_index), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"$sample_id\")\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}"
  },
  {
    "objectID": "workshops/6.1_operators.html#grouptuple",
    "href": "workshops/6.1_operators.html#grouptuple",
    "title": "Nextflow Development - Channel Operators",
    "section": "6.1.3 groupTuple ",
    "text": "6.1.3 groupTuple \nThe groupTuple operator collects tuples into groups based on a similar grouping key, emitting a new tuple for each distinct key. The groupTuple differs from the combine operator in that it is performed on one input channel, and the matching values are emitted as a list.\nChannel.of( [1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'] )\n    .groupTuple()\n    .view()\nOutput:\n[1, [A, B, C]]\n[2, [C, A]]\n[3, [B, D]]\nBy default, the first element of each tuple is used as the grouping key. The by option can be used to specify a different index. For example, to group by the second element of each tuple:\nChannel.of( [1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'] )\n    .groupTuple(by: 1)\n    .view()\n[[1, 2], A]\n[[1, 3], B]\n[[2, 1], C]\n[[3], D]\n\nIn the workflow script rnaseq.nf we defined the reads parameter to be multiple paired .fq files that are created into a channel using the fromFilePairs channel factory. This created a tuple where the first element is a unique grouping key, created automatically based on similarities in file name, and the second element contains the list of paired files.\n#!/usr/bin/env nextflow\n\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\nNow, move the /scratch/users/.../nf-training/data/ggal/lung_2.fq file into another directory so the folder contains one lung .fq file:\n&gt;&gt;&gt; mv /scratch/users/.../training/nf-training/data/ggal/lung_2.fq .\n&gt;&gt;&gt; ls /scratch/users/.../training/nf-training/data/ggal\ngut_1.fq\ngut_2.fq\ngut.transcriptome.fa\nliver_1.fq\nliver_2.fq\nliver.transcriptome.fa\nlung_1.fq\nlung.transcriptome.fa\nExercise\nUse the fromPath channel factory to read all .fq files as separate elements.\nThen, use map to create a mapping function that returns a tuple, where the first element is the grouping key, and the second element is the .fq file(s).\nThen, use groupTuple() to create channels containing both single and paired .fq files. Within the groupTuple() operator, set sort: true, which orders the groups numerically, ensuring the first .fq is first.\nExpected output:\n[lung, [/scratch/users/.../training/nf-training/data/ggal/lung_1.fq]]\n[gut, [/scratch/users/.../training/nf-training/data/ggal/gut_1.fq, /scratch/users/.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/scratch/users/.../training/nf-training/data/ggal/liver_1.fq, /scratch/users/.../training/nf-training/data/ggal/liver_2.fq]]\nInside the map function, the following can be used to extract the sample name from the .fq files. file is the local variable defined inside the function that represents each .fq file. The getName() method will return the file name without the full path, and replaceAll is used to remove the _2.fq and _1.fq file suffixes.\ndef group_key = file.getName().replaceAll(/_2.fq/,'').replaceAll(/_1.fq/,'')\nFor a full list of Nextflow file attributes, see here.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe fromPath channel is used to read all .fq files separately. The map function is then used to create a two-element tuple where the first element is a grouping key and the second element is the list of .fq file(s).\nreads_ch = Channel.fromPath(\"/home/sli/nextflow_training/training/nf-training/data/ggal/*.fq\")\n  .map { file -&gt;\n    def group_key = file.getName().replaceAll(/_2.fq/,'').replaceAll(/_1.fq/,'')\n    [group_key, file]\n  }\n  .groupTuple(sort: true)\n  .view()\n\n\n\nNow, run the workflow up to the combine step. The quant_inputs_ch should now consist of:\n[liver, /scratch/users/.../work/cf/42458b80e050a466d62baf99d0c1cf/salmon_idx, [/scratch/users/.../nf-training/data/ggal/liver_1.fq, /scratch/users/.../nf-training/data/ggal/liver_2.fq]]\n[lung, /scratch/users/.../work/64/90a77a5f1ed5a0000f6620fd1fab9a/salmon_idx, [/scratch/users/.../nf-training/data/ggal/lung_1.fq]]\n[gut, /scratch/users/.../work/37/352b00bfb71156a9250150428ddf1d/salmon_idx, [/scratch/users/.../nf-training/data/ggal/gut_1.fq, /scratch/users/.../nf-training/data/ggal/gut_2.fq]]"
  },
  {
    "objectID": "workshops/6.1_operators.html#flatten",
    "href": "workshops/6.1_operators.html#flatten",
    "title": "Nextflow Development - Channel Operators",
    "section": "6.1.4 flatten ",
    "text": "6.1.4 flatten \nThe flatten operator flattens each item from a source channel and emits the elements separately. Deeply nested inputs are also flattened.\nChannel.of( [1, [2, 3]], 4, [5, [6]] )\n    .flatten()\n    .view()\nOutput:\n1\n2\n3\n4\n5\n6\n\nWithin the script block of the QUANTIFICATION process in the RNA-seq pipeline, we are assuming the reads are paired, and specify -1 ${reads[0]} -2 ${reads[1]} as inputs to salmon quant.\nprocess QUANTIFICATION {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    tuple val(sample_id), path(salmon_index), path(reads)\n\n    output:\n    tuple val(sample_id) path(\"$sample_id\")\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\nNow that the input reads can be either single or paired, the QUANTIFICATION process needs to be modified to allow for either input type. This can be done using the flatten() operator, and conditional script statements. Additionally, the size() method can be used to calculate the size of a list.\nThe script block can be changed to the following:\n    script:\n    def input_reads = [reads]\n    if( input_reads.flatten().size() == 1 )\n        \"\"\"\n        salmon quant --threads $task.cpus --libType=U \\\n        -i $salmon_index -r $reads -o $sample_id\n        \"\"\"\n    else \n        \"\"\"\n        salmon quant --threads $task.cpus --libType=U \\\\\n        -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n        \"\"\"\nFirst, a new variable input_reads is defined, which consists of the reads input being converted into a list. This has to be done since Nextflow will automatically convert a list of length 1 into a path within process. If the size() method was used on a path type input, it will return the size of the file in bytes, and not the list size. Therefore, all inputs must first be converted into a list in order to correctly caculate the number of files.\ndef input_reads = [reads]\nFor reads that are already in a list (ie. paired reads), this will nest the input into another list, for example:\n[ [ file1, file2 ] ]\nIf the size() operator is used on this input, it will always return 1 since the encompassing list only contains one element. Therefore, the flatten() operator has to be used to emit the files as separate elements.\nThe final definition to obtain the number of files in reads becomes:\ninput_reads.flatten().size()\nFor single reads, the input to salmon quant becomes -r $reads\n\nExercise\nCurrently the TRIMGALORE process only accounts for paired reads.\nprocess TRIMGALORE {\n    container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n\n    input:\n        tuple val(sample_id), path(reads)\n    \n    output:\n        tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n        tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n        tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n        tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n        tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n    script:\n        \"\"\"\n        trim_galore \\\\\n        --paired \\\\\n        --gzip \\\\\n        ${reads[0]} \\\\\n        ${reads[1]}\n        \"\"\"\n}\nModify the process such that both single and paired reads can be used. For single reads, the following script block can be used:\n\"\"\"\ntrim_galore \\\\\n  --gzip \\\\\n  $reads\n\"\"\"\n\n\n\n\n\n\nSolution\n\n\n\n\n\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n\n  input:\n    tuple val(sample_id), path(reads)\n  \n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n  def input_reads = [reads]\n\n  if( input_reads.flatten().size() == 1 )\n    \"\"\"\n    trim_galore \\\\\n      --gzip \\\\\n      $reads\n    \"\"\"\n  else\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n\n}\n\n\n\nExtension\nModify the FASTQC process such that the output is a tuple where the first element is the grouping key, and the second element is the path to the fastqc logs.\nModify the MULTIQC process such that the output is a tuple where the first element is the grouping key, and the second element is the path to the generated html file.\nFinally, run the entire workflow, specifying an --outdir. The workflow block should look like this:\nworkflow {\n  index_ch = INDEX(transcriptome_ch)\n\n  quant_inputs_ch = index_ch.combine(reads_ch, by: 0)\n  quant_ch = QT(quant_inputs_ch)\n\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n\n  fastqc_ch = FASTQC_one(reads_ch)\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe output block of both processes have been modified to be tuples containing a grouping key.\nprocess FASTQC {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"fastqc_${sample_id}_logs\")\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(quantification)\n    tuple val(sample_id), path(fastqc)\n\n    output:\n    tuple val(sample_id), path(\"*.html\")\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\n\n\n\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, Nextflow Patterns materials from Nextflow, nf-core nf-core tools documentation and nf-validation"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html",
    "href": "workshops/8.1_scatter_gather_output.html",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of how to structure nextflow published outputs\nGain an understanding of how to do scatter & gather processes"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#environment-setup",
    "href": "workshops/8.1_scatter_gather_output.html#environment-setup",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "Environment Setup",
    "text": "Environment Setup\nSet up an interactive shell to run our Nextflow workflow:\nsrun --pty -p prod_short --mem 8GB --mincpus 2 -t 0-2:00 bash\nLoad the required modules to run Nextflow:\nmodule load nextflow/23.04.1\nmodule load singularity/3.7.3\nSet the singularity cache environment variable:\nexport NXF_SINGULARITY_CACHEDIR=/config/binaries/singularity/containers_devel/nextflow\nSingularity images downloaded by workflow executions will now be stored in this directory.\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session. A complete list of environment variables can be found here.\nThe training data can be cloned from:\ngit clone https://github.com/nextflow-io/training.git"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#rna-seq-workflow-and-module-files",
    "href": "workshops/8.1_scatter_gather_output.html#rna-seq-workflow-and-module-files",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "RNA-seq Workflow and Module Files ",
    "text": "RNA-seq Workflow and Module Files \nPreviously, we created three Nextflow files and one config file:\n├── nextflow.config\n├── rnaseq.nf\n├── modules.nf\n└── modules\n    └── trimgalore.nf\n\nrnaseq.nf: main workflow script where parameters are defined and processes were called.\n\n#!/usr/bin/env nextflow\n\nparams.reads = \"/scratch/users/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\nmodules.nf: script containing the majority of modules, including INDEX, QUANTIFICATION, FASTQC, and MULTIQC\n\nprocess INDEX {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nprocess FASTQC {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\nmodules/trimgalore.nf: script inside a modules folder, containing only the TRIMGALORE process\n\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n\n  input:\n    tuple val(sample_id), path(reads)\n  \n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n}\n\nnextflow.config: config file that enables singularity\n\nsingularity {\n  enabled = true\n  autoMounts = true\n  cacheDir = \"/config/binaries/singularity/containers_devel/nextflow\"\n}\nRun the pipeline, specifying --outdir:\n&gt;&gt;&gt; nextflow run rnaseq.nf --outdir output\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [soggy_jennings] DSL2 - revision: 87afc1d98d\nexecutor &gt;  local (16)\n[93/d37ef0] process &gt; INDEX          [100%] 1 of 1 ✔\n[b3/4c4d9c] process &gt; QT (1)         [100%] 3 of 3 ✔\n[d0/173a6e] process &gt; FASTQC_one (3) [100%] 3 of 3 ✔\n[58/0b8af2] process &gt; TRIMGALORE (3) [100%] 3 of 3 ✔\n[c6/def175] process &gt; FASTQC_two (3) [100%] 3 of 3 ✔\n[e0/bcf904] process &gt; MULTIQC (3)    [100%] 3 of 3 ✔"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#organise-outputs",
    "href": "workshops/8.1_scatter_gather_output.html#organise-outputs",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.1. Organise outputs",
    "text": "8.1. Organise outputs\nThe output declaration block defines the channels used by the process to send out the results produced. However, this output only stays in the work/ directory if there is no publishDir directive specified.\nGiven each task is being executed in separate temporary work/ folder (e.g., work/f1/850698…), you may want to save important, non-intermediary, and/or final files in a results folder.\nTo store our workflow result files, you need to explicitly mark them using the directive publishDir in the process that’s creating the files. For example:\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\nThe above example will copy all html files created by the MULTIQC process into the directory path specified in the params.outdir"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#store-outputs-matching-a-glob-pattern",
    "href": "workshops/8.1_scatter_gather_output.html#store-outputs-matching-a-glob-pattern",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.1.1. Store outputs matching a glob pattern",
    "text": "8.1.1. Store outputs matching a glob pattern\nYou can use more than one publishDir to keep different outputs in separate directories. For each directive specify a different glob pattern using the pattern option to store into each directory only the files that match the provided pattern.\nFor example:\nreads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    publishDir \"results/bam\", pattern: \"*.bam\"\n    publishDir \"results/bai\", pattern: \"*.bai\"\n\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path(\"*.bam\")\n    tuple val(sample_id), path(\"*.bai\")\n\n    script:\n    \"\"\"\n    echo your_command_here --sample $sample_id_paths &gt; ${sample_id}.bam\n    echo your_command_here --sample $sample_id_paths &gt; ${sample_id}.bai\n    \"\"\"\n}\nExercise\nUse publishDir and pattern to keep the outputs from the trimgalore.nf into separate directories.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n  publishDir \"$params.outdir/report\", mode: \"copy\", pattern:\"*report.txt\"\n  publishDir \"$params.outdir/trimmed_fastq\", mode: \"copy\", pattern:\"*fq.gz\"\n\n  input:\n    tuple val(sample_id), path(reads)\n  \n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n}\nOutput should now look like\n&gt;&gt;&gt; tree ./output\n./output\n├── gut.html\n├── liver.html\n├── lung.html\n├── report\n│   ├── gut_1.fq_trimming_report.txt\n│   ├── gut_2.fq_trimming_report.txt\n│   ├── liver_1.fq_trimming_report.txt\n│   ├── liver_2.fq_trimming_report.txt\n│   ├── lung_1.fq_trimming_report.txt\n│   └── lung_2.fq_trimming_report.txt\n└── trimmed_fastq\n    ├── gut_1_val_1.fq.gz\n    ├── gut_2_val_2.fq.gz\n    ├── liver_1_val_1.fq.gz\n    ├── liver_2_val_2.fq.gz\n    ├── lung_1_val_1.fq.gz\n    └── lung_2_val_2.fq.gz\n\n2 directories, 15 files"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#store-outputs-renaming-files-or-in-a-sub-directory",
    "href": "workshops/8.1_scatter_gather_output.html#store-outputs-renaming-files-or-in-a-sub-directory",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.1.2. Store outputs renaming files or in a sub-directory",
    "text": "8.1.2. Store outputs renaming files or in a sub-directory\nThe publishDir directive also allow the use of saveAs option to give each file a name of your choice, providing a custom rule as a closure.\nprocess foo {\n  publishDir 'results', saveAs: { filename -&gt; \"foo_$filename\" }\n\n  output: \n  path '*.txt'\n\n  '''\n  touch this.txt\n  touch that.txt\n  '''\n}\nThe same pattern can be used to store specific files in separate directories depending on the actual name.\nprocess foo {\n  publishDir 'results', saveAs: { filename -&gt; filename.endsWith(\".zip\") ? \"zips/$filename\" : filename }\n\n  output: \n  path '*'\n\n  '''\n  touch this.txt\n  touch that.zip\n  '''\n}\nExercise\nModify the MULTIQC output with saveAs such that resulting folder is as follow:\n./output\n├── MultiQC\n│   ├── multiqc_gut.html\n│   ├── multiqc_liver.html\n│   └── multiqc_lung.html\n├── report\n│   ├── gut_1.fq_trimming_report.txt\n│   ├── gut_2.fq_trimming_report.txt\n│   ├── liver_1.fq_trimming_report.txt\n│   ├── liver_2.fq_trimming_report.txt\n│   ├── lung_1.fq_trimming_report.txt\n│   └── lung_2.fq_trimming_report.txt\n└── trimmed_fastq\n    ├── gut_1_val_1.fq.gz\n    ├── gut_2_val_2.fq.gz\n    ├── liver_1_val_1.fq.gz\n    ├── liver_2_val_2.fq.gz\n    ├── lung_1_val_1.fq.gz\n    └── lung_2_val_2.fq.gz\n\n3 directories, 15 files\n\n\n\n\n\n\nWarning\n\n\n\nYou need to remove existing output folder/files if you want to have a clean output. By default, nextflow will overwrite existing files, and keep all the remaining files in the same specified output directory.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy', saveAs: { filename -&gt; filename.endsWith(\".html\") ? \"MultiQC/multiqc_$filename\" : filename }\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\n\n\nChallenge\nModify all the processes in rnaseq.nf such that we will have the following output structure\n./output\n├── gut\n│   ├── QC\n│   │   ├── fastqc_gut_logs\n│   │   │   ├── gut_1_fastqc.html\n│   │   │   ├── gut_1_fastqc.zip\n│   │   │   ├── gut_2_fastqc.html\n│   │   │   └── gut_2_fastqc.zip\n│   │   └── gut.html\n│   ├── report\n│   │   ├── gut_1.fq_trimming_report.txt\n│   │   └── gut_2.fq_trimming_report.txt\n│   └── trimmed_fastq\n│       ├── gut_1_val_1.fq.gz\n│       └── gut_2_val_2.fq.gz\n├── liver\n│   ├── QC\n│   │   ├── fastqc_liver_logs\n│   │   │   ├── liver_1_fastqc.html\n│   │   │   ├── liver_1_fastqc.zip\n│   │   │   ├── liver_2_fastqc.html\n│   │   │   └── liver_2_fastqc.zip\n│   │   └── liver.html\n│   ├── report\n│   │   ├── liver_1.fq_trimming_report.txt\n│   │   └── liver_2.fq_trimming_report.txt\n│   └── trimmed_fastq\n│       ├── liver_1_val_1.fq.gz\n│       └── liver_2_val_2.fq.gz\n└── lung\n    ├── QC\n    │   ├── fastqc_lung_logs\n    │   │   ├── lung_1_fastqc.html\n    │   │   ├── lung_1_fastqc.zip\n    │   │   ├── lung_2_fastqc.html\n    │   │   └── lung_2_fastqc.zip\n    │   └── lung.html\n    ├── report\n    │   ├── lung_1.fq_trimming_report.txt\n    │   └── lung_2.fq_trimming_report.txt\n    └── trimmed_fastq\n        ├── lung_1_val_1.fq.gz\n        └── lung_2_val_2.fq.gz\n\n15 directories, 27 files\n\n\n\n\n\n\nSolution\n\n\n\n\n\nprocess FASTQC {\n    publishDir \"$params.outdir/$sample_id/QC\", mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    //publishDir params.outdir, mode:'copy', saveAs: { filename -&gt; filename.endsWith(\".html\") ? \"MultiQC/multiqc_$filename\" : filename }\n    publishDir \"$params.outdir/$quantification/QC\", mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img'\n  publishDir \"${params.outdir}/${sample_id}/report\", mode: \"copy\", pattern:\"*report.txt\"\n  publishDir \"${params.outdir}/${sample_id}/trimmed_fastq\", mode: \"copy\", pattern:\"*fq.gz\"\n\n  input:\n    tuple val(sample_id), path(reads)\n\n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n}"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#scatter",
    "href": "workshops/8.1_scatter_gather_output.html#scatter",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.2 Scatter",
    "text": "8.2 Scatter\nThe scatter operation involves distributing large input data into smaller chunks that can be analysed across multiple processes in parallel.\nOne very simple example of native scatter is how nextflow handles Channel factories with the Channel.fromPath or Channel.fromFilePairs method, where multiple input data is processed in parallel.\nparams.reads = \"/scratch/users/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { FASTQC as FASTQC_one } from './modules.nf'\n\nworkflow {\n  fastqc_ch = FASTQC_one(reads_ch)\n}\nFrom the above snippet from our rnaseq.nf, we will get three execution of FASTQC_one for each pairs of our input data.\nOther than natively splitting execution by input data, Nextflow also provides operators to scatter existing input data for various benefits, such as faster processing. For example:\n\nsplitText\nsplitFasta\nsplitFastq\nmap with from or fromList\nflatten"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#process-per-file-chunk",
    "href": "workshops/8.1_scatter_gather_output.html#process-per-file-chunk",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.2.1 Process per file chunk",
    "text": "8.2.1 Process per file chunk\nExercise\nparams.infile = \"/data/reference/bed_files/Agilent_CRE_v2/S30409818_Covered_MERGED.bed\"\nparams.size = 100000\n\nprocess count_line {\n  debug true\n  input: \n  file x\n\n  script:\n  \"\"\"\n  wc -l $x \n  \"\"\"\n}\n\nworkflow {\n  Channel.fromPath(params.infile) \\\n    | splitText(by: params.size, file: true) \\\n    | count_line\n}\nExercise\nparams.infile = \"/scratch/users/rlupat/nfWorkshop/dev1/training/nf-training/data/ggal/*_{1,2}.fq\"\nparams.size = 1000\n\nworkflow {\n  Channel.fromFilePairs(params.infile, flat: true) \\\n    | splitFastq(by: params.size, pe: true, file: true) \\\n    | view()\n}"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#process-per-file-range",
    "href": "workshops/8.1_scatter_gather_output.html#process-per-file-range",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.2.1 Process per file range",
    "text": "8.2.1 Process per file range\nExercise\nChannel.from(1..22) \\\n   | map { chr -&gt; [\"sample${chr}\", file(\"${chr}.indels.vcf\"), file(\"${chr}.vcf\")] } \\\n   | view()\n&gt;&gt; nextflow run test_scatter.nf\n\n[sample1, /scratch/users/${users}/1.indels.vcf, /scratch/users/${users}/1.vcf]\n[sample2, /scratch/users/${users}/2.indels.vcf, /scratch/users/${users}/2.vcf]\n[sample3, /scratch/users/${users}/3.indels.vcf, /scratch/users/${users}/3.vcf]\n[sample4, /scratch/users/${users}/4.indels.vcf, /scratch/users/${users}/4.vcf]\n[sample5, /scratch/users/${users}/5.indels.vcf, /scratch/users/${users}/5.vcf]\n[sample6, /scratch/users/${users}/6.indels.vcf, /scratch/users/${users}/6.vcf]\n[sample7, /scratch/users/${users}/7.indels.vcf, /scratch/users/${users}/7.vcf]\n[sample8, /scratch/users/${users}/8.indels.vcf, /scratch/users/${users}/8.vcf]\n[sample9, /scratch/users/${users}/9.indels.vcf, /scratch/users/${users}/9.vcf]\n[sample10, /scratch/users${users}/10.indels.vcf, /scratch/users${users}/10.vcf]\n[sample11, /scratch/users${users}/11.indels.vcf, /scratch/users${users}/11.vcf]\n[sample12, /scratch/users${users}/12.indels.vcf, /scratch/users${users}/12.vcf]\n[sample13, /scratch/users${users}/13.indels.vcf, /scratch/users${users}/13.vcf]\n[sample14, /scratch/users${users}/14.indels.vcf, /scratch/users${users}/14.vcf]\n[sample15, /scratch/users${users}/15.indels.vcf, /scratch/users${users}/15.vcf]\n[sample16, /scratch/users${users}/16.indels.vcf, /scratch/users${users}/16.vcf]\n[sample17, /scratch/users${users}/17.indels.vcf, /scratch/users${users}/17.vcf]\n[sample18, /scratch/users${users}/18.indels.vcf, /scratch/users${users}/18.vcf]\n[sample19, /scratch/users${users}/19.indels.vcf, /scratch/users${users}/19.vcf]\n[sample20, /scratch/users${users}/20.indels.vcf, /scratch/users${users}/20.vcf]\n[sample21, /scratch/users${users}/21.indels.vcf, /scratch/users${users}/21.vcf]\n[sample22, /scratch/users${users}/22.indels.vcf, /scratch/users${users}/22.vcf]\nExercise\nparams.infile = \"/data/reference/bed_files/Agilent_CRE_v2/S30409818_Covered_MERGED.bed\"\nparams.size = 100000\n\nprocess split_bed_by_chr {\n  debug true\n\n  input:\n  path bed\n  val chr\n\n  output:\n  path \"*.bed\"\n\n  script:\n  \"\"\"\n  grep ^${chr}\\t ${bed} &gt; ${chr}.bed\n  \"\"\"\n}\n\nworkflow {\n    split_bed_by_chr(params.infile, Channel.from(1..22)) | view()\n}\nChallenge\nHow do we include chr X and Y into the above split by chromosome?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nworkflow {\n    split_bed_by_chr(params.infile, Channel.from(1..22,'X','Y').flatten()) | view()\n}"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#gather",
    "href": "workshops/8.1_scatter_gather_output.html#gather",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.3 Gather",
    "text": "8.3 Gather\nThe gather operation consolidates results from parallel computations (can be from scatter) into a centralized process for aggregation and further processing.\nSome of the Nextflow provided operators that facilitate this gather operation, include:\n\ncollect\ncollectFile\nmap + groupTuple"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#process-all-outputs-altogether",
    "href": "workshops/8.1_scatter_gather_output.html#process-all-outputs-altogether",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.3.1. Process all outputs altogether",
    "text": "8.3.1. Process all outputs altogether\nExercise\nparams.infile = \"/data/reference/bed_files/Agilent_CRE_v2/S30409818_Covered_MERGED.bed\"\nparams.size = 100000\n\nprocess split_bed_by_chr {\n  debug true\n\n  input:\n  path bed\n  val chr\n\n  output:\n  path \"*.bed\"\n\n  script:\n  \"\"\"\n  grep ^${chr}\\t ${bed} &gt; ${chr}.bed\n  \"\"\"\n}\n\nworkflow {\n    split_bed_by_chr(params.infile, Channel.from(1..22,'X','Y').flatten()) | collect | view()\n}"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#collect-outputs-into-a-file",
    "href": "workshops/8.1_scatter_gather_output.html#collect-outputs-into-a-file",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.3.2. Collect outputs into a file",
    "text": "8.3.2. Collect outputs into a file\nExercise\nparams.infile = \"/data/reference/bed_files/Agilent_CRE_v2/S30409818_Covered_MERGED.bed\"\nparams.size = 100000\n\nprocess split_bed_by_chr {\n  debug true\n\n  input:\n  path bed\n  val chr\n\n  output:\n  path \"*.bed\"\n\n  script:\n  \"\"\"\n  grep ^${chr}\\t ${bed} &gt; ${chr}.bed\n  \"\"\"\n}\n\nworkflow {\n    split_bed_by_chr(params.infile, Channel.from(1..22,'X','Y').flatten()) | collectFile(name: 'merged.bed', newLine:true) | view()\n}\nExercise\nworkflow {\n  Channel.fromPath(\"/scratch/users/rlupat/nfWorkshop/dev1/training/nf-training/data/ggal/*_1.fq\", checkIfExists: true) \\\n    | collectFile(name: 'combined_1.fq', newLine:true) \\\n    | view\n}"
  },
  {
    "objectID": "workshops/2.4_configs.html",
    "href": "workshops/2.4_configs.html",
    "title": "Nextflow Configs",
    "section": "",
    "text": "As we learnt in lesson 1.2.4, all nf-core pipelines have a unique set of pipeline-specific parameters that can be used in conjunction with Nextflow parameters to configure the workflow. Generally, nf-core pipelines can be customised at a few different levels:\n\n\n\n\n\n\n\nLevel of effect\nCustomisation feature\n\n\n\n\nThe workflow\nWhen diverging methods are available for a pipeline, you may choose a specific path to follow\n\n\nA process\nWhere the process is executed and what software version is used\n\n\nA tool\nApply specific thresholds or optional flags for a tool inside a process\n\n\nCompute resources\nResource allocation thresholds or software execution methods for a workflow or a process\n\n\n\nIt is important to remember that nf-core pipelines typically do not include all possible tool parameters. This makes it challenging to piece different sources of information together to determine which parameters you should be using.\n\n4.1.4. Default nf-core configuration\nLet’s take a closer look at configuration settings, which manage how the workflow is implemented on your system.\nNextflow’s portability is achieved by separating the workflow implementation (input data, custom parameters, etc.) from the configuration settings (tool access, compute resources, etc.) required to execute it. This portability facilitates reproducibility: by applying the same pipeline parameters as a colleague, you can achieve the same results on any machine by adjusting the resource configurations to suit your platform. This means there is no requirement to edit the pipeline code.\nTogether, nextflow.config and base.config can be used to define the default execution settings and parameters of an nf-core workflow.\nInside the conf/base.config file are the default compute resource settings to be used by the processes in the nf-core workflow. It uses process labels, specified with withLabel, to enable different sets of resources to be applied to groups of processes that require similar compute. Processes are labelled within the process main.nf file:\nprocess STAR_ALIGN {\n    ...\n\n    label 'process_high'\n\n    ...\n\n}\nWe can over-ride these default compute resources using the command line, or a custom configuration file specifed with -c.\nNow, take a few moments to look through nextflow.config.\nRecall that this file is more workflow-specific, and sets the defaults for the workflow parameters such as --max_cpus, --max_memory and --max_time. These are generous values that are expected to be over-ridden with your custom settings, to ensure that no single process attempts to use more resources than you have available on your platform. To over-ride these default parameters, the command line, or a parameters file specified with -params-file can be used.\nWithin conf/base.config, the check_max() function will ensure that a process-specifc resource setting will not exceed the maximum settings as dictated by by --max_cpus, --max_memory and --max_time. If the process setting does exceed any of the maximum CPUs, memory, or time, that value will be over-written to the values in --max_cpus, --max_memory and --max_time.\n\nBy default, all published nf-core modules contain a process label that categorises that process based on its resource usage. This means we won’t have to specify the resources for each individual process in the pipeline – we only need to tune these resource usage groups based on our compute infrastructure.\nAlso notice that nextflow.config contains the software profiles available to use, such as Apptainer, Singulariy, Docker, or Conda. Each process definition script main.nf will define which software can be used – these are usually container managers or Conda environment managers. For STAR_ALIGN:\nprocess STAR_ALIGN {\n    ...\n\n    conda \"bioconda::star=2.7.10a bioconda::samtools=1.16.1 conda-forge::gawk=5.1.0\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:1df389393721fc66f3fd8778ad938ac711951107-0' :\n        'biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:1df389393721fc66f3fd8778ad938ac711951107-0' }\"\n\n    ...\n\n}\nA different container will be ‘pulled’ from a repository if Docker/Singularity/Apptainer is specified, or environment files will be downloaded if Conda has been specified. The default software profile can be over-ridden by specifying -profile on the command line.\nExercise: What are the default settings for maximum CPU, memory and time for the STAR_ALIGN module? How have these defaults changed after applying our customisations previously?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, we need to determine what process label has been assigned to the STAR_ALIGN module.\nSTAR_ALIGN has the label process_high which by default sets 12 CPUs, 72GB of memory, 16 hours time limit, as specified in conf/base.config(https://github.com/nf-core/rnaseq/blob/3.14.0/conf/base.config).\nWe have previosuly applied --max_cpus 2 and --max_memory 6.GB, so the check_max() function would have reduced the final resources given to the STAR alignment process to 2 CPUs and 6GB of memory, while retaining the default maximum walltime.\n\n\n\n\n\n4.1.5. When to use a custom config file\n\n\n\n\n\n\nImportant\n\n\n\nTO DO Change CONTAINER AND PATH EXPORT\n\n\nIn our runs so far, we have avoided the need for a custom resource configuration file by:\n\nOver-riding the default  profile that dictates how software tools are accessed\n\nWithout this, our pipeline runs would fail since we do not have each workflow tool (such as STAR_ALIGN) installed localy on our machine\nAdditionally, since we use a shared container directory, the path was exported using the command export ...=...\n\nOver-riding the default values for CPUs and memory set in nextflow.config with --max_cpus 2 and --max_memory 6.GB to fit within our interactive sessions\n\nWithout these parameters, our pipeline runs would fail since Nextflow first checks that the requested resources are available before attempting to execute a workflow. When a process requests more resources than available, that process will fail.\n\n\nHowever, those are basic configurations. What if:\n\nWe wanted to increase the resources above what is set by default in pipeline process labels, to take advantage of high CPU or high memory infrastructures?\nWe wanted to run on an HPC or cloud infrastructure?\nWe wanted to execute specific modules on specific partitions on a cluster?\nWe wanted to use a non-default software container?\nWe wanted to customise outputs beyond what was possible with the nf-core workflow parameters?\n\n\n\n4.1.6 Submitting each process as an individual job to HPC\nRecall that Nextflow has a number of different scopes that can be included in configuration files. For example the params scope that we tested previously, and the profiles scope that defined software management methods.\nAgain, look inside conf/base.config. Notice that all the resource specifications are wrapped inside the process scope.\nprocess {\n\n    cpus   = { check_max( 1    * task.attempt, 'cpus'   ) }\n    memory = { check_max( 6.GB * task.attempt, 'memory' ) }\n    time   = { check_max( 4.h  * task.attempt, 'time'   ) }\n\n    ...\n\n}\nTo specify how a process is executed, the process scope can also be used. Currently, all our processes are running locally on our interactive session. These processes are managed by Nextflow, which determines which inputs are available to a particular process, and launches multiple processes in parallel if there are adequate resources available.\nHowever, instead of launching the processes in parallel locally, we can submit them as individual jobs on our HPC system. This will also execute the processes to run in parallel, but will allow for more resources to be specified beyond what is available locally.\n\n\n\n\n\n\nImportant\n\n\n\nTO DO Change to PARTITION and EXECUTOR in nectar. Change MAX LIMITS\n\n\nLet’s create a custom resource file, resources.config. Paste the following into your new file:\nprocess {\n    executor = 'slurm'\n    queue = 'PARTITION'\n}\n\nexecutor {\n    queueSize = 4\n}\nNextflow is compatible with many executors, including AWS, Azure, PBS, and many more – we will take a closer look later in the session. For our purposes, we are specifying the executor as slurm and the partition to be PARTITION, inside the process scope. Nexflow will submit each process as a separate job using the sbatch command.\nFor the purposees of this workshop, we will limit the number of concurrent jobs each user can submit to 4. This can be done using the executor scope, along with the queueSize parameter. There are many execution options that can be configured; for a full list see here.\nNote that we can now adjust the --max_memory and --max_cpus that we specified in our parameter file workshop-params.yaml to suit our HPC system. Change those parameters to the following:\nmax_memory: \"36.GB\" \nmax_cpus: 12\nNow, rerun the pipeline, specifying our new resources.config file, and our updated workshop-params.yaml.\n\n\n\n\n\n\nImportant\n\n\n\nTO DO Change to CONTAINER\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure the previous Nextflow run has completed. If it has not completed, you can simply cancel it by running control+C. Also, add the -resume option to your nextflow run command. This will cache any already completed processes. We will investigate this functionality later in the workshop.\n\n\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n    -resume\n    -profile &lt;CONTAINER&gt; \\\n    -params-file ./workshop-params.yaml \\\n    -c resources.config\nHas the executor been updated successfully? If slurm is being used, you should now notice executor &gt; slurm (4). This indicates that a maximum of 4 concurrent jobs have been submitted, as specified in queueSize.\nexecutor &gt;  slurm (4)\n[72/1f5082] NFC…ENOME:GTF_FILTER (chr22_with_ERCC92.fa) | 0 of 1\n[-        ] NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF2BED -\n[-        ] NFC…Q:PREPARE_GENOME:MAKE_TRANSCRIPTS_FASTA -\n[4e/90b642] NFC…OM_GETCHROMSIZES (chr22_with_ERCC92.fa) | 0 of 1\n[-        ] NFC…ASEQ:PREPARE_GENOME:STAR_GENOMEGENERATE -\n[-        ] NFCORE_RNASEQ:RNASEQ:CAT_FASTQ              -\n...\n\n\n4.1.7. Custom resource configuration using process labels\nTo achieve optimum computational efficiency on your platform, more granular control may be required beyond what is capable with --max_cpus, --max_memory and --max_time.\n\n\n\n\n\n\nNote\n\n\n\nIf you instead set --max_cpus 16 to the nf-core rnaseq workflow, the STAR_ALIGN module would still only utilise 12 CPUs. This is because it has been set with the label process_high, which sets the CPUs to 12. Since 12 does not exceed the maximum allowable CPUs of 16, 12 CPUs will be utilised in the process execution.\n    withLabel:process_high {\n        cpus   = { check_max( 12    * task.attempt, 'cpus'    ) }\n        memory = { check_max( 72.GB * task.attempt, 'memory'  ) }\n        time   = { check_max( 16.h  * task.attempt, 'time'    ) }\n    }\nHowever, if we do have 16 CPUs available and there are no ther processes with fulfilled input channels that could make use of the 4 remaining CPUs, those resources would sit idle while the STAR_ALIGN process is completing.\nTo optimise the resource allocations for the 16 CPU platform, we might for example set --max_cpus 8 so two samples could be aligned concurrently. Another option is to over-ride the CPU resources assigned to the STAR_ALIGN module and increase it to 16.\n\n\nThis can be done through the process scope. Let’s now add the following process label resources to our custom resources file, resources.config.\n    withLabel: process_low {\n        cpus = 2\n        memory = 6.GB\n    }\n    withLabel: process_medium {\n        cpus = 4\n        memory = 12.GB\n    } \n    withLabel: process_high {\n        cpus = 12\n        memory = 36.GB\n    }\n\n\n\n\n\n\nNote\n\n\n\nFor the purposes of our workshop, we are setting small resource limits for our HPC. Consider how this approach can be powerful when taking advantage of the compute resources available on your platform.\n\n\nSave the file then re-run the workflow with our custom configuration.\n\n\n\n\n\n\nImportant\n\n\n\nTO DO Change CONTAINER\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure the previous Nextflow run has completed. If it has not completed, you can simply cancel it by running control+C. Also, add the -resume option to your nextflow run command. This will cache any already completed processes. We will investigate this functionality later in the workshop.\n\n\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n    -resume\n    -profile &lt;CONTAINER&gt; \\\n    -params-file ./workshop-params.yaml \\\n    -c resources.config\n\n\n4.1.8. Custom resource configuration using process names\nSince process labels can only specifiy resources to groups of processes that share the same label through withLabel, we can achieve greater control using withName, to specify resources for a particular process.\nSimilar to withLabel, using withName allows us to adjust the requirements for a specific process without needing to edit any pipeline module code (ie. the main.nf module file). With withName, multiple module names can also be specified using wildcards or or (* or |) notation.\nwithName has a higher priority than withLabel, meaning anything contained in withName will over-ride conflicting values in `withLabel’.\nFirst, let’s ensure we have the specific path name for the module that we wish to target. We will be using the MULTIQC module as an example. When the rnaseq pipeline was executed, an execution file was created inside the pipeline output folder. This file is located within the pipeline_info folder, and prefixed with execution_trace along with the date of pipeline execution. Note: Change &lt;date_of_pipeline&gt; to the file date that is inside your folder.\nls lesson2.1/pipeline_info/execution_trace_&lt;date_of_pipeline&gt;.txt\nThis file contains a full log of each process that has ran, along with resources specified to the process and if the execution was successful. To get the full name path for the MULTIQC process, let’s search for this process name inside the execution trace\ngrep MULTIQC lesson2.1/pipeline_info/execution_trace_&lt;date_of_pipeline&gt;.txt\n36      f0/731167       23769653        NFCORE_RNASEQ:RNASEQ:MULTIQC_CUSTOM_BIOTYPE (HBR_Rep2_ERCC)  COMPLETED       0       2025-05-04 15:55:44.676 9.7s    0ms     38.2%   3.1 MB  5.4 MB       1.4 MB  2.6 KB\n52      c9/b96b9e       23769667        NFCORE_RNASEQ:RNASEQ:MULTIQC_CUSTOM_BIOTYPE (HBR_Rep1_ERCC)  COMPLETED       0       2025-05-04 15:55:49.699 9.7s    0ms     40.0%   3 MB    5.4 MB       1.4 MB  2.6 KB\n66      b7/aba0aa       23769685        NFCORE_RNASEQ:RNASEQ:MULTIQC (1)        COMPLETED   02025-05-04 15:57:14.401 1m 35s  1m 18s  198.4%  186.7 MB        1.1 GB  44.1 GB 12.2 MB\nThis search returned three results. For our purposes, we will be using the last result, and the information contained in the fourth column, which provides the full process name NFCORE_RNASEQ:RNASEQ:MULTIQC. This process name indicates what workflows the particular module originated from.\nFor MULTIQC, any of the following names can be used:\n\n'NFCORE_RNASEQ:RNASEQ:MULTIQC': Use the MULTIQC module inside the RNASEQ workflow in NFCORE_RNASEQ\n'.*:RNASEQ:MULTIQC': Use the MULTIQC module inside the RNASEQ workflow in any upstream workflows\n'.*:MULTIQC': Use the MULTIQC module in any upstream workflows\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are running the pipeline for the first time, it can be difficult to determine the full name path to use. If you are unsure of how to build the path, you can look through the modules.config files specified for your pipeline on Github. These usually contain module specific parameters that can guide you in creating your own name path.\n\n\nLet’s now add the following to our configuration file resources.config. Inside the process scope, provide the name for the MULTIQC module using the withName selector. Now, we wish to use a specific container when running that module. Change it to 'quay.io/biocontainers/multiqc:1.14--pyhdfd78af_0'\nprocess {\n\n  ...\n\n  withName: '.*:MULTIQC' {\n    container = 'quay.io/biocontainers/multiqc:1.14--pyhdfd78af_0'\n  }\n} \n\n\n\n\n\n\nImportant\n\n\n\nTO DO Change EXECUTOR\n\n\n\n\n\n\n\n\nCompleted configuration file\n\n\n\n\n\nprocess {\n    executor = 'slurm'\n    queue = 'EXECUTOR'\n\n    withLabel: process_low {\n        cpus = 2\n        memory = 6.GB\n    }\n    withLabel: process_medium {\n        cpus = 4\n        memory = 12.GB\n    } \n    withLabel: process_high {\n        cpus = 12\n        memory = 36.GB\n    }\n\n    withName: '.*:MULTIQC' {\n        container = 'quay.io/biocontainers/multiqc:1.14--pyhdfd78af_0'\n    }\n\n}\n\nexecutor {\n    queueSize = 4\n}\n\n\n\n\n\n\n\n\n\nWhat if the parameter I want to apply isn’t available?\n\n\n\nRecall earlier that nf-core modules use ext.args to pass additional arguments to a module. This uses a special Nextflow directive ext. If an nf-core pipeline does not have a pre-defined parameter for a process, you may be able to implement ext.args.\nThe inclusion of ext.args is currently best practice for all DSL2 nf-core modules where additional parameters may be required to run a process. However, this may not be implemented for all modules in all nf-core pipelines. Depending on the pipeline, these process modules may not have defined the ext.args variable in the script blocks and is thus not available for applying customisation. If that is the case consider submitting a feature request or a making pull request on the pipeline’s GitHub repository to implement this!\n\n\nSave the config then resume your run, setting outdir to lesson2.1_multiqc, along with the resource file resources.config and parameter file workshop-params.yaml:\n\n\n\n\n\n\nImportant\n\n\n\nTO DO Change CONTAINER\n\n\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n  -resume \n  -profile &lt;CONTAINER&gt; \\\n  -c my_resources.config \\\n  -params-file workshop-params.yaml \\\n  --outdir lesson2.1_multiqc \\\nIf your execution path for the MULTIQC module was not specified correctly, a pipeline warning would be printed, such as:\nWARN: There's no process matching config selector: ...\n\n\n\n\n\n\nConfiguration order of priority\n\n\n\nPreviously, we saw that withName has a higher priority than withLabel. There are additional configuration priorities managed by Nextflow, when conflicting parameters are provided.\nThe settings specific with -c resources.config will over-ride those that appear in the default nf-core configurations nextflow.config and conf/base.config.\nAdditionally, any parameters provided in the command line will over-ride those in the -c configuration file.\nTo avoid confusion, it is best not to name your custom configuration files nextflow.config!\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core workflows work ‘out of the box’ but there are compute and software configurations we can customise to optimise the pipeline execution on our compute infrastructure\nnf-core uses the default parameters in nextflow.config and conf/base.config, both of which are automatically used by the pipeline\nA custom configuration can be applied using -c, and will over-ride settings in the default configs\nCustomisations can be targeted to either groups of processes, or specific processes using withLabel or withName\nWorkflow parameters can be specific in -params-file and not -c"
  },
  {
    "objectID": "workshops/2.2_troubleshooting.html",
    "href": "workshops/2.2_troubleshooting.html",
    "title": "Troubleshooting Nextflow run",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn basic troubleshooting of nextflow log\nLearn the structure of nextflow work directory\nExamine the run command stitched together by nextflow for manual debugging\n\n\n\n\n2.2.1. Nextflow log\nIt is important to keep a record of the commands you have used to generate your results. Nextflow helps with this by creating and storing metadata and logs about the run in hidden files and folders in your current directory (unless otherwise specified). This data can be used by Nextflow to generate reports. It can also be queried using the Nextflow log command:\nnextflow log\nThe log command has multiple options to facilitate the queries and is especially useful while debugging a workflow and inspecting execution metadata. You can view all of the possible log options with -h flag:\nnextflow log -h\nTo query a specific execution you can use the RUN NAME or a SESSION ID:\nnextflow log &lt;run name&gt;\nTo get more information, you can use the -f option with named fields. For example:\nnextflow log &lt;run name&gt; -f process,hash,duration\nThere are many other fields you can query. You can view a full list of fields with the -l option:\nnextflow log -l\nExercise: Use the log command to view with process, hash, and script fields for your tasks from your most recent Nextflow execution.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, use the log command to get a list of your recent executions:\nnextflow log\nTIMESTAMP           DURATION    RUN NAME            STATUS  REVISION ID SESSION ID                              COMMAND                                                                                                                 \n2025-05-27 15:23:39 2m 16s      hopeful_dubinsky    OK      b89fac3265  54c06115-6867-45e3-86b3-8566a69f7406    nextflow run nf-core/rnaseq -r 3.14.0 -profile apptainer -params-file ./workshop-params.yaml                            \n2025-05-27 15:56:36 37.1s       tiny_bartik         OK      b89fac3265  54c06115-6867-45e3-86b3-8566a69f7406    nextflow run nf-core/rnaseq -r 3.14.0 -profile apptainer -params-file ./workshop-params.yaml -resume                    \n2025-05-27 15:57:23 35.6s       angry_mcclintock    OK      b89fac3265  54c06115-6867-45e3-86b3-8566a69f7406    nextflow run nf-core/rnaseq -r 3.14.0 -profile apptainer -params-file ./workshop-params.yaml -resume --outdir my_results\nQuery the process, hash, and script using the -f option for the most recent run:\nnextflow log marvelous_shannon -f process,hash,script\n\n[... truncated ...]\n\nNFCORE_RNASEQ:RNASEQ:QUANTIFY_PSEUDO_ALIGNMENT:SALMON_QUANT d7/01e251   \n    salmon quant \\\n        --geneMap genome.filtered.gtf \\\n        --threads 2 \\\n        --libType=ISR \\\n        --index salmon \\\n        -1 SRR6357071_1_val_1.fq.gz -2 SRR6357071_2_val_2.fq.gz \\\n         \\\n        -o SRR6357071\n\n    if [ -f SRR6357071/aux_info/meta_info.json ]; then\n        cp SRR6357071/aux_info/meta_info.json \"SRR6357071_meta_info.json\"\n    fi\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"NFCORE_RNASEQ:RNASEQ:QUANTIFY_PSEUDO_ALIGNMENT:SALMON_QUANT\":\n        salmon: $(echo $(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n\n[... truncated ... ]\n\nNFCORE_RNASEQ:RNASEQ:MULTIQC    7c/b0bbc5   \n    multiqc \\\n        -n multiqc_report.html \\\n        -f \\\n         \\\n         \\\n        .\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"NFCORE_RNASEQ:RNASEQ:MULTIQC\":\n        multiqc: $( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \n\n\n\n\n\n2.2.2. Execution cache and resume\nTask execution caching is an essential feature of modern workflow managers. As such, Nextflow provides an automated caching mechanism for every execution. When using the Nextflow -resume option, successfully completed tasks from previous executions are skipped and the previously cached results are used in downstream tasks.\nNextflow caching mechanism works by assigning a unique ID to each task. The task unique ID is generated as a 128-bit hash value composing the the complete file path, file size, and last modified timestamp. These ID’s are used to create a separate execution directory where the tasks are executed and the outputs are stored. Nextflow will take care of the inputs and outputs in these folders for you.\nYou can re-launch the previously executed nf-core/rnaseq workflow again using -resume, and observe the progress. Change the output directory to be my_results. Notice the time it takes to complete the workflow.\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n    -profile apptainer \\\n    -params-file ./workshop-params.yaml \\\n    -resume \\\n    --outdir my_results\n[6d/10f0b4] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF_FILTER (genome.fasta)                      [100%] 1 of 1, cached: 1 ✔\n[77/74cbf2] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF2BED (genome.filtered.gtf)                  [100%] 1 of 1, cached: 1 ✔\n[02/f7e668] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:MAKE_TRANSCRIPTS_FASTA (rsem/genome.fasta)     [100%] 1 of 1, cached: 1 ✔\n...\nExecuting this workflow will create a my_results directory that contain selected results files, as well as the work directory, which contains further sub-directories.\nIn the schematic above, the hexadecimal numbers, such as 6d/10f0b4, identify the unique task execution. These numbers are also the prefix of the work subdirectories where each task is executed.\nYou can inspect the files produced by a task by looking inside the work directory and using these numbers to find the task-specific execution path. Use tab to autocomplete the full file path:\nls work/6d/10f0b4d0e6cf920e35657ce78feb1d/\nIf you look inside the work directory of a FASTQC process, you will find the files that were staged and created when this task was executed:\nls -la  work/e9/60b2e80b2835a3e1ad595d55ac5bf5/ \ntotal 1940\ndrwxrwxr-x 2 larigan larigan   4096 May 27 15:24 .\ndrwxrwxr-x 3 larigan larigan   4096 May 27 15:23 ..\n-rw-rw-r-- 1 larigan larigan      0 May 27 15:24 .command.begin\n-rw-rw-r-- 1 larigan larigan      0 May 27 15:24 .command.err\n-rw-rw-r-- 1 larigan larigan     34 May 27 15:24 .command.log\n-rw-rw-r-- 1 larigan larigan     34 May 27 15:24 .command.out\n-rw-rw-r-- 1 larigan larigan  10394 May 27 15:24 .command.run\n-rw-rw-r-- 1 larigan larigan    468 May 27 15:24 .command.sh\n-rw-rw-r-- 1 larigan larigan    261 May 27 15:24 .command.trace\n-rw-rw-r-- 1 larigan larigan      1 May 27 15:24 .exitcode\n-rw-rw-r-- 1 larigan larigan 598884 May 27 15:24 SRR6357071_1_fastqc.html\n-rw-rw-r-- 1 larigan larigan 365752 May 27 15:24 SRR6357071_1_fastqc.zip\nlrwxrwxrwx 1 larigan larigan     66 May 27 15:24 SRR6357071_1.fastq.gz -&gt; /home/larigan/rnaseq_data/testdata/GSE110004/SRR6357071_1.fastq.gz\nlrwxrwxrwx 1 larigan larigan     21 May 27 15:24 SRR6357071_1.gz -&gt; SRR6357071_1.fastq.gz\n-rw-rw-r-- 1 larigan larigan 604569 May 27 15:24 SRR6357071_2_fastqc.html\n-rw-rw-r-- 1 larigan larigan 355487 May 27 15:24 SRR6357071_2_fastqc.zip\nlrwxrwxrwx 1 larigan larigan     66 May 27 15:24 SRR6357071_2.fastq.gz -&gt; /home/larigan/rnaseq_data/testdata/GSE110004/SRR6357071_2.fastq.gz\nlrwxrwxrwx 1 larigan larigan     21 May 27 15:24 SRR6357071_2.gz -&gt; SRR6357071_2.fastq.gz\n-rw-rw-r-- 1 larigan larigan     83 May 27 15:24 versions.yml\nThe FASTQC process runs twice, executing in a different work directories for each set of inputs. Therefore, in the previous example, the work directory [e9/60b2e8] represents just one of the two sets of input data that was processed.\nIt’s very likely you will execute a workflow multiple times as you find the parameters that best suit your data. You can save a lot of storage space (and time) by resuming a workflow from the last step that was completed successfully and/or unmodified.\nIn practical terms, the workflow is executed from the beginning. However, before launching the execution of a process, Nextflow uses the unique task ID to check if the work directory already exists and that it contains a valid command exit state with the expected output files. If this condition is satisfied, the task execution is skipped and previously computed results are saved in the output directory.\nNotably, the -resume functionality is very sensitive. Even touching a file in the work directory can invalidate the cache.\nExercise: Invalidate the cache by touching a .fastq.gz file inside the FASTQC task work directory (you can use the touch command). Execute the workflow again with the -resume option. Has the cache has been invalidated?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExecute the workflow for the first time (if you have not already).\nUse the task ID shown for the FASTQC process and use it to find and touch a .fastq.gz file:\ntouch work/ff/21abfa87cc7cdec037ce4f36807d32/SRR6357071_1.fastq.gz\nExecute the workflow again with the -resume command option:\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n    -profile apptainer \\\n    -params-file ./workshop-params.yaml \\\n    -resume \\\n    --outdir my_results\nYou should see that some task were invalid and were executed again.\nWhy did this happen?\nIn this example, the caching of one of the two FASTQC tasks were invalid. The fastq file we touch is used by in the pipeline in multiple places. Thus, touching the symbolic link for this file and changing the date of last modification disrupted one of the task execution and its related downstream processes.\n\n\n\n\n\n2.2.3. Troubleshoot warning and error messages\nIf we go back to our last exercise (exercise_rnaseq output), you might recall that while that workflow execution completed successfully, there were a couple of warning messages that may be cause for concern:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 04-May-2025 15:03:01\nDuration    : 7m 59s\nCPU hours   : 0.8\nSucceeded   : 66\n\n\n\n\n\n\nHandling dodgy error messages\n\n\n\nThe first warning message isn’t very descriptive (see this pull request). You might come across issues like this when running nf-core pipelines, too. Bug reports and user feedback is very important to open source software communities like nf-core. If you come across any issues, submit a GitHub issue or start a discussion in the relevant nf-core Slack channel so others are aware and it can be addressed by the pipeline’s developers.\n\n\n➤ Take a look at the MultiQC report, as directed by the second message. You can find the MultiQC report in the exercise_rnaseq directory:\nls -la exercise_rnaseq/multiqc/star_salmon/\ntotal 1402\ndrwxrwxr-x 4 rlupat rlupat    4096 Nov 22 00:29 .\ndrwxrwxr-x 3 rlupat rlupat    4096 Nov 22 00:29 ..\ndrwxrwxr-x 2 rlupat rlupat    8192 Nov 22 00:29 multiqc_data\ndrwxrwxr-x 5 rlupat rlupat    4096 Nov 22 00:29 multiqc_plots\n-rw-rw-r-- 1 rlupat rlupat 1419998 Nov 22 00:29 multiqc_report.html\n➤ Download the multiqc_report.html using the file navigator panel on the left side of your VS Code window. Right click the file navagator, then select Download. Open the file on your computer.\nTake a look a the section labelled WARNING: Fail Strand Check\nThe warning indicates that the read strandedness we specified in our samplesheet.csv and inferred strandedness identified by the RSeqQC process in the pipeline do not match. In the samplesheet.csv, it seems we have incorrectly specified strandedness as forward, when our raw reads actually show an equal distribution of sense and antisense reads.\nFor those who are not familiar with RNAseq data, incorrectly specified strandedness may negatively impact the read quantification step (process: Salmon quant) and give us inaccurate results. So, let’s clarify how the Salmon quant process is gathering strandedness information for our input files by default and find a way to address this with the parameters provided by the nf-core/rnaseq pipeline.\n\n\n2.2.4. Identify the run command for a process\nTo observe the exact command used a process, we can attempt to infer this information from the module’s main.nf script in the modules/ directory. However, given all the different parameters that may be applied at the process level, this may not be very clear.\n➤ Take a look at the Salmon quant main.nf file.\nThis file contains many function definitions within the process, variable substitutions, and internal parameters determined based on strandedness. This makes it very hard to see what is actually happening in the code, given all the different variables and conditional arguments inside this script.\nAbove the script block, we can see strandedness is being applied using a few different conditional arguments. Instead of trying to infer how the $strandedness variable is being defined and applied to the process, let’s use the hidden command files saved for this process in its work execution directory.\n\n\n\n\n\n\nHidden files in the work directory!\n\n\n\nRemember that the pipeline’s results are cached in the work directory. In addition to the cached files, each task execution directory inside the work directory contains a number of hidden files:\n\n.command.sh: The command used for the task.\n.command.run: Specifying resources, executor, software management profiles to use.\n.command.out: The task’s standard output log.\n.command.err: The task’s standard error log.\n.command.log: A wrapper for the execution output.\n.command.begin: A file created as soon as the job is launched.\n.exitcode: A file containing the task exit code (0 if successful)\n\n\n\nWithin the nextflow log command that we discussed previously, there are multiple options to facilitate pipeline debugging and inspecting pipeline execution metadata. To understand how Salmon is interpreting strandedness, we’re going to use this command to determine the full path to hidden .command.sh scripts for each Salmon quant task that was run. This will allow us to investigate how Salmon handles strandedness and if there is a way for us to override this.\n➤ Use the nextflow log command to get the unique run name information of previously executed pipelines. Then, add that run name to your command:\nnextflow log &lt;run-name&gt;\nAfter running the command, we can see that it provided a list of all the work subdirectories created for each processes when the pipeline was executed. How do we use this information to find the speicfic hidden.command.sh for Salmon tasks?\n➤ Let’s use Bash to query a Nextflow run with the run name from the previous lesson. First, save your run name in a Bash variable run_name. For example:\nrun_name=marvelous_shannon\n➤ And let’s save the tool of interest (salmon) in another Bash variable tool:\ntool=salmon\n➤ Next, run the following bash command:\nnextflow log ${run_name} | while read line;\n    do\n      cmd=$(ls ${line}/.command.sh 2&gt;/dev/null);\n      if grep -q $tool $cmd;\n      then  \n        echo $cmd;     \n      fi; \n    done \nThis will list all process .command.sh scripts containing the word ‘salmon’. Notice that there are a few different processes that run Salmon to perform other steps in the workflow. We are looking for Salmon quant which performs the read quantification:\n/home/larigan/lesson2.1/work/9c/9cdaec01c009a4fef6de3b50b0d2c9/.command.sh\n/home/larigan/lesson2.1/work/d9/e696aa3903f2f7bef3ead3852a7d51/.command.sh\n/home/larigan/lesson2.1/work/57/95f7806c62313c5788780d1fadc89a/.command.sh\n/home/larigan/lesson2.1/work/2f/bef610318ab85c7fdbb3a773c568d5/.command.sh\n/home/larigan/lesson2.1/work/ec/d8a46743dc73214b04e09c7ae9ecb4/.command.sh\n/home/larigan/lesson2.1/work/f2/cc71ed58bbfba78ea034a26bd48370/.command.sh\n/home/larigan/lesson2.1/work/3b/0a2737be44be977e4b695c5bade23f/.command.sh\n/home/larigan/lesson2.1/work/8d/fed78effdd1b28435a698d8a6efb7a/.command.sh\n/home/larigan/lesson2.1/work/65/bd7329a29aaf2136f25173a22918ae/.command.sh\nCompared with the salmon quant main.nf file, we get a lot more fine scale details from the .command.sh process scripts:\nmain.nf:\nsalmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n.command.sh:\nsalmon quant \\\n    --geneMap genome.filtered.gtf \\\n    --threads 2 \\\n    --libType=ISF \\\n    -t genome.transcripts.fa \\\n    -a SRR6357071.Aligned.toTranscriptome.out.bam \\\n     \\\n    -o SRR6357071\nFrom .command.sh, we see that --libType has been set to ISF (ie. forward strandedness), based on our samplesheet.\nExercise: Besides changing the samplesheet input, we can use parameter settings to over-ride the --libType. Use the pipeline Parameters documentation to determine what parameter has to be changed. Instead, we would like this to be ISR (ie. reverse strandedness). How can we do this?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom the pipeline documentation, the --salmon_quant_libtype can be changed. To change the libType specified to Salmon to be ISR, we can specify --salmon_quant_libtype ISR using the command line or in a parameter file.\n\n\n\n\n\n2.2.5. Using a parameter file\nFrom the previous section we learn that Nextflow can accept a yaml parameter file. Any of the pipeline-specific parameters can be supplied to a Nextflow pipeline in this way.\nExercise: Set the Salmon libType to ISR, inside the workshop-params.yaml file we created previously.\n\n\n\n\n\n\nYAML Formatting Tip\n\n\n\n\nStrings need to be inside double quotes\nBooleans (true/false) and numbers do not require quotes\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nworkshop-params.yaml should now contain one additional parameter:\nsalmon_quant_libtype: \"ISR\" \n\n\n\n➤ Now that our params file has been saved, we can rerun the pipeline:\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n  -resume \n  -profile apptainer \\\n  -params-file workshop-params.yaml \\\n  --outdir exercise_rnaseq \nAs the workflow runs a second time, you will notice 4 things:\n\nThe nextflow run command is much tidier, due to the use of a -params-file that stores pipeline parameters used in a Nextflow run\nThe -resume flag. Nextflow has many run options including the ability to use cached output\nSome processes will be pulled from the cache. These processes remain unaffected by our addition of the new parameter.\n\nThis run of the pipeline will complete in a much shorter time compared to starting the pipeline from the beginning, due to pipeline caching.\n\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 27-May-2025 17:13:48\nDuration    : 1m 55s\nCPU hours   : 0.2 (70.8% cached)\nSucceeded   : 11\nCached      : 41\nWe still seem to be getting the warning Please check MultiQC report: 2/2 samples failed strandedness check.-. Let’s check what --libType has been used inside the salmon process.\nExercise: Determine the hexadecimal code output by Nextflow for a SALMON_QUANT process in your most recent run. Use this code to determine the work execution directory for SALMON_QUANT, and look inside the .command.sh. What --libType has been used? Is it the one we specified in our parameter file?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn the Nextflow output, the following line provides the hexadeximal for a SALMON_QUANT process.\n[d9/69e2a7] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_QUANT (SRR6357071)                                       [100%] 2 of 2 ✔\nThe work execution directory for this process is located in (using tab to autocomplete the folder name):\nls -a work/d9/69e2a7563de5e6046f140a1317c1d2/\nInside the .command.sh file, the --libType parameter matches the one specified in our parameter file:\n#!/bin/bash -euo pipefail\nsalmon quant \\\n    --geneMap genome.filtered.gtf \\\n    --threads 2 \\\n    --libType=ISR \\\n    -t genome.transcripts.fa \\\n    -a SRR6357071.Aligned.toTranscriptome.out.bam \\\n     \\\n    -o SRR6357071\n\nif [ -f SRR6357071/aux_info/meta_info.json ]; then\n    cp SRR6357071/aux_info/meta_info.json \"SRR6357071_meta_info.json\"\nfi\n\ncat &lt;&lt;-END_VERSIONS &gt; versions.yml\n\"NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_QUANT\":\n    salmon: $(echo $(salmon --version) | sed -e \"s/salmon //g\")\nEND_VERSIONS\nThe tool is working as we expected!\n\n\n\n\n\n\n\n\n\nHow do I get rid of the strandedness check warning message?\n\n\n\nIf we want to remove the warning message Please check MultiQC report: 2/2 samples failed strandedness check, we will have to change the strandedness fields in our samplesheet.csv. Keep in mind, doing this will invalidate the pipeline’s cache and will cause the pipeline to run from the beginning.\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nUse nextflow log to query the record of commands used in the pipeline\nUse -resume to re-launch previously executed workflows in order to get Nextflow to utilise its task execution caching feature\nExamine the .command.sh to inside the work directory to troubleshoot the command that Nextflow use to run a particular task\n\n\n\n\n\n\nNext Chapter: Introduction to Nextflow Processes and Channels\n\n\nThis workshop is adapted from various nextflow training materials, including:\n\nNextflow Training Materials\nCustomising Nf-Core Workshop\nHello Nextflow Workshop"
  },
  {
    "objectID": "workshops/4.1_draft_future_sess.html",
    "href": "workshops/4.1_draft_future_sess.html",
    "title": "Nextflow Development - Metadata Parsing",
    "section": "",
    "text": "Currently, we have defined the reads parameter as a string:\nparams.reads = \"/.../training/nf-training/data/ggal/gut_{1,2}.fq\"\nTo group the reads parameter, the fromFilePairs channel factory can be used. Add the following to the workflow block and run the workflow:\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\nreads_ch.view()\nThe reads parameter is being converted into a file pair group using fromFilePairs, and is assigned to reads_ch. The reads_ch consists of a tuple of two items – the first is the grouping key of the matching pair (gut), and the second is a list of paths to each file:\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\nGlob patterns can also be used to create channels of file pair groups. Inside the data directory, we have pairs of gut, liver, and lung files that can all be read into reads_ch.\n&gt;&gt;&gt; ls \"/.../training/nf-training/data/ggal/\"\n\ngut_1.fq  gut_2.fq  liver_1.fq  liver_2.fq  lung_1.fq  lung_2.fq  transcriptome.fa\nRun the rnaseq.nf workflow specifying all .fq files inside /.../training/nf-training/data/ggal/ as the reads parameter via the command line:\nnextflow run rnaseq.nf --reads '/.../training/nf-training/data/ggal/*_{1,2}.fq'\nFile paths that include one or more wildcards (ie. *, ?, etc.) MUST be wrapped in single-quoted characters to avoid Bash expanding the glob on the command line.\nThe reads_ch now contains three tuple elements with unique grouping keys:\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/.../training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\nThe grouping key metadata can also be explicitly created without having to rely on file names, using the map channel operator. Let’s start by creating a samplesheet rnaseq_samplesheet.csv with column headings sample_name, fastq1, and fastq2, and fill in a custom sample_name, along with the paths to the .fq files.\nsample_name,fastq1,fastq2\ngut_sample,/.../training/nf-training/data/ggal/gut_1.fq,/.../training/nf-training/data/ggal/gut_2.fq\nliver_sample,/.../training/nf-training/data/ggal/liver_1.fq,/.../training/nf-training/data/ggal/liver_2.fq\nlung_sample,/.../training/nf-training/data/ggal/lung_1.fq,/.../training/nf-training/data/ggal/lung_2.fq\nLet’s now supply the path to rnaseq_samplesheet.csv to the reads parameter in rnaseq.nf.\nparams.reads = \"/.../rnaseq_samplesheet.csv\"\nPreviously, the reads parameter consisted of a string of the .fq files directly. Now, it is a string to a .csv file containing the .fq files. Therefore, the channel factory method that reads the input file also needs to be changed. Since the parameter is now a single file path, the fromPath method can first be used, which creates a channel of Path type object. The splitCsv channel operator can then be used to parse the contents of the channel.\nreads_ch = Channel.fromPath(params.reads)\nreads_ch.view()\n\nreads_ch = reads_ch.splitCsv(header:true)\nreads_ch.view()\nWhen using splitCsv in the above example, header is set to true. This will use the first line of the .csv file as the column names. Let’s run the pipeline containing the new input parameter.\n&gt;&gt;&gt; nextflow run rnaseq.nf\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [distraught_avogadro] DSL2 - revision: 525e081ba2\nreads: rnaseq_samplesheet.csv\nreads: $params.reads\nexecutor &gt;  local (1)\n[4e/eeae2a] process &gt; INDEX [100%] 1 of 1 ✔\n/.../rnaseq_samplesheet.csv\n[sample_name:gut_sample, fastq1:/.../training/nf-training/data/ggal/gut_1.fq, fastq2:/.../training/nf-training/data/ggal/gut_2.fq]\n[sample_name:liver_sample, fastq1:/.../training/nf-training/data/ggal/liver_1.fq, fastq2:/.../training/nf-training/data/ggal/liver_2.f]\n[sample_name:lung_sample, fastq1:/.../training/nf-training/data/ggal/lung_1.fq, fastq2:/.../training/nf-training/data/ggal/lung_2.fq]\nThe /.../rnaseq_samplesheet.csv is the output of reads_ch directly after the fromPath channel factory method was used. Here, the channel is a Path type object. After invoking the splitCsv channel operator, the reads_ch is now replaced with a channel consisting of three elements, where each element is a row in the .csv file, returned as a list. Since header was set to true, each element in the list is also mapped to the column names. This can be used when creating the custom grouping key.\nTo create grouping key metadata from the list output by splitCsv, the map channel operator can be used.\n  reads_ch = reads_ch.map { row -&gt; \n      grp_meta = \"$row.sample_name\"\n      [grp_meta, [row.fastq1, row.fastq2]]\n      }\n  reads_ch.view()\nHere, for each list in reads_ch, we assign it to a variable row. We then create custom grouping key metadata grp_meta based on the sample_name column from the .csv, which can be accessed via the row variable by . separation. After the custom metadata key is assigned, a tuple is created by assigning grp_meta as the first element, and the two .fq files as the second element, accessed via the row variable by . separation.\nLet’s run the pipeline containing the custom grouping key:\n&gt;&gt;&gt; nextflow run rnaseq.nf\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [happy_torricelli] DSL2 - revision: e9e1499a97\nreads: rnaseq_samplesheet.csv\nreads: $params.reads\n[-        ] process &gt; INDEX -\n[gut_sample, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver_sample, [/home/sli/test/training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung_sample, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\nThe custom grouping key can be created from multiple values in the samplesheet. For example, grp_meta = [sample : row.sample_name , file : row.fastq1] will create the metadata key using both the sample_name and fastq1 file names. The samplesheet can also be created to include multiple sample characteristics, such as lane, data_type, etc. Each of these characteristics can be used to ensure an adequte grouping key is creaed for that sample."
  },
  {
    "objectID": "workshops/4.1_draft_future_sess.html#metadata-parsing",
    "href": "workshops/4.1_draft_future_sess.html#metadata-parsing",
    "title": "Nextflow Development - Metadata Parsing",
    "section": "",
    "text": "Currently, we have defined the reads parameter as a string:\nparams.reads = \"/.../training/nf-training/data/ggal/gut_{1,2}.fq\"\nTo group the reads parameter, the fromFilePairs channel factory can be used. Add the following to the workflow block and run the workflow:\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\nreads_ch.view()\nThe reads parameter is being converted into a file pair group using fromFilePairs, and is assigned to reads_ch. The reads_ch consists of a tuple of two items – the first is the grouping key of the matching pair (gut), and the second is a list of paths to each file:\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\nGlob patterns can also be used to create channels of file pair groups. Inside the data directory, we have pairs of gut, liver, and lung files that can all be read into reads_ch.\n&gt;&gt;&gt; ls \"/.../training/nf-training/data/ggal/\"\n\ngut_1.fq  gut_2.fq  liver_1.fq  liver_2.fq  lung_1.fq  lung_2.fq  transcriptome.fa\nRun the rnaseq.nf workflow specifying all .fq files inside /.../training/nf-training/data/ggal/ as the reads parameter via the command line:\nnextflow run rnaseq.nf --reads '/.../training/nf-training/data/ggal/*_{1,2}.fq'\nFile paths that include one or more wildcards (ie. *, ?, etc.) MUST be wrapped in single-quoted characters to avoid Bash expanding the glob on the command line.\nThe reads_ch now contains three tuple elements with unique grouping keys:\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/.../training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\nThe grouping key metadata can also be explicitly created without having to rely on file names, using the map channel operator. Let’s start by creating a samplesheet rnaseq_samplesheet.csv with column headings sample_name, fastq1, and fastq2, and fill in a custom sample_name, along with the paths to the .fq files.\nsample_name,fastq1,fastq2\ngut_sample,/.../training/nf-training/data/ggal/gut_1.fq,/.../training/nf-training/data/ggal/gut_2.fq\nliver_sample,/.../training/nf-training/data/ggal/liver_1.fq,/.../training/nf-training/data/ggal/liver_2.fq\nlung_sample,/.../training/nf-training/data/ggal/lung_1.fq,/.../training/nf-training/data/ggal/lung_2.fq\nLet’s now supply the path to rnaseq_samplesheet.csv to the reads parameter in rnaseq.nf.\nparams.reads = \"/.../rnaseq_samplesheet.csv\"\nPreviously, the reads parameter consisted of a string of the .fq files directly. Now, it is a string to a .csv file containing the .fq files. Therefore, the channel factory method that reads the input file also needs to be changed. Since the parameter is now a single file path, the fromPath method can first be used, which creates a channel of Path type object. The splitCsv channel operator can then be used to parse the contents of the channel.\nreads_ch = Channel.fromPath(params.reads)\nreads_ch.view()\n\nreads_ch = reads_ch.splitCsv(header:true)\nreads_ch.view()\nWhen using splitCsv in the above example, header is set to true. This will use the first line of the .csv file as the column names. Let’s run the pipeline containing the new input parameter.\n&gt;&gt;&gt; nextflow run rnaseq.nf\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [distraught_avogadro] DSL2 - revision: 525e081ba2\nreads: rnaseq_samplesheet.csv\nreads: $params.reads\nexecutor &gt;  local (1)\n[4e/eeae2a] process &gt; INDEX [100%] 1 of 1 ✔\n/.../rnaseq_samplesheet.csv\n[sample_name:gut_sample, fastq1:/.../training/nf-training/data/ggal/gut_1.fq, fastq2:/.../training/nf-training/data/ggal/gut_2.fq]\n[sample_name:liver_sample, fastq1:/.../training/nf-training/data/ggal/liver_1.fq, fastq2:/.../training/nf-training/data/ggal/liver_2.f]\n[sample_name:lung_sample, fastq1:/.../training/nf-training/data/ggal/lung_1.fq, fastq2:/.../training/nf-training/data/ggal/lung_2.fq]\nThe /.../rnaseq_samplesheet.csv is the output of reads_ch directly after the fromPath channel factory method was used. Here, the channel is a Path type object. After invoking the splitCsv channel operator, the reads_ch is now replaced with a channel consisting of three elements, where each element is a row in the .csv file, returned as a list. Since header was set to true, each element in the list is also mapped to the column names. This can be used when creating the custom grouping key.\nTo create grouping key metadata from the list output by splitCsv, the map channel operator can be used.\n  reads_ch = reads_ch.map { row -&gt; \n      grp_meta = \"$row.sample_name\"\n      [grp_meta, [row.fastq1, row.fastq2]]\n      }\n  reads_ch.view()\nHere, for each list in reads_ch, we assign it to a variable row. We then create custom grouping key metadata grp_meta based on the sample_name column from the .csv, which can be accessed via the row variable by . separation. After the custom metadata key is assigned, a tuple is created by assigning grp_meta as the first element, and the two .fq files as the second element, accessed via the row variable by . separation.\nLet’s run the pipeline containing the custom grouping key:\n&gt;&gt;&gt; nextflow run rnaseq.nf\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [happy_torricelli] DSL2 - revision: e9e1499a97\nreads: rnaseq_samplesheet.csv\nreads: $params.reads\n[-        ] process &gt; INDEX -\n[gut_sample, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver_sample, [/home/sli/test/training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung_sample, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\nThe custom grouping key can be created from multiple values in the samplesheet. For example, grp_meta = [sample : row.sample_name , file : row.fastq1] will create the metadata key using both the sample_name and fastq1 file names. The samplesheet can also be created to include multiple sample characteristics, such as lane, data_type, etc. Each of these characteristics can be used to ensure an adequte grouping key is creaed for that sample."
  },
  {
    "objectID": "workshops/5.2_nf_core_modules.html",
    "href": "workshops/5.2_nf_core_modules.html",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of how nf-core modules/subworkflows can be installed\nInstall a specific version of an nf-core module/subworkflows\nUnderstand how modules.json is used in the nf-core pipeline\nGain an understanding of how to remove nf-core modules/subworkflows"
  },
  {
    "objectID": "workshops/5.2_nf_core_modules.html#nf-core-modules",
    "href": "workshops/5.2_nf_core_modules.html#nf-core-modules",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1 nf-core modules",
    "text": "5.1 nf-core modules\nPreviously, we used the nf-core pipelines command to create our initial pipeline template. We can now add modules to this template using nf-core modules. This command also contains subcommands, which can be used to manage modules.\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n           \n                                                                                             \n Usage: nf-core modules [OPTIONS] COMMAND [ARGS]...                                          \n                                                                                             \n Commands to manage Nextflow DSL2 modules (tool wrappers).                                   \n                                                                                             \n╭─ For pipelines ───────────────────────────────────────────────────────────────────────────╮\n│ list         List modules in a local pipeline or remote repository.                       │\n│ info         Show developer usage information about a given module.                       │\n│ install      Install DSL2 modules within a pipeline.                                      │\n│ update       Update DSL2 modules within a pipeline.                                       │\n│ remove       Remove a module from a pipeline.                                             │\n│ patch        Create a patch file for minor changes in a module                            │\n╰───────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Developing new modules ──────────────────────────────────────────────────────────────────╮\n│ create         Create a new DSL2 module from the nf-core template.                        │\n│ lint           Lint one or more modules in a directory.                                   │\n│ test           Run nf-test for a module.                                                  │\n│ bump-versions  Bump versions for one or more modules in a clone of the nf-core/modules    │\n│                repo.                                                                      │\n╰───────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Options ─────────────────────────────────────────────────────────────────────────────────╮\n│ --git-remote  -g  TEXT  Remote git repo to fetch files from                               │\n│ --branch      -b  TEXT  Branch of git repository hosting modules.                         │\n│ --no-pull     -N        Do not pull in latest changes to local clone of modules           │\n│                         repository.                                                       │\n│ --help        -h        Show this message and exit.                                       │\n╰───────────────────────────────────────────────────────────────────────────────────────────╯\nTo check what modules have been added into our pipeline run nf-core modules list local within the pipeline folder.\ncd ./nf-core-customrnaseq\n\nnf-core modules list local\nThis command will output each module that is part of the pipeline, the source repository, version, message at last commit, and the date it was last modified.\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Repository type: pipeline                                                           \nINFO     Modules installed in '.':                                                           \n                                                                                             \n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name ┃ Repository      ┃ Version SHA ┃ Message                        ┃ Date       ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc      │ nf-core/modules │ 0810805     │ use detailed profiles in       │ 2024-12-13 │\n│             │                 │             │ tests/nf-test.config (#7207)   │            │\n│ multiqc     │ nf-core/modules │ f0719ae     │ bump multiqc 1.26 to 1.27      │ 2025-01-27 │\n│             │                 │             │ (#7364)                        │            │\n└─────────────┴─────────────────┴─────────────┴────────────────────────────────┴────────────┘\nTo obtain a list of all available nf-core modules available, nf-core modules list remote can be used.\n\n5.1.1 Installing modules nf-core modules install\nPreviously, we created a simple workflow that indexed the transcriptome file (INDEX), performed quantification (QUANTIFICATION) and FastQC (FASTQC) on the sample FASTQ files. Finally, perfomed MultiQC on both the outputs of QUANTIFICATION and FASTQC.\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [sad_jennings] DSL2 - revision: cfae7ccc0e\nexecutor &gt;  local (7)\n[b5/6bece3] process &gt; INDEX              [100%] 1 of 1 ✔\n[32/46f20b] process &gt; QUANTIFICATION (3) [100%] 3 of 3 ✔\n[44/27aa8d] process &gt; FASTQC (2)         [100%] 3 of 3 ✔\nWe will be recreating these steps, using the nf-core template as a guide.\nRecall that when we created each process, we manually defined the input(s) and their structure, the output(s) and their structure, the process script, and any containers required to execute the process. This can become time consuming, especially when creating pipelines with many processes.\nBefore creating a module ourselves, we should always first check the nf-core modules page to see if the module we are interested in exists.\nFor our INDEX and QUANTIFICATION processes, let’s check if are already modules that use salmon index and salmon quant.\nprocess INDEX {\n    container \"https://depot.galaxyproject.org/singularity/salmon:1.10.3--h6dccd9a_2\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\nprocess QUANTIFICATION {\n    container \"https://depot.galaxyproject.org/singularity/salmon:1.10.3--h6dccd9a_2\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\nSimilar to nf-core pipelines, nf-core modules also have documentation that specifies the inputs to the process, the outputs to the process, and how to install the module to our pipeline.\nTo install a module, navigate inside your nf-core pipeline folder. Then, the module can be installed. First, install the SALMON_INDEX module.\ncd ./nf-core-customrnaseq\n\nnf-core modules install salmon/index\nIf the module has been installed successfully, you will get the following message, which also includes how the module can be included in the analysis script workflows/customrnaseq.nf\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Installing 'salmon/index'                                                           \nINFO     Use the following statement to include this module:                                 \n                                                                                             \n include { SALMON_INDEX } from '../modules/nf-core/salmon/index/main'                        \n                                                                  \nBy default, the example include command provided by running nf-core modules install will provide the relative output path from the analysis workflow file. Therefore, this can be copied directly into workflows/customrnaseq.nf. Recall that if you are importing the module from within a subworkflow file in the subworkflows folder, you will need to change the relative file path to match the new structure.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nIf you are installing the module outside of the nf-core pipeline folder, you may see the below message:\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nWARNING  'repository_type' not defined in .nf-core.yml                                       \n? Is this repository a pipeline or a modules repository? (Use arrow keys)\n » Pipeline\n   Modules repository\nCancel your nf-core modules install command, navigate inside your pipeline folder, then retry.\n\n\n\nNote that this module has been installed inside the modules folder in the pipeline, inside the nf-core subfolder.\nmodules\n└── nf-core\n    ├── fastqc\n    |   ├── ...\n    ├── multiqc\n    |   ├── ...\n    └── salmon\n        └── index\n            ├── environment.yml\n            ├── main.nf\n            ├── meta.yml\n            └── tests\n                ├── main.nf.test\n                └── main.nf.test.snap\nExercise: Using the nf-core modules page, find a module that performs salmon quant. Install it to your pipeline, and check what location it has been added. Then, include the module in your analysis workflow script workflows/customrnaseq.nf.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe nf-core SALMON_QUANT module can perform salmon quant.\nTo install this module, the following command can be ran inside my pipeline folder nf-core-customrnaseq:\nnf-core modules install salmon/quant\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Installing 'salmon/quant'                                                           \nINFO     Use the following statement to include this module:                                 \n                                                                                             \n include { SALMON_QUANT } from '../modules/nf-core/salmon/quant/main'                        \n                                                                       \nThis module has been successfully installed in the modules folder, inside nf-core.\nmodules\n└── nf-core\n    ├── ...\n    └── salmon\n        ├── index\n        │   ├── ...\n        └── quant\n            ├── environment.yml\n            ├── main.nf\n            ├── meta.yml\n            └── tests\n                ├── main.nf.test\n                ├── main.nf.test.snap\n                └── nextflow.config\nSince the example include command provided by running nf-core modules install will provide the relative output path from the analysis workflow file, the following can be copied directly into workflows/customrnaseq.nf.\ninclude { SALMON_QUANT } from '../modules/nf-core/salmon/quant/main'  \n\n\n\nWe can now list all the modules installed in our pipeline:\nnf-core modules list local\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Repository type: pipeline                                                                                                                                                                                       \nINFO     Modules installed in '.':                                                                                                                                                                                       \n                                                                                                                                                                                                                         \n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name  ┃ Repository      ┃ Version SHA ┃ Message                                                                     ┃ Date       ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc       │ nf-core/modules │ 0810805     │ use detailed profiles in tests/nf-test.config (#7207)                       │ 2024-12-13 │\n│ multiqc      │ nf-core/modules │ f0719ae     │ bump multiqc 1.26 to 1.27 (#7364)                                           │ 2025-01-27 │\n│ salmon/index │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative path in antismash/antismashlite tests (#8116) │ 2025-03-26 │\n│ salmon/quant │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative path in antismash/antismashlite tests (#8116) │ 2025-03-26 │\n└──────────────┴─────────────────┴─────────────┴─────────────────────────────────────────────────────────────────────────────┴────────────┘\n\nModule list: modules.json\nThe modules.json contains detailed information of all the modules installed in the pipeline, where it was installed from, and the version of the module.\n\n\n\n\n\n\nmodules.json\n\n\n\n\n\n{\n    \"name\": \"nf-core/customrnaseq\",\n    \"homePage\": \"https://github.com/nf-core/customrnaseq\",\n    \"repos\": {\n        \"https://github.com/nf-core/modules.git\": {\n            \"modules\": {\n                \"nf-core\": {\n                    ...\n\n                    \"salmon/index\": {\n                        \"branch\": \"master\",\n                        \"git_sha\": \"05954dab2ff481bcb999f24455da29a5828af08d\",\n                        \"installed_by\": [\"modules\"]\n                    },\n                    \"salmon/quant\": {\n                        \"branch\": \"master\",\n                        \"git_sha\": \"05954dab2ff481bcb999f24455da29a5828af08d\",\n                        \"installed_by\": [\"modules\"]\n                    }\n                }\n            },\n            \"subworkflows\": {\n                \"nf-core\": {\n                    \"utils_nextflow_pipeline\": {\n                        \"branch\": \"master\",\n                        \"git_sha\": \"c2b22d85f30a706a3073387f30380704fcae013b\",\n                        \"installed_by\": [\"subworkflows\"]\n                    },\n\n                    ...\n\n                    }\n    ...\n}\n\n\n\nWhat if we remove the module folder for salmon/quant?\nrm -r modules/nf-core/salmon/quant\n\nls modules/nf-core/salmon/quant\nls: cannot access 'modules/nf-core/salmon/quant': No such file or directory\nLet’s now rerun the following command:\nnf-core modules list local\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Repository type: pipeline                                                           \nINFO     Reinstalling modules found in 'modules.json' but missing from directory:            \n         'modules/nf-core/salmon/quant'                                                      \nINFO     Modules installed in '.':                                                           \n                                                                                             \n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name  ┃ Repository      ┃ Version SHA ┃ Message                       ┃ Date       ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc       │ nf-core/modules │ 0810805     │ use detailed profiles in      │ 2024-12-13 │\n│              │                 │             │ tests/nf-test.config (#7207)  │            │\n│ multiqc      │ nf-core/modules │ f0719ae     │ bump multiqc 1.26 to 1.27     │ 2025-01-27 │\n│              │                 │             │ (#7364)                       │            │\n│ salmon/index │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative │ 2025-03-26 │\n│              │                 │             │ path in                       │            │\n│              │                 │             │ antismash/antismashlite tests │            │\n│              │                 │             │ (#8116)                       │            │\n│ salmon/quant │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative │ 2025-03-26 │\n│              │                 │             │ path in                       │            │\n│              │                 │             │ antismash/antismashlite tests │            │\n│              │                 │             │ (#8116)                       │            │\n└──────────────┴─────────────────┴─────────────┴───────────────────────────────┴────────────┘\nIn the command output, we see that our deleted module is automatically reinstalled, based on the list in modules.json\nls modules/nf-core/salmon/quant\nenvironment.yml  main.nf  meta.yml  tests\n\n\nInstalling specific module versions\nInside the modules.json file, the git_sha is listed, which corresponds to a specific version of the module. To obtain the SHA for a module, navigate to the module Github page, and click History.\n\nThe full SHA path can then be copied.\n\nIf there is a specific module version we would like to install, this can be done using nf-core modules. Run the following command:\nnf-core modules install -h\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\n                                                                                             \n Usage: nf-core modules install [OPTIONS] &lt;tool&gt; or &lt;tool/subtool&gt;                           \n                                                                                             \n Install DSL2 modules within a pipeline.                                                     \n                                                                                             \n╭─ Options ─────────────────────────────────────────────────────────────────────────────────╮\n│ --dir     -d  PATH          Pipeline directory. [default: current working directory]      │\n│ --prompt  -p                Prompt for the version of the module                          │\n│ --force   -f                Force reinstallation of module if it already exists           │\n│ --sha     -s  &lt;commit sha&gt;  Install module at commit SHA                                  │\n│ --help    -h                Show this message and exit.                                   │\n╰───────────────────────────────────────────────────────────────────────────────────────────╯\nThe SHA version can be specified using the --sha parameter. Since we have already installed the module, the --force parameter will also be required.\nExercise: Install the salmon/quant module, for SHA version 85b5f8a0d9df9ce7587af50e2ee75b37c97515c6. Use either nf-core modules list local or the modules.json file to verify the correct version was installed.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe following command can be used to install salmon/quant, SHA version 85b5f8a0d9df9ce7587af50e2ee75b37c97515c6.\nnf-core modules install --sha 85b5f8a0d9df9ce7587af50e2ee75b37c97515c6 --force salmon/quant\nInside the modules.json, the git_sha has been updated to the correct version.\n\"salmon/quant\": {\n    \"branch\": \"master\",\n    \"git_sha\": \"85b5f8a0d9df9ce7587af50e2ee75b37c97515c6\",\n    \"installed_by\": [\"modules\"]\n}\nUsing nf-core modules list local, the Version SHA has also been updated\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Repository type: pipeline                                                           \nINFO     Modules installed in '.':                                                           \n                                                                                             \n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name  ┃ Repository      ┃ Version SHA ┃ Message                       ┃ Date       ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc       │ nf-core/modules │ 0810805     │ use detailed profiles in      │ 2024-12-13 │\n│              │                 │             │ tests/nf-test.config (#7207)  │            │\n│ multiqc      │ nf-core/modules │ f0719ae     │ bump multiqc 1.26 to 1.27     │ 2025-01-27 │\n│              │                 │             │ (#7364)                       │            │\n│ salmon/index │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative │ 2025-03-26 │\n│              │                 │             │ path in                       │            │\n│              │                 │             │ antismash/antismashlite tests │            │\n│              │                 │             │ (#8116)                       │            │\n│ salmon/quant │ nf-core/modules │ 85b5f8a     │ Fix language server error in  │ 2025-03-18 │\n│              │                 │             │ salmon/quant (#7843)          │            │\n└──────────────┴─────────────────┴─────────────┴───────────────────────────────┴────────────┘\n\n\n\n\n\n\n5.1.2 Updating modules: nf-core modules update\nA limitation of nf-core modules install is that it doesn’t explicitly print what has been changed within the module. The nf-core modules update command is an alternative that can do this.\nLet’s update salmon/quant\nnf-core modules update salmon/quant\nThe command will then prompt you if you wish to view the differences between the current installation and the update.\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\n? Do you want to view diffs of the proposed changes? (Use arrow keys)\n » No previews, just update everything\n   Preview diff in terminal, choose whether to update files\n   Just write diffs to a patch file\nUse the up/down arrow keys can to choose Preview diff in terminal, choose whether to update files and press Enter.\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\n? Do you want to view diffs of the proposed changes? Preview diff in terminal, choose whether to update files\nINFO     Changes in component 'nf-core/salmon/quant' between (85b5f8a0d9df9ce7587af50e2ee75b37c97515c6) and (05954dab2ff481bcb999f24455da29a5828af08d)                                                                   \nINFO     'modules/nf-core/salmon/quant/environment.yml' is unchanged                                                                                                                                                     \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/meta.yml' is unchanged                                                                                                                                                            \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/main.nf' is unchanged                                                                                                                                                             \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/tests/nextflow.config' is unchanged                                                                                                                                               \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/tests/main.nf.test' is unchanged                                                                                                                                                  \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/tests/main.nf.test.snap' is unchanged                                                                                                                                             \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/tests/tags.yml' was removed                                                                                                                                                       \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n? Update module 'salmon/quant'? (y/N)\nWe see that the only change between the two module versions is that tags.yml was removed. Since these changes are acceptable to us, we can now press y. Your module should now be updated.\nINFO     Updating 'nf-core/salmon/quant'                                                                                                                                                                                 \nINFO     Updates complete ✨   \n\n\n5.1.3 Removing modules\nPreviously, we saw that by simply removing the module folder didn’t delete the module from the pipeline. The pipeline automatically searches for missing module files and redownloads them, based on the SHA version specified in modules.json.\nTo remove a module from a pipeline, the following can be used:\nnf-core modules remove\nExercise:\n\nUse the nf-core modules page to find and install any module of any version to your pipeline\nVerify the module has been installed by checking the modules.json file, or by listing the modules in the pipeline\nRemove the newly installed module, and ensure it is no longer part of the pipeline\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe module chosen is samtools/sort. To install this module, the following command can be used (inside the pipeline folder):\nnf-core modules install samtools/sort\nListing the modules in the pipeline:\nnf-core modules list local\n                                      ,--./,-.\n      ___     __   __   __   ___     /,-._.--~\\ \n|\\ | |__  __ /  ` /  \\ |__) |__         }  {\n| \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                      `._,._,'\n\nnf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Repository type: pipeline                                                                                                 \nINFO     Modules installed in '.':                                                                                                 \n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name   ┃ Repository      ┃ Version SHA ┃ Message                                                            ┃ Date       ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc        │ nf-core/modules │ 0810805     │ use detailed profiles in tests/nf-test.config (#7207)              │ 2024-12-13 │\n│ multiqc       │ nf-core/modules │ f0719ae     │ bump multiqc 1.26 to 1.27 (#7364)                                  │ 2025-01-27 │\n│ salmon/index  │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative path in antismash/antismashlite      │ 2025-03-26 │\n│               │                 │             │ tests (#8116)                                                      │            │\n│ salmon/quant  │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative path in antismash/antismashlite      │ 2025-03-26 │\n│               │                 │             │ tests (#8116)                                                      │            │\n│ samtools/sort │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative path in antismash/antismashlite      │ 2025-03-26 │\n│               │                 │             │ tests (#8116)                                                      │            │\n└───────────────┴─────────────────┴─────────────┴────────────────────────────────────────────────────────────────────┴────────────┘\nThe module is also present in modules.json:\n{\n    \"name\": \"nf-core/customrnaseq\",\n    \"homePage\": \"https://github.com/nf-core/customrnaseq\",\n    \"repos\": {\n        \"https://github.com/nf-core/modules.git\": {\n            \"modules\": {\n                \"nf-core\": {\n                    ...\n                    \"samtools/sort\": {\n                        \"branch\": \"master\",\n                        \"git_sha\": \"05954dab2ff481bcb999f24455da29a5828af08d\",\n                        \"installed_by\": [\"modules\"]\n                    }\n                }\n            },\n    ...\n}\nTo remove ssamtools/sort, the following command is used:\nnf-core modules remove samtools/sort\n                                          ,--./,-.\n      ___     __   __   __   ___     /,-._.--~\\ \n|\\ | |__  __ /  ` /  \\ |__) |__         }  {\n| \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                      `._,._,'\n\nnf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Removed files for 'samtools/sort' and its dependencies 'samtools/sort'.    \nChecking the module has been correctly removed:\nnf-core modules list local\nThe module samtools/sort has been removed from the modules list.\n\n                                      ,--./,-.\n      ___     __   __   __   ___     /,-._.--~\\ \n|\\ | |__  __ /  ` /  \\ |__) |__         }  {\n| \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                      `._,._,'\n\nnf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Repository type: pipeline                                                                                                                  \nINFO     Modules installed in '.':                                                                                                                  \n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name  ┃ Repository      ┃ Version SHA ┃ Message                                                                     ┃ Date       ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc       │ nf-core/modules │ 0810805     │ use detailed profiles in tests/nf-test.config (#7207)                       │ 2024-12-13 │\n│ multiqc      │ nf-core/modules │ f0719ae     │ bump multiqc 1.26 to 1.27 (#7364)                                           │ 2025-01-27 │\n│ salmon/index │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative path in antismash/antismashlite tests (#8116) │ 2025-03-26 │\n│ salmon/quant │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative path in antismash/antismashlite tests (#8116) │ 2025-03-26 │\n└──────────────┴─────────────────┴─────────────┴─────────────────────────────────────────────────────────────────────────────┴────────────┘"
  },
  {
    "objectID": "workshops/5.2_nf_core_modules.html#nf-core-subworkflows",
    "href": "workshops/5.2_nf_core_modules.html#nf-core-subworkflows",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2 nf-core subworkflows",
    "text": "5.2 nf-core subworkflows\nLike modules, subworkflows can be added to a workflow using the nf-core tools suite.\nnf-core subworkflows\nThe subworkflows command contains similar sub-commands to modules\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\n                                                                                              \n Usage: nf-core subworkflows [OPTIONS] COMMAND [ARGS]...                                      \n                                                                                              \n Commands to manage Nextflow DSL2 subworkflows (tool wrappers).                               \n                                                                                              \n╭─ For pipelines ────────────────────────────────────────────────────────────────────────────╮\n│ list        List subworkflows in a local pipeline or remote repository.                    │\n│ info        Show developer usage information about a given subworkflow.                    │\n│ install     Install DSL2 subworkflow within a pipeline.                                    │\n│ update      Update DSL2 subworkflow within a pipeline.                                     │\n│ remove      Remove a subworkflow from a pipeline.                                          │\n╰────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Developing new subworkflows ──────────────────────────────────────────────────────────────╮\n│ create      Create a new subworkflow from the nf-core template.                            │\n│ lint        Lint one or more subworkflows in a directory.                                  │\n│ test        Run nf-test for a subworkflow.                                                 │\n╰────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Commands ─────────────────────────────────────────────────────────────────────────────────╮\n│ patch     Create a patch file for minor changes in a subworkflow                           │\n╰────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Options ──────────────────────────────────────────────────────────────────────────────────╮\n│ --git-remote  -g  TEXT  Remote git repo to fetch files from                                │\n│ --branch      -b  TEXT  Branch of git repository hosting modules.                          │\n│ --no-pull     -N        Do not pull in latest changes to local clone of modules            │\n│                         repository.                                                        │\n│ --help        -h        Show this message and exit.                                        │\n╰────────────────────────────────────────────────────────────────────────────────────────────╯\n\nlist: list the installed subworkflows in the pipeline\ninstall: install a subworkflow to the pipeline\nupdate: update the pipeline version using the GitHub SHA\nremove: remoe a subworkflow from the pipeline\n\nExercise: Search the nf-core subworkflows page for the fastq_subsample_fq_salmon subworkflow. Install it into the workflow, and list all the subworkflows that are installed in the pipeline.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe fastq_subsample_fq_salmon subworkflow is located in the nf-core subworkflows page here. To install the module, the following command can be used:\nnf-core subworkflows install fastq_subsample_fq_salmon\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Installing 'fastq_subsample_fq_salmon'                                                                                                     \nINFO     Use the following statement to include this subworkflow:                                                                                   \n                                                                                                                                                    \n include { FASTQ_SUBSAMPLE_FQ_SALMON } from '../subworkflows/nf-core/fastq_subsample_fq_salmon/main'  \nTo list the subworkflows in the pipeline:\nnf-core subworkflows list local \nThis output contains the newly installed subworkflow fastq_subsample_fq_salmon:\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Repository type: pipeline                                                                                                                  \nINFO     Subworkflows installed in '.':                                                                                                             \n                                                                                                                                                    \n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Subworkflow Name          ┃ Repository      ┃ Version SHA ┃ Message                                                                 ┃ Date       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastq_subsample_fq_salmon │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative path in antismash/antismashlite tests     │ 2025-03-26 │\n│                           │                 │             │ (#8116)                                                                 │            │\n│ utils_nextflow_pipeline   │ nf-core/modules │ c2b22d8     │ Utils Nextflow: Avoid unused variables (#7070)                          │ 2024-11-24 │\n│ utils_nfcore_pipeline     │ nf-core/modules │ 51ae540     │ Replace switch with else-if in utils_nfcore_pipeline (#7168)            │ 2024-12-06 │\n│ utils_nfschema_plugin     │ nf-core/modules │ 2fd2cd6     │ fails with lenient mode (#6865)                                         │ 2024-10-28 │\n└───────────────────────────┴─────────────────┴─────────────┴─────────────────────────────────────────────────────────────────────────┴────────────┘\n\n\n\n\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, Nextflow Patterns materials from Nextflow, nf-core nf-core tools documentation and nf-validation –&gt;"
  },
  {
    "objectID": "workshops/3.1_creating_a_workflow.html",
    "href": "workshops/3.1_creating_a_workflow.html",
    "title": "Introduction to Nextflow",
    "section": "",
    "text": "Objectives\n\n\n\n\nDevelop a basic Nextflow workflow\nRead data of different types into a Nextflow workflow\nOutput Nextflow process results to a predefined directory\n\n\n\n\n\nLet’s first create a folder inside our work directory called lesson3.2 and move into it:\nmkdir ./lesson3.2 && cd $_\nLet’s create a Nextflow script rnaseq.nf for a RNA-seq workflow. The code begins with a shebang, which declares Nextflow as the interpreter.\n#!/usr/bin/env nextflow\nWe will use the following data:\nls ../training/nf-training/data/ggal\ngut_1.fq  gut_2.fq  liver_1.fq  liver_2.fq  lung_1.fq  lung_2.fq  transcriptome.fa\nThis dataset contains three paired FASTQ samples, gut, liver, and lung, along with a transcriptome file transcriptome.fa\nOne way to define these to Nextflow is inside the workflow a Nextflow script. Copy the following into rnaseq.nf:\nparams.reads = \"/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/.../training/nf-training/data/ggal/transcriptome.fa\"\n\n\n\n\n\n\nCaution\n\n\n\nMake sure to put the full path to the location of your data. If relative paths are used, this can cause file value errors.\n\n\nWorkflow parameters can be defined and accessed inside the Nextflow script by prepending the prefix params to a variable name, separated by a dot character, eg. params.reads.\nIn the example above, the reads parameter is defined as multiple .fq files, using the * wildcard and { } – this notation will search for all files with .fq file extension, preceeded by either _1 or _2. The transcriptome_file parameter is defined as one file, /.../training/nf-training/data/ggal/transcriptome.fa.\n\n\n\nIn Nextflow, commands or scripts can be executed inside a process.\nprocess INDEX {\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index -t $transcriptome -i salmon_idx\n    \"\"\"\n}\nThis INDEX process takes one input, and assigns it as the variable transcriptome. The path type qualifier will allow Nextflow to stage the files in the process execution directory, where they can be accessed by the script via the defined variable name, transcriptome. The code between the three double-quotes of the script block will be executed, which accesses the input transcriptome variable using $. The output is a path, with a filename salmon_idx.\nNote that the name of the input file is not used and is only referenced to by the input variable name. This feature allows pipeline tasks to be self-contained and decoupled from the execution environment. As best practice, avoid referencing files that are not defined in the process script.\nTo execute the INDEX process, a workflow scope will need to be added. This scope outlines what processes are executed.\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n}\nHere, the params.transcriptome_file parameter we defined earlier in the Nextflow script is used as an input into the INDEX process. The output of the process is assigned to the index_ch channel.\nRun the Nextflow script:\nnextflow run rnaseq.nf\nERROR ~ Error executing process &gt; 'INDEX'\n\nCaused by:\n  Process `INDEX` terminated with an error exit status (127)\n\nCommand executed:\n\n  salmon index -t transcriptome.fa -i salmon_index\n\nCommand exit status:\n  127\n\nCommand output:\n  (empty)\n\nCommand error:\n  .command.sh: line 2: salmon: command not found\n\nWork dir:\n  /.../work/85/495a21afcaaf5f94780aff6b2a964c\n\nTip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`\n\n -- Check '.nextflow.log' file for details\nWhen a process execution exits with a non-zero exit status, the workflow will be stopped. Nextflow will output the cause of the error, the command that caused the error, the exit status, the standard output (if available), the comand standard error, and the work directory where the process was executed.\nLet’s first look inside the process execution directory:\nls -a work/85/495a21afcaaf5f94780aff6b2a964c \n.   .command.begin  .command.log  .command.run  .exitcode\n..  .command.err    .command.out  .command.sh   transcriptome.fa\nWe can see that the input file transcriptome.fa has been staged inside this process execution directory by being symbolically linked. This allows it to be accessed by the script.\nInside the .command.err script, we can see that the salmon command was not found, resulting in the termination of the Nextflow workflow.\nContainers can be used to execute the process within an environment that contains the package of interest. Create a config file nextflow.config containing the following:\napptainer {\n  enabled = true\n  autoMounts = true\n  cacheDir = \"/home/&lt;username&gt;/apptainer_cache\"\n}\nThis will download any containers to the directory /home/&lt;username&gt;/apptainer_cache. Before a process is executed, this folder will be\nThe container process directive can be used to specify the required container:\nprocess INDEX {\n    container \"https://depot.galaxyproject.org/singularity/salmon:1.10.3--h6dccd9a_2\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\nRun the Nextflow script:\nnextflow run rnaseq.nf\nN E X T F L O W  ~  version 23.03.2\nLaunching `rnaseq.nf` [distraught_goldwasser] DSL2 - revision: bdebf34e16\nexecutor &gt;  local (1)\n[37/7ef8f0] process &gt; INDEX [100%] 1 of 1 ✔\nThe newly created nextflow.config file does not need to be specified in the nextflow run command. This file is automatically searched for and used by Nextflow.\n\n\n\nPreviously, we have defined the reads parameter to be the following:\nparams.reads = \"/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nExercise: Convert the reads parameter into a tuple channel called reads_ch, where the first element is a unique grouping key, and the second element is the paired .fq files. Then, view the contents of reads_ch\n\n\n\n\n\n\nSolution\n\n\n\n\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\nreads_ch.view()\nThe fromFilePairs channel factory will automatically group input files into a tuple with a unique grouping key. The view() channel operator can be used to view the contents of the channel.\nnextflow run rnaseq.nf\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/.../training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, [/.../training/nf-training/data/ggal&gt;/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\n\n\n\n\n\n\nLet’s add a new process QUANTIFICATION that uses both the indexed transcriptome file and the .fq file pairs to execute the salmon quant command.\nprocess QUANTIFICATION {\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\nThe QUANTIFICATION process takes two inputs, the first is the path to the salmon_index created from the INDEX process. The second input is set to match the output of fromFilePairs – a tuple where the first element is a value (ie. grouping key), and the second element is a list of paths to the .fq reads.\nIn the script block, the salmon quant command saves the output of the tool as $sample_id. This output is emitted by the QUANTIFICATION process, using $ to access the Nextflow variable.\nExercise:\nSet the following as the execution container for QUANTIFICATION:\n/home/&lt;username&gt;/apptainer_cache/depot.galaxyproject.org-singularity-salmon-1.10.3--h6dccd9a_2.img\nAssign index_ch and reads_ch as the inputs to this process, and save the process outputs as quant_ch. Then, view the contents of quant_ch\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo assign a container to a process, the container directive can be used.\nprocess QUANTIFICATION {\n    container \"/home/&lt;username&gt;/apptainer_cache/depot.galaxyproject.org-singularity-salmon-1.10.3--h6dccd9a_2.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\nTo run the QUANTIFICATION process and emit the outputs as quant_ch, the following can be added to the end of the workflow block:\nquant_ch = QUANTIFICATION(index_ch, reads_ch)\nquant_ch.view()\nThe script can now be run:\nnextflow run rnaseq.nf \nN E X T F L O W  ~  version 23.03.2\nLaunching `rnaseq.nf` [elated_cray] DSL2 - revision: abe41f4f69\nexecutor &gt;  local (4)\n[e5/e75095] process &gt; INDEX              [100%] 1 of 1 ✔\n[4c/68a000] process &gt; QUANTIFICATION (1) [100%] 3 of 3 ✔\n/.../work/b1/d861d26d4d36864a17d2cec8d67c80/liver\n/.../work/b4/a6545471c1f949b2723d43a9cce05f/lung\n/.../work/4c/68a000f7c6503e8ae1fe4d0d3c93d8/gut\nIn the Nextflow output, we can see that the QUANTIFICATION process has been ran three times, since the reads_ch consists of three elements. Nextflow will automatically run the QUANTIFICATION process on each of the elements in the input channel, creating separate process execution work directories for each execution.\n\n\n\n\n\n\nNow, let’s implement a FASTQC quality control process for the input fastq reads.\nExercise:\nCreate a process called FASTQC that takes reads_ch as an input, and declares the process input to be a tuple matching the structure of reads_ch, where the first element is assigned the variable sample_id, and the second variable is assigned the variable reads. This FASTQC process will first create an output directory fastqc_${sample_id}_logs, then perform fastqc on the input reads and save the results in the newly created directory fastqc_${sample_id}_logs:\nmkdir fastqc_${sample_id}_logs\nfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\nTake fastqc_${sample_id}_logs as the output of the process, and assign it to the channel fastqc_ch. Finally, specify the process container to be the following:\n'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0'\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis will be the structure of the FASTQC process:\nprocess FASTQC {\n    container ...\n\n    input:\n    tuple ...\n\n    output:\n    path ...\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\nChange the container to be the path to the container in our cache directory.\nFrom the script block, we can see that the outputs of the process will be fastqc_${sample_id}_logs. Add this to the output section.\nFrom the script block, we can see that the inputs required are sample_id and reads. Add this into your input tuple, ensuring the tuple structure matches the structure of reads_ch. Don’t forget to add either val or path qualifiers within the tuple!\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe process FASTQC is created in rnaseq.nf. Since the input channel is a tuple, the process input declaration is a tuple containing elements that match the structure of the incoming channel. The first element of the tuple is assigned the variable sample_id, and the second element of the tuple is assigned the variable reads. The relevant container is specified using the container process directive.\nprocess FASTQC {\n    container 'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\nIn the workflow scope, the following can be added:\nfastqc_ch = FASTQC(reads_ch)\nThe FASTQC process is called, taking reads_ch as an input. The output of the process is assigned to be fastqc_ch.\nnextflow run rnaseq.nf\nN E X T F L O W  ~  version 23.03.2\nLaunching `rnaseq.nf` [sad_jennings] DSL2 - revision: cfae7ccc0e\nexecutor &gt;  local (7)\n[b5/6bece3] process &gt; INDEX              [100%] 1 of 1 ✔\n[32/46f20b] process &gt; QUANTIFICATION (3) [100%] 3 of 3 ✔\n[44/27aa8d] process &gt; FASTQC (2)         [100%] 3 of 3 ✔\nIn the Nextflow output, we can see that the FASTQC has been ran three times as expected, since the reads_ch consists of three elements.\n\n\n\n\n\n\nSo far, the generated outputs have all been saved inside the Nextflow work directory. For the FASTQC process, the specified output directory is only created inside the process execution directory. To save results to a custom folder, the publishDir process directive can be used.\nLet’s create a new MULTIQC process in our workflow that takes the outputs from the QUANTIFICATION and FASTQC processes, to create a final report using the multiqc tool. We will then publish these process outputs to a directory outside of the process execution directory.\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container 'https://depot.galaxyproject.org/singularity/multiqc:1.28--pyhdfd78af_0'\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\nIn the MULTIQC process, the multiqc command is performed on both quantification and fastqc inputs. Note that in multiqc, the specific inputs don’t need to be provided to the command.\nWe then publish the output report to a directory defined by the outdir parameter. Only files that match the declaration in the output block are published, not all the outputs of a process (ie. only files with .html file extensions will be copied to the output directory). By default, files are published to the output folder through a symbolic link to the file produced in the process work directory. This behavior can be modified using the mode option, eg. copy, which copies the file from the process execution directory to the specified output directory.\nNow, add the following to the end of workflow scope:\nmultiqc_ch = MULTIQC(quant_ch, fastqc_ch)\nRun the pipeline, specifying an output directory using the outdir parameter:\nnextflow run rnaseq.nf --outdir \"results\"\nA results directory containing the multiqc reports now will be created.\nls results\ngut.html  liver.html  lung.html\n\n\n\n\n\n\nFull rnaseq.nf workflow\n\n\n\n\n\n#!/usr/bin/env nextflow\n\nparams.reads = \"/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/.../training/nf-training/data/ggal/transcriptome.fa\"\n\nprocess INDEX {\n    container \"https://depot.galaxyproject.org/singularity/salmon:1.10.3--h6dccd9a_2\"\n    \n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index -t $transcriptome -i salmon_idx\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    container \"/home/&lt;username&gt;/apptainer_cache/depot.galaxyproject.org-singularity-salmon-1.10.3--h6dccd9a_2.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nprocess FASTQC {\n    container 'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container 'https://depot.galaxyproject.org/singularity/multiqc:1.28--pyhdfd78af_0'\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\nworkflow {\n  reads_ch = Channel.fromFilePairs(\"$params.reads\")\n\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QUANTIFICATION(index_ch, reads_ch)\n  quant_ch.view()\n\n  fastqc_ch = FASTQC(reads_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n\n}\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nEnvironments can be defined using the container process directive\nApptainer cache can be defined using the cacheDir variable in the apptainer scope\nThe input declaration for a process must match the structure of the channel that is passed into that process\nOutput files can be saved to a directory using the publishDir process directive\n\n\n\n\n\n\n\n\n\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, and Nextflow Patterns materials from Nextflow and nf-core\n^*Draft for Future Sessions"
  },
  {
    "objectID": "workshops/3.1_creating_a_workflow.html#creating-an-rnaseq-workflow",
    "href": "workshops/3.1_creating_a_workflow.html#creating-an-rnaseq-workflow",
    "title": "Introduction to Nextflow",
    "section": "",
    "text": "Objectives\n\n\n\n\nDevelop a basic Nextflow workflow\nRead data of different types into a Nextflow workflow\nOutput Nextflow process results to a predefined directory\n\n\n\n\n\nLet’s first create a folder inside our work directory called lesson3.2 and move into it:\nmkdir ./lesson3.2 && cd $_\nLet’s create a Nextflow script rnaseq.nf for a RNA-seq workflow. The code begins with a shebang, which declares Nextflow as the interpreter.\n#!/usr/bin/env nextflow\nWe will use the following data:\nls ../training/nf-training/data/ggal\ngut_1.fq  gut_2.fq  liver_1.fq  liver_2.fq  lung_1.fq  lung_2.fq  transcriptome.fa\nThis dataset contains three paired FASTQ samples, gut, liver, and lung, along with a transcriptome file transcriptome.fa\nOne way to define these to Nextflow is inside the workflow a Nextflow script. Copy the following into rnaseq.nf:\nparams.reads = \"/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/.../training/nf-training/data/ggal/transcriptome.fa\"\n\n\n\n\n\n\nCaution\n\n\n\nMake sure to put the full path to the location of your data. If relative paths are used, this can cause file value errors.\n\n\nWorkflow parameters can be defined and accessed inside the Nextflow script by prepending the prefix params to a variable name, separated by a dot character, eg. params.reads.\nIn the example above, the reads parameter is defined as multiple .fq files, using the * wildcard and { } – this notation will search for all files with .fq file extension, preceeded by either _1 or _2. The transcriptome_file parameter is defined as one file, /.../training/nf-training/data/ggal/transcriptome.fa.\n\n\n\nIn Nextflow, commands or scripts can be executed inside a process.\nprocess INDEX {\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index -t $transcriptome -i salmon_idx\n    \"\"\"\n}\nThis INDEX process takes one input, and assigns it as the variable transcriptome. The path type qualifier will allow Nextflow to stage the files in the process execution directory, where they can be accessed by the script via the defined variable name, transcriptome. The code between the three double-quotes of the script block will be executed, which accesses the input transcriptome variable using $. The output is a path, with a filename salmon_idx.\nNote that the name of the input file is not used and is only referenced to by the input variable name. This feature allows pipeline tasks to be self-contained and decoupled from the execution environment. As best practice, avoid referencing files that are not defined in the process script.\nTo execute the INDEX process, a workflow scope will need to be added. This scope outlines what processes are executed.\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n}\nHere, the params.transcriptome_file parameter we defined earlier in the Nextflow script is used as an input into the INDEX process. The output of the process is assigned to the index_ch channel.\nRun the Nextflow script:\nnextflow run rnaseq.nf\nERROR ~ Error executing process &gt; 'INDEX'\n\nCaused by:\n  Process `INDEX` terminated with an error exit status (127)\n\nCommand executed:\n\n  salmon index -t transcriptome.fa -i salmon_index\n\nCommand exit status:\n  127\n\nCommand output:\n  (empty)\n\nCommand error:\n  .command.sh: line 2: salmon: command not found\n\nWork dir:\n  /.../work/85/495a21afcaaf5f94780aff6b2a964c\n\nTip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`\n\n -- Check '.nextflow.log' file for details\nWhen a process execution exits with a non-zero exit status, the workflow will be stopped. Nextflow will output the cause of the error, the command that caused the error, the exit status, the standard output (if available), the comand standard error, and the work directory where the process was executed.\nLet’s first look inside the process execution directory:\nls -a work/85/495a21afcaaf5f94780aff6b2a964c \n.   .command.begin  .command.log  .command.run  .exitcode\n..  .command.err    .command.out  .command.sh   transcriptome.fa\nWe can see that the input file transcriptome.fa has been staged inside this process execution directory by being symbolically linked. This allows it to be accessed by the script.\nInside the .command.err script, we can see that the salmon command was not found, resulting in the termination of the Nextflow workflow.\nContainers can be used to execute the process within an environment that contains the package of interest. Create a config file nextflow.config containing the following:\napptainer {\n  enabled = true\n  autoMounts = true\n  cacheDir = \"/home/&lt;username&gt;/apptainer_cache\"\n}\nThis will download any containers to the directory /home/&lt;username&gt;/apptainer_cache. Before a process is executed, this folder will be\nThe container process directive can be used to specify the required container:\nprocess INDEX {\n    container \"https://depot.galaxyproject.org/singularity/salmon:1.10.3--h6dccd9a_2\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\nRun the Nextflow script:\nnextflow run rnaseq.nf\nN E X T F L O W  ~  version 23.03.2\nLaunching `rnaseq.nf` [distraught_goldwasser] DSL2 - revision: bdebf34e16\nexecutor &gt;  local (1)\n[37/7ef8f0] process &gt; INDEX [100%] 1 of 1 ✔\nThe newly created nextflow.config file does not need to be specified in the nextflow run command. This file is automatically searched for and used by Nextflow.\n\n\n\nPreviously, we have defined the reads parameter to be the following:\nparams.reads = \"/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nExercise: Convert the reads parameter into a tuple channel called reads_ch, where the first element is a unique grouping key, and the second element is the paired .fq files. Then, view the contents of reads_ch\n\n\n\n\n\n\nSolution\n\n\n\n\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\nreads_ch.view()\nThe fromFilePairs channel factory will automatically group input files into a tuple with a unique grouping key. The view() channel operator can be used to view the contents of the channel.\nnextflow run rnaseq.nf\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/.../training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, [/.../training/nf-training/data/ggal&gt;/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\n\n\n\n\n\n\nLet’s add a new process QUANTIFICATION that uses both the indexed transcriptome file and the .fq file pairs to execute the salmon quant command.\nprocess QUANTIFICATION {\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\nThe QUANTIFICATION process takes two inputs, the first is the path to the salmon_index created from the INDEX process. The second input is set to match the output of fromFilePairs – a tuple where the first element is a value (ie. grouping key), and the second element is a list of paths to the .fq reads.\nIn the script block, the salmon quant command saves the output of the tool as $sample_id. This output is emitted by the QUANTIFICATION process, using $ to access the Nextflow variable.\nExercise:\nSet the following as the execution container for QUANTIFICATION:\n/home/&lt;username&gt;/apptainer_cache/depot.galaxyproject.org-singularity-salmon-1.10.3--h6dccd9a_2.img\nAssign index_ch and reads_ch as the inputs to this process, and save the process outputs as quant_ch. Then, view the contents of quant_ch\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo assign a container to a process, the container directive can be used.\nprocess QUANTIFICATION {\n    container \"/home/&lt;username&gt;/apptainer_cache/depot.galaxyproject.org-singularity-salmon-1.10.3--h6dccd9a_2.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\nTo run the QUANTIFICATION process and emit the outputs as quant_ch, the following can be added to the end of the workflow block:\nquant_ch = QUANTIFICATION(index_ch, reads_ch)\nquant_ch.view()\nThe script can now be run:\nnextflow run rnaseq.nf \nN E X T F L O W  ~  version 23.03.2\nLaunching `rnaseq.nf` [elated_cray] DSL2 - revision: abe41f4f69\nexecutor &gt;  local (4)\n[e5/e75095] process &gt; INDEX              [100%] 1 of 1 ✔\n[4c/68a000] process &gt; QUANTIFICATION (1) [100%] 3 of 3 ✔\n/.../work/b1/d861d26d4d36864a17d2cec8d67c80/liver\n/.../work/b4/a6545471c1f949b2723d43a9cce05f/lung\n/.../work/4c/68a000f7c6503e8ae1fe4d0d3c93d8/gut\nIn the Nextflow output, we can see that the QUANTIFICATION process has been ran three times, since the reads_ch consists of three elements. Nextflow will automatically run the QUANTIFICATION process on each of the elements in the input channel, creating separate process execution work directories for each execution.\n\n\n\n\n\n\nNow, let’s implement a FASTQC quality control process for the input fastq reads.\nExercise:\nCreate a process called FASTQC that takes reads_ch as an input, and declares the process input to be a tuple matching the structure of reads_ch, where the first element is assigned the variable sample_id, and the second variable is assigned the variable reads. This FASTQC process will first create an output directory fastqc_${sample_id}_logs, then perform fastqc on the input reads and save the results in the newly created directory fastqc_${sample_id}_logs:\nmkdir fastqc_${sample_id}_logs\nfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\nTake fastqc_${sample_id}_logs as the output of the process, and assign it to the channel fastqc_ch. Finally, specify the process container to be the following:\n'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0'\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis will be the structure of the FASTQC process:\nprocess FASTQC {\n    container ...\n\n    input:\n    tuple ...\n\n    output:\n    path ...\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\nChange the container to be the path to the container in our cache directory.\nFrom the script block, we can see that the outputs of the process will be fastqc_${sample_id}_logs. Add this to the output section.\nFrom the script block, we can see that the inputs required are sample_id and reads. Add this into your input tuple, ensuring the tuple structure matches the structure of reads_ch. Don’t forget to add either val or path qualifiers within the tuple!\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe process FASTQC is created in rnaseq.nf. Since the input channel is a tuple, the process input declaration is a tuple containing elements that match the structure of the incoming channel. The first element of the tuple is assigned the variable sample_id, and the second element of the tuple is assigned the variable reads. The relevant container is specified using the container process directive.\nprocess FASTQC {\n    container 'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\nIn the workflow scope, the following can be added:\nfastqc_ch = FASTQC(reads_ch)\nThe FASTQC process is called, taking reads_ch as an input. The output of the process is assigned to be fastqc_ch.\nnextflow run rnaseq.nf\nN E X T F L O W  ~  version 23.03.2\nLaunching `rnaseq.nf` [sad_jennings] DSL2 - revision: cfae7ccc0e\nexecutor &gt;  local (7)\n[b5/6bece3] process &gt; INDEX              [100%] 1 of 1 ✔\n[32/46f20b] process &gt; QUANTIFICATION (3) [100%] 3 of 3 ✔\n[44/27aa8d] process &gt; FASTQC (2)         [100%] 3 of 3 ✔\nIn the Nextflow output, we can see that the FASTQC has been ran three times as expected, since the reads_ch consists of three elements.\n\n\n\n\n\n\nSo far, the generated outputs have all been saved inside the Nextflow work directory. For the FASTQC process, the specified output directory is only created inside the process execution directory. To save results to a custom folder, the publishDir process directive can be used.\nLet’s create a new MULTIQC process in our workflow that takes the outputs from the QUANTIFICATION and FASTQC processes, to create a final report using the multiqc tool. We will then publish these process outputs to a directory outside of the process execution directory.\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container 'https://depot.galaxyproject.org/singularity/multiqc:1.28--pyhdfd78af_0'\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\nIn the MULTIQC process, the multiqc command is performed on both quantification and fastqc inputs. Note that in multiqc, the specific inputs don’t need to be provided to the command.\nWe then publish the output report to a directory defined by the outdir parameter. Only files that match the declaration in the output block are published, not all the outputs of a process (ie. only files with .html file extensions will be copied to the output directory). By default, files are published to the output folder through a symbolic link to the file produced in the process work directory. This behavior can be modified using the mode option, eg. copy, which copies the file from the process execution directory to the specified output directory.\nNow, add the following to the end of workflow scope:\nmultiqc_ch = MULTIQC(quant_ch, fastqc_ch)\nRun the pipeline, specifying an output directory using the outdir parameter:\nnextflow run rnaseq.nf --outdir \"results\"\nA results directory containing the multiqc reports now will be created.\nls results\ngut.html  liver.html  lung.html\n\n\n\n\n\n\nFull rnaseq.nf workflow\n\n\n\n\n\n#!/usr/bin/env nextflow\n\nparams.reads = \"/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/.../training/nf-training/data/ggal/transcriptome.fa\"\n\nprocess INDEX {\n    container \"https://depot.galaxyproject.org/singularity/salmon:1.10.3--h6dccd9a_2\"\n    \n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index -t $transcriptome -i salmon_idx\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    container \"/home/&lt;username&gt;/apptainer_cache/depot.galaxyproject.org-singularity-salmon-1.10.3--h6dccd9a_2.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nprocess FASTQC {\n    container 'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container 'https://depot.galaxyproject.org/singularity/multiqc:1.28--pyhdfd78af_0'\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\nworkflow {\n  reads_ch = Channel.fromFilePairs(\"$params.reads\")\n\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QUANTIFICATION(index_ch, reads_ch)\n  quant_ch.view()\n\n  fastqc_ch = FASTQC(reads_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n\n}\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nEnvironments can be defined using the container process directive\nApptainer cache can be defined using the cacheDir variable in the apptainer scope\nThe input declaration for a process must match the structure of the channel that is passed into that process\nOutput files can be saved to a directory using the publishDir process directive\n\n\n\n\n\n\n\n\n\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, and Nextflow Patterns materials from Nextflow and nf-core\n^*Draft for Future Sessions"
  },
  {
    "objectID": "workshops/1.3_processes_and_channels.html",
    "href": "workshops/1.3_processes_and_channels.html",
    "title": "Nextflow Channels and Processes",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of Nextflow channels and processes\nGain an understanding of Nextflow syntax\nRead data of different types into a Nextflow workflow\nCreate Nextflow processes consisting of multiple scripting languages\n\n\n\n\n3.1.1. Download data\nDownload the training data to your work directory:\ngit clone https://github.com/nextflow-io/training.git\n\n\n3.1.2. Channels and channel factories\nChannels are a key data structure of Nextflow, used to pass data between processes.\n\nQueue Channels\nA queue channel connects two processes or operators, and is implicitly created by process outputs, or using channel factories such as Channel.of or Channel.fromPath.\nThe training/nf-training/snippet.nf script creates a three-element channel consisting of the strings \"1\", \"2\", and \"3\". This is assigned to a channel called ch. To create the channel, the Channel.of channel factory is used, which can create a channel from arguments such as strings or integers. The view() operator can be used to view the contents of the channel.\n#!/usr/bin/env nextflow\n\nch = Channel.of(\"1\",\"2\",\"3\")\nch.view()\nWhen you run the script training/nf-training/snippet.nf, something similar to the following will be returned:\nnextflow run training/nf-training/snippet.nf\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `training/nf-training/snippet.nf` [intergalactic_rutherford] DSL2 - revision: 59a79bc0dd\n\n1\n2\n3\nNow, modify the script training/nf-training/snippet.nf to the following:\n#!/usr/bin/env nextflow\n\nch1 = Channel.of(1, 2, 3)\nch2 = Channel.of(1)\n\nprocess SUM {\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\n\nworkflow {\n    SUM(ch1, ch2).view()\n}\nWe now have a workflow that creates two queue channels, ch1 and ch2, that are input into the SUM process inside the workflow function. The SUM process sums the two inputs and prints the standard output result using the view() channel operator.\nnextflow run training/nf-training/snippet.nf\nAfter running the script, you may notice the only output is 2. In the output log, we can also see that one task for the SUM process has been launched, as indicated by 1 of 1 ✔.\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `training/nf-training/snippet.nf` [cheeky_shannon] DSL2 - revision: 508a8e352b\n\nexecutor &gt;  local (1)\n[37/0561fd] SUM (1) | 1 of 1 ✔\n2\nSince ch1 and ch2 are queue channels, the single element of ch2 has been consumed when it was initially passed to the SUM process with the first element of ch1. Even though there are other elements to be consumed in ch1, no new process instances will be launched. This is because a process waits until it receives an input value from all the channels declared as an input. The channel values are consumed serially one after another and the first empty channel causes the process execution to stop, even though there are values in other channels.\n\n\nValue Channels\nA value channel differs from a queue channel in that it is bound to a single value, and it can be read unlimited times without consuming its contents. To use the single element in ch2 multiple times, you can use the Channel.value channel factory.\nModify ch2 to the following: ch2 = Channel.value(1) and run the script.\nnextflow run training/nf-training/snippet.nf\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `training/nf-training/snippet.nf` [curious_payne] DSL2 - revision: acd5299c29\n\nexecutor &gt;  local (3)\n[ec/62956a] SUM (2) | 3 of 3 ✔\n2\n\n4\n\n3\nNow that ch2 has been read in as a value channel, its value can be read unlimited times without consuming its contents. We can also see in the output log that three separate tasks have been executed and completed, as indicated by 3 of 3 ✔\nIn many situations, Nextflow will implicitly convert variables into value channels when they are input into a process. Modify the invocation of the SUM process in the workflow function to the following: SUM(ch1, 1).view() and run the script. Here we are directly using the integer 1 as an input to the SUM process.\nnextflow run training/nf-training/snippet.nf\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `training/nf-training/snippet.nf` [astonishing_baekeland] DSL2 - revision: dd0fb5d771\n\nexecutor &gt;  local (3)\n[c3/15235d] SUM (1) | 3 of 3 ✔\n3\n\n4\n\n2\nThis integer has been automatically cast as a value channel, allowing it to be used multiple times for each of the elements in ch1, without its contents being consumed. Again, in the output logs we see that three separate tasks have been executed successfully.\n\n\n\n3.1.3. Processes\nIn Nextflow, a process is the basic computing task to execute functions (i.e., custom scripts or tools).\nThe process definition starts with the keyword process, followed by the process name, commly written in upper case by convention, and finally the process body delimited by curly brackets.\nThe process body can contain many definition blocks:\nprocess &lt; name &gt; {\n    [ directives ] \n\n    input: \n    &lt; process inputs &gt;\n\n    output: \n    &lt; process outputs &gt;\n\n    [script|shell|exec]: \n    \"\"\"\n    &lt; user script to be executed &gt;\n    \"\"\"\n}\n\nDirectives are optional declarations of settings such as cpus, time, executor, container.\ninput: the expected names and qualifiers of variables into the process\noutput: the expected names and qualifiers of variables output from the process\nscript: defines the command to be executed by the process\n\nInside the script block, all $ characters need to be escaped with a \\. This is true for both referencing Bash variables created inside the script block (ie. echo \\$z) as well as performing Bash commands (ie. echo \\$(($x+$y))), but not when referencing Nextflow variables declared in the input (ie. $x+$y).\nprocess SUM {\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    z='SUM'\n    echo \\$z\n    echo \\$(($x+$y))\n    \"\"\"\n}\nBy default, the process command is interpreted as a Bash script. However, any other scripting language can be used by simply starting the script with the corresponding Shebang declaration. To reference Python variables created inside the Python script, no $ is required. For example:\nprocess PYSTUFF {\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    x = 'Hello'\n    y = 'world!'\n    print(x,y)\n    \"\"\"\n}\n\nworkflow {\n    PYSTUFF().view()\n}\n\nVals\nThe val qualifier allows any data type to be received as input to the process. In the example below, the num queue channel is created from integers 1 and 2, and string Hello, and input into the BASICEXAMPLE process, where it is declared with the qualifier val and assigned to the local variable x. Within this process, this input is referred to and accessed locally by the specified variable name x, prepended with $.\nnum = Channel.of(1, 2, \"Hello\")\n\nprocess BASICEXAMPLE {\n    input:\n    val x\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo process job $x\n    \"\"\"\n}\n\nworkflow {\n    BASICEXAMPLE(num).view()\n}\nIn the above example the process is executed three times, for each element in the channel num. Thus, it results in an output similar to the one shown below:\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `training/nf-training/snippet.nf` [jovial_austin] DSL2 - revision: 9227a67fbc\n\nprocess job 1\nprocess job 2\nprocess job Hello\nThe val qualifier can also be used to specify the process output.\nnum = Channel.of(1, 2, \"Hello\")\n\nprocess BASICEXAMPLE {\n    input:\n    val x\n\n    output:\n    val x\n\n    script:\n    \"\"\"\n    echo process job $x &gt; file.txt\n    \"\"\"\n}\n\nworkflow {\n    BASICEXAMPLE(num).view()\n}\nIn this example, each element of num is printed to a file file.txt. The same input value is then returned as the output. Its contents are printed using the view() channel operator.\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `training/nf-training/snippet.nf` [jovial_austin] DSL2 - revision: 9227a67fbc\n\nexecutor &gt;  local (3)\n[52/d66b4d] process &gt; BASICEXAMPLE (3) [100%] 3 of 3 ✔\n2\n1\nHello\n\n\nPaths\nThe path qualifier allows the handling of files inside a process. When a new instance of a process is executed, a new process execution directory will be created just for that process.\nWhen the path qualifier is specified as the input, Nextflow will stage the file inside the process execution directory (eg. creating a symbolic link to the file), allowing it to be accessed by the script using the specified name in the input declaration, without having to provide the full file path.\nLet’s first create a folder called lesson3.1 and move into it:\nmkdir ./lesson3.1 && cd $_\nNow, take a look inside our data folder. This folder contains multiple .fq files, along with a .fa file.\nls ../training/nf-training/data/ggal\ngut_1.fq  gut_2.fq  liver_1.fq  liver_2.fq  lung_1.fq  lung_2.fq  transcriptome.fa\nSave the following code block as a new file foo.nf inside lesson3.1:\nch_reads = Channel.fromPath('../training/nf-training/data/ggal/*.fq') \n\nprocess FOO {\n    input:\n    path fq\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    ls $fq\n    \"\"\"\n}\n\nworkflow {\n    FOO(ch_reads).view()\n}\nIn this example, the wildcard character * is used to match for .fq files, and assigned to ch_reads. This queue channel is input to the process FOO. In the input declaration of the process body, the file is referred to as fq, which has been declared with the path qualifier. The script block then lists the path of the Nextflow variable fq, which is prepended with $, and this standard output is returned.\nnextflow run foo.nf\nWhen the script is ran, the FOO process is executed six times, once for each element in the ch_reads channel, consuming its contents.\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `foo.nf` [stoic_williams] DSL2 - revision: 58ab4e5e92\n\nexecutor &gt;  local (6)\n[1f/a01f11] FOO (6) | 6 of 6 ✔\ngut_2.fq\n\nlung_1.fq\n\nliver_1.fq\n\nliver_2.fq\n\ngut_1.fq\n\nlung_2.fq\nNote that here, the full path name is not printed, just the file name. This is because the path qualifier will stage each execution of the process separately inside an execution directory. Each process execution directory is saved inside a work folder, which is automatically create by Nextflow.\nYou will notice that inside your work directory, there are six folders, one corresponding to each .fq file inside the ch_reads queue channel. By default, only one execution directory will be printed – in this example it is 1f/a01f11.\nInside the FOO execution directory (ie. ./work/1f/a01f11...), the input file has been staged (symbolically linked) under just the file name, allowing the script to access the file within the execution directory without requiring the full path.\n&gt;&gt;&gt; ls -l ./work/1f/a01f11...\ngut_2.fq -&gt; /.../training/nf-training/data/ggal/gut_2.fq\nSimilarly, the path qualifier can also be used to specify one or more files that will be output by the process.\nModify your foo.nf script to the following:\nch_reads = Channel.fromPath('../training/nf-training/data/ggal/*.fq') \n\nprocess FOO {\n    input:\n    path fq\n\n    output:\n    path \"*.txt\"\n\n    script:\n    \"\"\"\n    echo $fq &gt; sample.txt\n    \"\"\"\n}\n\nworkflow {\n    ch_output = FOO(ch_reads)\n    ch_output.view()\n}\nIn this example, we are now printing the fq file to a file sample_used.txt. The output is declared with the path qualifier and specified using the wildcard *, which will match any output files containing the .txt extension. The path to sample_used.txt will returned and assigned to ch_output, and can be viewed with .view()\nnextflow run foo.nf\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `foo.nf` [awesome_poitras] DSL2 - revision: 7b830769cb\n\nexecutor &gt;  local (6)\n[d5/840b58] process &gt; FOO (4) [100%] 6 of 6 ✔\n/.../work/50/da6e3380c47504e1b52f2e552183eb/sample.txt\n/.../work/ac/ad3e12673a826f7aead7445e477fb1/sample.txt\n/.../work/ac/4df2c1ab0eb1fc84661efd6589d8a2/sample.txt\n/.../work/db/90d554944f53b3d0390a01736e27c5/sample.txt\n/.../work/fb/0a44e3b46e892469144094494b1a4d/sample.txt\n/.../work/d5/840b58fde765a97379edec43426c05/sample.txt\n\n\nTuples\nTo define paired/grouped input and output information, the tuple qualifier can be used. The input and output declarations for tuples must be declared with a tuple qualifier followed by the definition of each element in the tuple.\nIn the example below, ch_reads is a channel created using the fromFilePairs channel factory, which automatically creates a tuple from file pairs.\nch_reads = Channel.fromFilePairs(\"../training/nf-training/data/ggal/*_{1,2}.fq\")\nch_reads.view()\nThis created tuple consists of two elements – the first element is always the grouping key of the matching pair (based on similarities in the file name), and the second is a list of paths to each file.\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/.../training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\nTo input a tuple into a process, the tuple qualifier must be used in the input block. Below, the first element of the tuple (ie. the grouping key) is declared with the val qualifier, and the second element of the tuple is declared with the path qualifier. The FOO process then prints the .fq file paths to a file called sample.txt, and returns that output file as a tuple containing the same grouping key, declared with val, and the output file created inside the process, declared with path.\nch_reads = Channel.fromFilePairs(\"../training/nf-training/data/ggal/*_{1,2}.fq\")\n\nprocess FOO {\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path('sample.txt')\n\n    script:\n    \"\"\"\n    echo $sample_id_paths &gt; sample.txt\n    \"\"\"\n}\n\nworkflow {\n    sample_ch = FOO(ch_reads)\n    sample_ch.view()\n}\nUpdate foo.nf and run the script.\nnextflow run foo.nf\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `foo.nf` [deadly_hypatia] DSL2 - revision: 59fe4396c3\n\nexecutor &gt;  local (3)\n[6e/125990] FOO (2) | 3 of 3 ✔\n[lung, /.../work/3e/d17f681f95541b56e9b3561f2623b8/sample.txt]\n[gut, /.../work/42/bec200096a897ff70a1a0e2d9afd44/sample.txt]\n[liver, /.../work/6e/125990d7d8506c8d41f312e2e500ad/sample.txt]\nHere, the FOO process is executed three times in parallel, so there’s no guarantee of a particular execution order. Therefore, if the script was ran again, the final result may be printed out in a different order:\nnextflow run foo.nf\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `foo.nf` [mighty_einstein] DSL2 - revision: 59fe4396c3\n\nexecutor &gt;  local (3)\n[eb/bf2f06] FOO (2) [100%] 3 of 3 ✔\n[gut, /.../work/67/baae976fa0d5e83ff175ce35c0405c/sample.txt]\n[liver, /.../work/eb/bf2f0695d7e3aa57e61ca509c3594c/sample.txt]\n[lung, /.../work/47/3dd9f920214bf0a1c1539fd8001a21/sample.txt]\nThe use of the tuple qualifier is especially important when the output of a process is being used as an input into another process. This qualifier allows sample metadata information to be stored, critical in ensuring the correct inputs are being used for downstream processes.\n\n\n\n\n\n\nKey points\n\n\n\n\nWhen a queue channel is input into a process, each element will be serially consumed until the channel is empty\nValue channels can be used unlimited times without consuming its contents\n$ characters need to be escaped with \\ when referencing Bash variables and functions, while Nextflow variables do not\nThe scripting language within a process can be altered by starting the script with the desired Shebang declaration\n\n\n\n\n\n\n\nNext Chapter: Creating an RNAseq Workflow\n\n\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, and Nextflow Patterns materials from Nextflow and nf-core*^"
  },
  {
    "objectID": "workshops/1.1_intro_nextflow.html",
    "href": "workshops/1.1_intro_nextflow.html",
    "title": "Introduction to Nextflow",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the benefits of a workflow manager.\nLearn Nextflow terminology.\nLearn the basic commands and options required to run a Nextflow workflow"
  },
  {
    "objectID": "workshops/1.1_intro_nextflow.html#footnotes",
    "href": "workshops/1.1_intro_nextflow.html#footnotes",
    "title": "Introduction to Nextflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.lexico.com/definition/workflow↩︎"
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html",
    "href": "workshops/5.1_nf_core_template.html",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "",
    "text": "Objectives\n\n\n\n\nDevelop a basic Nextflow workflow with nf-core templates\nTest and set up profiles for a Nextflow workflow\nCreate conditional processes, and conditional scripts within a processs\nRead data of different types into a Nextflow workflow"
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#recap-nf-core",
    "href": "workshops/5.1_nf_core_template.html#recap-nf-core",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "4.1 Recap: nf-core",
    "text": "4.1 Recap: nf-core\n\nWhat is nf-core?\nnf-core provides a standardised set of analysis workflows that are modular, scalable, and portable. These workflows follow best practices, and allow researchers from around the world to collaborate on workflow development and maintainence, all of which are tested and peer reviewed. Ultimately, this helps increase the reliability and reproducibility of bioinformatics analysis.\nnf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276–278 (2020). Nature Biotechnology\nKey features of nf-core workflows include: extensive documentation, which outlines installation, usage, and expected output files, stable releases which ensure reproducibility, portability of packaged software that is automatically downloaded using Conda/Singularity/Docker, and cloud-ready allowing scalability.\n\n\nnf-core workflow structure\nnf-core workflows follow a common template structure. For nf-core/rnaseq:\n\nThe main folders and files we will focus on today:\n\nModules and subworkflows are stored as separate .nf files\n\nmodules.json: contains a list of the modules and subworkflows installed in the\n\nConfiguration files are stored as separate .config files.\n\nconf/base.config: contain default resource allocations for process groups (ie. process_high, etc.)\nnextflow.config: contain default workflow parameters and software management profiles\n\nA main workflow .nf file that is used to launch the workflow\nAn analysis workflow file inside workflows/&lt;pipeline&gt;/main.nf is used to chain together separate modules/subworkflows\nA schema file assets/schema_input.json, used to specify required values in the input samplesheet. This can be used to build sample metadata\nA schema file nextflow_schema.json, used to specify required parameters to the pipeline"
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#pipeline-syntax",
    "href": "workshops/5.1_nf_core_template.html#pipeline-syntax",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "4.2 Pipeline syntax",
    "text": "4.2 Pipeline syntax\n\n4.2.1 Module file and process structure\nIn nf-core pipelines, each process is separated into a separate module file. For example, RNASEQ’s fastqc module file contains just one process – FASTQC.\n\n\n\n\n\n\nfastqc/main.nf\n\n\n\n\n\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda \"bioconda::fastqc=0.12.1\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0' :\n        'biocontainers/fastqc:0.12.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    // Make list of old name and new name pairs to use for renaming in the bash while loop\n    def old_new_pairs = reads instanceof Path || reads.size() == 1 ? [[ reads, \"${prefix}.${reads.extension}\" ]] : reads.withIndex().collect { entry, index -&gt; [ entry, \"${prefix}_${index + 1}.${entry.extension}\" ] }\n    def rename_to = old_new_pairs*.join(' ').join(' ')\n    def renamed_files = old_new_pairs.collect{ old_name, new_name -&gt; new_name }.join(' ')\n    \"\"\"\n    printf \"%s %s\\\\n\" $rename_to | while read old_name new_name; do\n        [ -f \"\\${new_name}\" ] || ln -s \\$old_name \\$new_name\n    done\n\n    fastqc \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        $renamed_files\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed '/FastQC v/!d; s/.*v//' )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed '/FastQC v/!d; s/.*v//' )\n    END_VERSIONS\n    \"\"\"\n}\n\n\n\nThis process follows the typical structure we have seen previously:\n\nThe process name FASTQC is declared after the process scope\nDirectives tag, label, conda, and container are defined at the start of the process\ninput block that defines process inputs and their qualifier\noutput block that defines process process outputs and their qualifier\nscript block that contains what the process will execute\nstub block for prototyping workflow logic\n\n\nDirectives\nPreviously, we saw that processes were grouped together based on their resource requirements using the withLabel function. This function utilises the label directive to ‘annotate’ the process.\nIn the FASTQC process, the tag directive is used to display the sample name when that process is being ran. For example:\nprocess FOO {\n  tag \"$code\"\n\n  input:\n  val code\n\n  script:\n  \"\"\"\n  echo $code\n  \"\"\"\n}\n\nworkflow {\n    ch_in = Channel.of('alpha', 'gamma', 'omega')\n    FOO(ch_in)\n}\nThe following will be printed, where the process names contain the tag that has been defined:\n[6e/28919b] Submitted process &gt; FOO (alpha)\n[d2/1c6175] Submitted process &gt; FOO (gamma)\n[1c/3ef220] Submitted process &gt; FOO (omega)\nIn the FASTQC process, the tag has been defined as $meta.id, which we will investigate later.\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda \"bioconda::fastqc=0.12.1\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0' :\n        'biocontainers/fastqc:0.12.1--hdfd78af_0' }\"\nThe conda and container directives define the software dependencies that will automatically be created by Nextflow, based on the profile specified by the user. In the FASTQC process, conda contains the name of the channel where a package needs to be downloaded, and container defines where the container is hosted.\n\n\nInput\nThe input block defines the input channels to a process. It consists of a qualifier (ie. input data type), and a variable name that can be referenced in the process. Based on the qualifier, Nextflow can handle the input differently.\n\npath: Nextflow will ‘stage’ the input into the work execution directory of the process\nval: the input vale can be accessed by the name\ntuple: group common inputs, each with another qualifier\n\nFor FASTQC, the input is a tuple, where the first element is assigned the variable meta that is of data type val. The second element in the tuple is set as the variable reads, that is of the data type path. This means file paths that are provided to this FASTQC process will be symbolically linked inside the execution directory, where it can be accessed by within the script.\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n\nOutput\nSimilar to input, the output block allows the output of the process to be specified, along with its accompanying qualifier. Only files that are defined within this output block can be saved to the output directory.\nNote that wildcards can be used when defining process outputs. For example, using path(\"*.html\") will match for all files with an .html extension.\nAdditionally, there is an option to emit that output with a name that can be referenced to inside the workflow scope. Adding to the previous example:\nprocess FOO {\n  tag \"$code\"\n\n  input:\n  val code\n\n  output:\n  path \"*.txt\"\n\n  script:\n  \"\"\"\n  echo $code &gt; values.txt\n  \"\"\"\n}\n\nworkflow {\n    ch_in = Channel.of('alpha', 'gamma', 'omega')\n    ch_out = FOO(ch_in)\n}\nThe value that is input to the process is now printed into a file values.txt. This file is matched using the * in the output block, and returned with qualifier path. Within the workflow scope, the output of the FOO process is assigned to ch_out.\nThere are many ways to obtain the output of a process within the workflow scope. Another way is to provide the inputs to the process without assigning it to a new channel. Any process outputs can then be accessed using the .out attribute.\nworkflow {\n    ch_in = Channel.of('alpha', 'gamma', 'omega')\n\n    FOO(ch_in)\n    ch_out = FOO.out\n}\nThis becomes useful if there are multiple outputs in a process. We have created an additional output file edited_values.txt that will be output by FOO:\nprocess FOO {\n  tag \"$code\"\n\n  input:\n  val code\n\n  output:\n  path \"values.txt\"\n  path \"edited_values.txt\"\n\n  script:\n  \"\"\"\n  echo $code &gt; values.txt\n  echo edited_$code &gt; edited_values.txt\n  \"\"\"\n}\n\nworkflow {\n    ch_in = Channel.of('alpha', 'gamma', 'omega')\n\n    FOO(ch_in)\n    ch_out = FOO.out[0]\n}\nInside the workflow scope, the different outputs can be accessed by indexing the .out attribute. Alternatively, emit can be used:\nprocess FOO {\n  tag \"$code\"\n\n  input:\n  val code\n\n  output:\n  path \"values.txt\", emit: original\n  path \"edited_values.txt\", emit: edited\n\n  script:\n  \"\"\"\n  echo $code &gt; values.txt\n  echo edited_$code &gt; edited_values.txt\n  \"\"\"\n}\n\nworkflow {\n    ch_in = Channel.of('alpha', 'gamma', 'omega')\n\n    FOO(ch_in)\n    ch_out = FOO.out.original\n}\nThe original values.txt has been emitted as original, which can be accessed within the .out attribute inside the workflow scope.\n\n\nScript\nThe script block defines the commands that is executed by the process. By default, it is executed as a Bash script, and can be any command or script normally executed on the command line. Within this block, Bash variables can be referenced to using an escaped dollar sign, ie. \\$. Nextflow variables can be referenced to using the singular dollar sign, ie. $.\nLet’s take a look at the script block for RNASEQ’s FASTQC process:\nscript:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    // Make list of old name and new name pairs to use for renaming in the bash while loop\n    def old_new_pairs = reads instanceof Path || reads.size() == 1 ? [[ reads, \"${prefix}.${reads.extension}\" ]] : reads.withIndex().collect { entry, index -&gt; [ entry, \"${prefix}_${index + 1}.${entry.extension}\" ] }\n    def rename_to = old_new_pairs*.join(' ').join(' ')\n    def renamed_files = old_new_pairs.collect{ old_name, new_name -&gt; new_name }.join(' ')\n    \"\"\"\n    printf \"%s %s\\\\n\" $rename_to | while read old_name new_name; do\n        [ -f \"\\${new_name}\" ] || ln -s \\$old_name \\$new_name\n    done\n\n    fastqc \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        $renamed_files\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed '/FastQC v/!d; s/.*v//' )\n    END_VERSIONS\n    \"\"\"\nThe script starts by defnining local variables that can be accessed within the execution section (encapsulated by three double-quotes). These definitions are written in Groovy, which we will investigate this later in the session. Notice that\n\nNextflow variables, such as args, are accessed within the quoted section of the script using $\nA singular back-slash in Bash is also escaped, resulting in \\\\\nA versions.yml file is created, which tracks the tool versions used in the pipeline\n\n\n\n\n4.2.2 Importing modules into a Nextflow script\nSince processes are stored as separate Nextflow files, they need to be imported to a workflow script before they can be called inside a workflow scope.\nWithin the fastq_fastqc_umitools_fastp subworkflow file, the FASTQC process is imported using the following command:\ninclude { FASTQC as FASTQC_RAW  } from '../../../modules/nf-core/fastqc/main'\ninclude { FASTQC as FASTQC_TRIM } from '../../../modules/nf-core/fastqc/main'\nNotice that the FASTQC nodule is included twice, once under the alias FASTQC_RAW, and the other time under the alias FASTQC_TRIM. In Nextflow, once a process has been used once, it cannot be re-used. Therefore, if we wish to re-use the same process, it has to be separately included under a new alias.\nAlso notice that while processes are stored inside a main.nf script, the Nextflow file extension .nf does not need to be added when we include the module.\nFinally, the relative path to the module is being used. This means the path may be different, depending on the origin of the include.\nrnaseq\n├── modules\n│   └── nf-core\n│       ├── fastqc\n│       │   ├── main.nf\n│       │   └── ...\n│       └─ ...\n├── subworkflows\n│   └── nf-core\n│       ├── fastq_fastqc_umitools_fastp\n│       │   ├── main.nf\n│       │   └── ...\n│       └─ ...\n├── workflows\n│   └── rnaseq\n│       ├── main.nf\n│       └─ ...\n└-─ ...\nThe subworkflow file is located inside subworkflows/nf-cor/fastq_fastqc_umitools_fastp/main.nf. Therefore, we need to go back three directory levels before reaching the modules folder. Therefore, the relative path will be:\n'../../../modules/nf-core/fastqc/main'\nIf we wish to include the FASTQC module from inside workflows/rnaseq/main.nf, we only need to go back two directory levels before reaching the modules folder. Therefore, the relative path will be:\n'../../modules/nf-core/fastqc/main'\n\n\n4.2.3 Workflow scopes\nIn Nextflow, a workflow can be used to define what processes are executed, and the flow of data from one process to another.\nworkflow &lt;NAME&gt; {\n    take:\n    ...\n\n    main:\n    ...\n\n    emit:\n    ...\n}\nA workflow can be defined with a name, which allows it to be called and imported by another script or workflow. Only named workflows can be imported and called by other workflows. The main sections of the named workflow are:\n\ntake: declares inputs to the workflow\nmain: defines data flow logic\nemit: declares the outputs of the workflow\n\nConsider the fastq_fastqc_umitools_fastp subworkflow.\n\n\n\n\n\n\nsubworkflows/nf-core/fastq_fastqc_umitools_fastp/main.nf\n\n\n\n\n\n//\n// Read QC, UMI extraction and trimming\n//\n\ninclude { FASTQC as FASTQC_RAW  } from '../../../modules/nf-core/fastqc/main'\ninclude { FASTQC as FASTQC_TRIM } from '../../../modules/nf-core/fastqc/main'\ninclude { UMITOOLS_EXTRACT      } from '../../../modules/nf-core/umitools/extract/main'\ninclude { FASTP                 } from '../../../modules/nf-core/fastp/main'\n\n//\n// Function that parses fastp json output file to get total number of reads after trimming\n//\nimport groovy.json.JsonSlurper\n\ndef getFastpReadsAfterFiltering(json_file, min_num_reads) {\n\n    if ( workflow.stubRun ) { return min_num_reads }\n\n    def Map json = (Map) new JsonSlurper().parseText(json_file.text).get('summary')\n    return json['after_filtering']['total_reads'].toLong()\n}\n\ndef getFastpAdapterSequence(json_file){\n\n    if ( workflow.stubRun ) { return \"\" }\n\n    def Map json = (Map) new JsonSlurper().parseText(json_file.text)\n    try{\n        adapter = json['adapter_cutting']['read1_adapter_sequence']\n    } catch(Exception ex){\n        adapter = \"\"\n    }\n    return adapter\n}\n\nworkflow FASTQ_FASTQC_UMITOOLS_FASTP {\n    take:\n    reads             // channel: [ val(meta), [ reads ] ]\n    skip_fastqc       // boolean: true/false\n    with_umi          // boolean: true/false\n    skip_umi_extract  // boolean: true/false\n    umi_discard_read  // integer: 0, 1 or 2\n    skip_trimming     // boolean: true/false\n    adapter_fasta     // file: adapter.fasta\n    save_trimmed_fail // boolean: true/false\n    save_merged       // boolean: true/false\n    min_trimmed_reads // integer: &gt; 0\n\n    main:\n    ch_versions = Channel.empty()\n    fastqc_raw_html = Channel.empty()\n    fastqc_raw_zip  = Channel.empty()\n    if (!skip_fastqc) {\n        FASTQC_RAW (\n            reads\n        )\n        fastqc_raw_html = FASTQC_RAW.out.html\n        fastqc_raw_zip  = FASTQC_RAW.out.zip\n        ch_versions     = ch_versions.mix(FASTQC_RAW.out.versions.first())\n    }\n\n    umi_reads = reads\n    umi_log   = Channel.empty()\n    if (with_umi && !skip_umi_extract) {\n        UMITOOLS_EXTRACT (\n            reads\n        )\n        umi_reads   = UMITOOLS_EXTRACT.out.reads\n        umi_log     = UMITOOLS_EXTRACT.out.log\n        ch_versions = ch_versions.mix(UMITOOLS_EXTRACT.out.versions.first())\n\n        // Discard R1 / R2 if required\n        if (umi_discard_read in [1,2]) {\n            UMITOOLS_EXTRACT\n                .out\n                .reads\n                .map {\n                    meta, reads -&gt;\n                        meta.single_end ? [ meta, reads ] : [ meta + [single_end: true], reads[umi_discard_read % 2] ]\n                }\n                .set { umi_reads }\n        }\n    }\n\n    trim_reads        = umi_reads\n    trim_json         = Channel.empty()\n    trim_html         = Channel.empty()\n    trim_log          = Channel.empty()\n    trim_reads_fail   = Channel.empty()\n    trim_reads_merged = Channel.empty()\n    fastqc_trim_html  = Channel.empty()\n    fastqc_trim_zip   = Channel.empty()\n    trim_read_count   = Channel.empty()\n    adapter_seq       = Channel.empty()\n\n    if (!skip_trimming) {\n        FASTP (\n            umi_reads,\n            adapter_fasta,\n            false, // don't want to set discard_trimmed_pass, else there will be no reads output\n            save_trimmed_fail,\n            save_merged\n        )\n        trim_json         = FASTP.out.json\n        trim_html         = FASTP.out.html\n        trim_log          = FASTP.out.log\n        trim_reads_fail   = FASTP.out.reads_fail\n        trim_reads_merged = FASTP.out.reads_merged\n        ch_versions       = ch_versions.mix(FASTP.out.versions.first())\n\n        //\n        // Filter FastQ files based on minimum trimmed read count after adapter trimming\n        //\n        FASTP\n            .out\n            .reads\n            .join(trim_json)\n            .map { meta, reads, json -&gt; [ meta, reads, getFastpReadsAfterFiltering(json, min_trimmed_reads.toLong()) ] }\n            .set { ch_num_trimmed_reads }\n\n        ch_num_trimmed_reads\n            .filter { meta, reads, num_reads -&gt; num_reads &gt;= min_trimmed_reads.toLong() }\n            .map { meta, reads, num_reads -&gt; [ meta, reads ] }\n            .set { trim_reads }\n\n        ch_num_trimmed_reads\n            .map { meta, reads, num_reads -&gt; [ meta, num_reads ] }\n            .set { trim_read_count }\n\n        trim_json\n            .map { meta, json -&gt; [meta, getFastpAdapterSequence(json)] }\n            .set { adapter_seq }\n\n        if (!skip_fastqc) {\n            FASTQC_TRIM (\n                trim_reads\n            )\n            fastqc_trim_html = FASTQC_TRIM.out.html\n            fastqc_trim_zip  = FASTQC_TRIM.out.zip\n            ch_versions      = ch_versions.mix(FASTQC_TRIM.out.versions.first())\n        }\n    }\n\n    emit:\n    reads = trim_reads // channel: [ val(meta), [ reads ] ]\n\n    fastqc_raw_html    // channel: [ val(meta), [ html ] ]\n    fastqc_raw_zip     // channel: [ val(meta), [ zip ] ]\n\n    umi_log            // channel: [ val(meta), [ log ] ]\n    adapter_seq        // channel: [ val(meta), [ adapter_seq] ]\n\n    trim_json          // channel: [ val(meta), [ json ] ]\n    trim_html          // channel: [ val(meta), [ html ] ]\n    trim_log           // channel: [ val(meta), [ log ] ]\n    trim_reads_fail    // channel: [ val(meta), [ fastq.gz ] ]\n    trim_reads_merged  // channel: [ val(meta), [ fastq.gz ] ]\n    trim_read_count    // channel: [ val(meta), val(count) ]\n\n    fastqc_trim_html   // channel: [ val(meta), [ html ] ]\n    fastqc_trim_zip    // channel: [ val(meta), [ zip ] ]\n\n    versions = ch_versions // channel: [ versions.yml ]\n}\n\n\n\nThe subworkflow script begins by including any modules used within script. Within the workflow defintion, the name is defined as FASTQ_FASTQC_UMITOOLS_FASTP.\nThe following are defined as the workflow inputs:\n    take:\n    reads             // channel: [ val(meta), [ reads ] ]\n    skip_fastqc       // boolean: true/false\n    with_umi          // boolean: true/false\n    skip_umi_extract  // boolean: true/false\n    umi_discard_read  // integer: 0, 1 or 2\n    skip_trimming     // boolean: true/false\n    adapter_fasta     // file: adapter.fasta\n    save_trimmed_fail // boolean: true/false\n    save_merged       // boolean: true/false\n    min_trimmed_reads // integer: &gt; 0\nThese inputs can then be used within the main block of the workflow. At the start of the block, some empty channels are created. Next, the skip_fastqc input is used to determine if the FASTQC process will be executed. If the parameter is True, then FASTQC_RAW will be executed on the input reads that are specified to the workflow. As we saw previously, the .out attribute can be used to specific output file that is emitted by the process.\n    main:\n    ch_versions = Channel.empty()\n    fastqc_raw_html = Channel.empty()\n    fastqc_raw_zip  = Channel.empty()\n    if (!skip_fastqc) {\n        FASTQC_RAW (\n            reads\n        )\n        fastqc_raw_html = FASTQC_RAW.out.html\n        fastqc_raw_zip  = FASTQC_RAW.out.zip\n        ch_versions     = ch_versions.mix(FASTQC_RAW.out.versions.first())\n    }\n\n    ...\nIn the workflow’s emit block, pipeline outputs are defined.\n emit:\n    reads = trim_reads // channel: [ val(meta), [ reads ] ]\n\n    fastqc_raw_html    // channel: [ val(meta), [ html ] ]\n    fastqc_raw_zip     // channel: [ val(meta), [ zip ] ]\n\n    umi_log            // channel: [ val(meta), [ log ] ]\n    adapter_seq        // channel: [ val(meta), [ adapter_seq] ]\n\n    trim_json          // channel: [ val(meta), [ json ] ]\n    trim_html          // channel: [ val(meta), [ html ] ]\n    trim_log           // channel: [ val(meta), [ log ] ]\n    trim_reads_fail    // channel: [ val(meta), [ fastq.gz ] ]\n    trim_reads_merged  // channel: [ val(meta), [ fastq.gz ] ]\n    trim_read_count    // channel: [ val(meta), val(count) ]\n\n    fastqc_trim_html   // channel: [ val(meta), [ html ] ]\n    fastqc_trim_zip    // channel: [ val(meta), [ zip ] ]\n\n    versions = ch_versions // channel: [ versions.yml ]\nTo access the outputs of a named workflow, let’s first check the fastq_qc_trim_filter_setstrandedness subworkflow. Since subworkflows are sparated into separate .nf files, they also have to be included inside the Nextflow script.\ninclude { FASTQ_FASTQC_UMITOOLS_FASTP        } from '../fastq_fastqc_umitools_fastp'\nSimilar to a module, the included subworkflow can now be used within the main section of a workflow.\n    //\n    // SUBWORKFLOW: Read QC, extract UMI and trim adapters with fastp\n    //\n    if (trimmer == 'fastp') {\n        FASTQ_FASTQC_UMITOOLS_FASTP (\n            ch_filtered_reads,\n            skip_fastqc,\n            with_umi,\n            skip_umi_extract,\n            umi_discard_read,\n            skip_trimming,\n            [],\n            save_trimmed,\n            fastp_merge,\n            min_trimmed_reads\n        )\n        ch_filtered_reads      = FASTQ_FASTQC_UMITOOLS_FASTP.out.reads\n        ch_trim_read_count     = FASTQ_FASTQC_UMITOOLS_FASTP.out.trim_read_count\n\n        ch_versions = ch_versions.mix(FASTQ_FASTQC_UMITOOLS_FASTP.out.versions)\n        ch_multiqc_files = FASTQ_FASTQC_UMITOOLS_FASTP.out.fastqc_raw_zip\n            .mix(FASTQ_FASTQC_UMITOOLS_FASTP.out.fastqc_trim_zip)\n            .mix(FASTQ_FASTQC_UMITOOLS_FASTP.out.trim_json)\n            .mix(ch_multiqc_files)\n    }\nSimilar to how inputs to processes can be specified, inputs to workflows are specified positionally. To access the outputs of the FASTQ_FASTQC_UMITOOLS_FASTP subworkflow, the .out attribute can be used, along with the desired output declared in emit."
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#nf-core-tools",
    "href": "workshops/5.1_nf_core_template.html#nf-core-tools",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "4.3 nf-core tools",
    "text": "4.3 nf-core tools\nnf-core is a python package that contain tools for both building and running workflows.\nTake a look at what is available within with the nf-core/tools suite\nnf-core --help\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\n                                                                                                    \n Usage: nf-core [OPTIONS] COMMAND [ARGS]...                                                         \n                                                                                                    \n nf-core/tools provides a set of helper tools for use with nf-core Nextflow pipelines.              \n It is designed for both end-users running pipelines and also developers creating new pipelines.    \n                                                                                                    \n╭─ Commands ───────────────────────────────────────────────────────────────────────────────────────╮\n│ pipelines         Commands to manage nf-core pipelines.                                          │\n│ modules           Commands to manage Nextflow DSL2 modules (tool wrappers).                      │\n│ subworkflows      Commands to manage Nextflow DSL2 subworkflows (tool wrappers).                 │\n│ interface         Launch the nf-core interface                                                   │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Options ────────────────────────────────────────────────────────────────────────────────────────╮\n│ --version                        Show the version and exit.                                      │\n│ --verbose        -v              Print verbose output to the console.                            │\n│ --hide-progress                  Don't show progress bars.                                       │\n│ --log-file       -l  &lt;filename&gt;  Save a verbose log to a file.                                   │\n│ --help           -h              Show this message and exit.                                     │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nWithin nf-core, the commands available are pipelines, modules, subworkflows, and interface. Today we will be using pipelines and modules to create our own pipeline.\nnf-core pipelines\n\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\n                                                                                                    \n Usage: nf-core pipelines [OPTIONS] COMMAND [ARGS]...                                               \n                                                                                                    \n Commands to manage nf-core pipelines.                                                              \n                                                                                                    \n╭─ For users ──────────────────────────────────────────────────────────────────────────────────────╮\n│ list                  List available nf-core pipelines with local info.                          │\n│ launch                Launch a pipeline using a web GUI or command line prompts.                 │\n│ download              Download a pipeline, nf-core/configs and pipeline singularity images.      │\n│ create-params-file    Build a parameter file for a pipeline.                                     │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ For developers ─────────────────────────────────────────────────────────────────────────────────╮\n│ create         Create a new pipeline using the nf-core template.                                 │\n│ lint           Check pipeline code against nf-core guidelines.                                   │\n│ bump-version   Update nf-core pipeline version number with `nf-core pipelines bump-version`.     │\n│ sync           Sync a pipeline TEMPLATE branch with the nf-core template.                        │\n│ schema         Suite of tools for developers to manage pipeline schema.                          │\n│ rocrate        Make an Research Object Crate                                                     │\n│ create-logo    Generate a logo with the nf-core logo template.                                   │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Options ────────────────────────────────────────────────────────────────────────────────────────╮\n│ --help  -h    Show this message and exit.                                                        │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nFor users who wish to run existing nf-core pipelines, the commands list, launch, download, and create-params-file are available. To build a workflow following the standardised nf-core template structure, developer tools can be used. We will focus on the developer commands."
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#nf-core-pipelines-create",
    "href": "workshops/5.1_nf_core_template.html#nf-core-pipelines-create",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "4.4 nf-core pipelines create",
    "text": "4.4 nf-core pipelines create\nLet’s start by creating a pipeline:\nnf-core pipelines create\nThe pipeline we will be creating is a custom rnaseq pipeline customrnaseq. Navigate through the interactive window that is launched on the terminal to create the template.\n\nSelect nf-core as the pipeline type\nEnter a workflow name (consisting of only letters, and no special characters or numbers), description, and author name\nLeave all template features (use reference genomes, use multiqc, use fastqc, use nf-schema) on as default\nSpecify the path to where your template will be generated\nToday we won’t be pushing the pipeline to a GitHub repository, so we will be selecting Finish without creating a repo\n\nThe created template will be stored in a folder with our specified pipeline name, prefixed with nf-core.\nThis newly created pipeline folder contains the same strucure as nf-core/rnaseq. Notice that:\n\nConfiguration files are contained within the conf directory\nModules and subworkflows are contained as separate .nf scripts inside their own directories\nThe main workflow file ./nf-core-customrnaseq/main.nf can be used to launch the pipeline.\nThere is also a workflow script ./nf-core-customrnaseq/workflows/customrnaseq.nf, which we will add modules and subworkflows to.\n\n\n\n\n\n\n\nPipeline template structure\n\n\n\n\n\n./nf-core-customrnaseq/\n├── assets\n│   ├── adaptivecard.json\n│   ├── email_template.html\n│   ├── email_template.txt\n│   ├── methods_description_template.yml\n│   ├── multiqc_config.yml\n│   ├── nf-core-customrnaseq_logo_light.png\n│   ├── samplesheet.csv\n│   ├── schema_input.json\n│   ├── sendmail_template.txt\n│   └── slackreport.json\n├── CHANGELOG.md\n├── CITATIONS.md\n├── CODE_OF_CONDUCT.md\n├── conf\n│   ├── base.config\n│   ├── igenomes.config\n│   ├── igenomes_ignored.config\n│   ├── modules.config\n│   ├── test.config\n│   └── test_full.config\n├── docs\n│   ├── images\n│   │   ├── nf-core-customrnaseq_logo_dark.png\n│   │   └── nf-core-customrnaseq_logo_light.png\n│   ├── output.md\n│   ├── README.md\n│   └── usage.md\n├── LICENSE\n├── main.nf\n├── modules\n│   └── nf-core\n│       ├── fastqc\n│       │   ├── environment.yml\n│       │   ├── main.nf\n│       │   ├── meta.yml\n│       │   └── tests\n│       │       ├── main.nf.test\n│       │       └── main.nf.test.snap\n│       └── multiqc\n│           ├── environment.yml\n│           ├── main.nf\n│           ├── meta.yml\n│           └── tests\n│               ├── main.nf.test\n│               ├── main.nf.test.snap\n│               ├── nextflow.config\n│               └── tags.yml\n├── modules.json\n├── nextflow.config\n├── nextflow_schema.json\n├── README.md\n├── ro-crate-metadata.json\n├── subworkflows\n│   ├── local\n│   │   └── utils_nfcore_customrnaseq_pipeline\n│   │       └── main.nf\n│   └── nf-core\n│       ├── utils_nextflow_pipeline\n│       │   ├── main.nf\n│       │   ├── meta.yml\n│       │   └── tests\n│       │       ├── main.function.nf.test\n│       │       ├── main.function.nf.test.snap\n│       │       ├── main.workflow.nf.test\n│       │       ├── nextflow.config\n│       │       └── tags.yml\n│       ├── utils_nfcore_pipeline\n│       │   ├── main.nf\n│       │   ├── meta.yml\n│       │   └── tests\n│       │       ├── main.function.nf.test\n│       │       ├── main.function.nf.test.snap\n│       │       ├── main.workflow.nf.test\n│       │       ├── main.workflow.nf.test.snap\n│       │       ├── nextflow.config\n│       │       └── tags.yml\n│       └── utils_nfschema_plugin\n│           ├── main.nf\n│           ├── meta.yml\n│           └── tests\n│               ├── main.nf.test\n│               ├── nextflow.config\n│               └── nextflow_schema.json\n├── tower.yml\n└── workflows\n    └── customrnaseq.nf\n\n\n\n\n4.4.1 Workflow script main.nf\nLet’s take a look inside ./nf-core-customrnaseq/main.nf:\n\nThe main workflow, customrnaseq, is included as a workflow\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    IMPORT FUNCTIONS / MODULES / SUBWORKFLOWS / WORKFLOWS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\ninclude { CUSTOMRNASEQ  } from './workflows/customrnaseq'\ninclude { PIPELINE_INITIALISATION } from './subworkflows/local/utils_nfcore_customrnaseq_pipeline'\ninclude { PIPELINE_COMPLETION     } from './subworkflows/local/utils_nfcore_customrnaseq_pipeline'\ninclude { getGenomeAttribute      } from './subworkflows/local/utils_nfcore_customrnaseq_pipeline'\n\n/*\nBy default, pipeline initialisation and completion subworkflows are included to log nextflow version, pipeline version, validate parameters, and summarise pipeline parameters.\n\nPIPELINE_INITIALISATION will also parse the input samplesheet to create a channel that can be input to modules/subworkflows\n\n./nf-core-customrnaseq/subworkflows/local/utils_nfcore_customrnaseq_pipeline/main.nf\n\n\n\n\n\n\n\nPIPELINE_INITIALISATION\n\n\n\n\n\nworkflow PIPELINE_INITIALISATION {\n\ntake:\nversion           // boolean: Display version and exit\nvalidate_params   // boolean: Boolean whether to validate parameters against the schema at runtime\nmonochrome_logs   // boolean: Do not use coloured log outputs\nnextflow_cli_args //   array: List of positional nextflow CLI args\noutdir            //  string: The output directory where the results will be saved\ninput             //  string: Path to input samplesheet\n    \nmain:\n\n// ...\n\n//\n// Create channel from input file provided through params.input\n//\n\nChannel\n    .fromList(samplesheetToList(params.input, \"${projectDir}/assets/schema_input.json\"))\n    .map {\n        meta, fastq_1, fastq_2 -&gt;\n            if (!fastq_2) {\n                return [ meta.id, meta + [ single_end:true ], [ fastq_1 ] ]\n            } else {\n                return [ meta.id, meta + [ single_end:false ], [ fastq_1, fastq_2 ] ]\n            }\n    }\n    .groupTuple()\n    .map { samplesheet -&gt;\n        validateInputSamplesheet(samplesheet)\n    }\n    .map {\n        meta, fastqs -&gt;\n            return [ meta, fastqs.flatten() ]\n    }\n    .set { ch_samplesheet }\n\nemit:\nsamplesheet = ch_samplesheet\nversions    = ch_versions\n}\n\n\n\n\nA new workflow block NFCORE_CUSTOMRNASEQ has been created to launch the main analysis workflow\n//\n// WORKFLOW: Run main analysis pipeline depending on type of input\n//\nworkflow NFCORE_CUSTOMRNASEQ {\n\n    take:\n    samplesheet // channel: samplesheet read in from --input\n\n    main:\n\n    //\n    // WORKFLOW: Run pipeline\n    //\n    CUSTOMRNASEQ (\n        samplesheet\n    )\n    emit:\n    multiqc_report = CUSTOMRNASEQ.out.multiqc_report // channel: /path/to/multiqc_report.html\n}\nThe main workflow calls the initialisation pipeline, inputs the channel created in PIPELINE_COMPLETION to the new workflow block NFCORE_CUSTOMRNASEQ (which launches the analysis workflow), then runs PIPELINE_COMPLETION\n\n\n\n\n\n\n\nmain.nf\n\n\n\n\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    RUN MAIN WORKFLOW\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\nworkflow {\n\n    main:\n    //\n    // SUBWORKFLOW: Run initialisation tasks\n    //\n    PIPELINE_INITIALISATION (\n        params.version,\n        params.validate_params,\n        params.monochrome_logs,\n        args,\n        params.outdir,\n        params.input\n    )\n\n    //\n    // WORKFLOW: Run main workflow\n    //\n    NFCORE_CUSTOMRNASEQ (\n        PIPELINE_INITIALISATION.out.samplesheet\n    )\n    //\n    // SUBWORKFLOW: Run completion tasks\n    //\n    PIPELINE_COMPLETION (\n        params.email,\n        params.email_on_fail,\n        params.plaintext_email,\n        params.outdir,\n        params.monochrome_logs,\n        params.hook_url,\n        NFCORE_CUSTOMRNASEQ.out.multiqc_report\n    )\n}\n\n/*\n\n\n\n\n\n4.4.2 Analysis workflow script workflows/customrnaseq.nf\nNow, let’s look inside the main analysis script ./nf-core-customrnaseq/workflows/customrnaseq.nf.\nThe first section of the script includes some default modules FASTQC and MULTIQC, as well as default subworkflows paramsSummaryMultiqc, softwareVersionsToYAML and methodsDescriptionText, and plugin paramsSummaryMap.\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    IMPORT MODULES / SUBWORKFLOWS / FUNCTIONS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\ninclude { FASTQC                 } from '../modules/nf-core/fastqc/main'\ninclude { MULTIQC                } from '../modules/nf-core/multiqc/main'\ninclude { paramsSummaryMap       } from 'plugin/nf-schema'\ninclude { paramsSummaryMultiqc   } from '../subworkflows/nf-core/utils_nfcore_pipeline'\ninclude { softwareVersionsToYAML } from '../subworkflows/nf-core/utils_nfcore_pipeline'\ninclude { methodsDescriptionText } from '../subworkflows/local/utils_nfcore_customrnaseq_pipeline'\nThe workflow block then defines the name of our pipeline CUSTOMRNASEQ, taking the parsed samplesheet (as a channel) as the input. The workflow then runs the FASTQC module, which takes the samplesheet channel as input. The FASTQC output files are added to the ch_multiqc_files channel, and the FastQC version is added to ch_versions using the .mix operator. Then, all module software versions are collected and assigned to ch_collated_versions. Finally, MULTIQC is ran on all required files.\n\n\n\n\n\n\nworkflows/customrnaseq.nf\n\n\n\n\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    RUN MAIN WORKFLOW\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\nworkflow CUSTOMRNASEQ {\n\n    take:\n    ch_samplesheet // channel: samplesheet read in from --input\n    main:\n\n    ch_versions = Channel.empty()\n    ch_multiqc_files = Channel.empty()\n    //\n    // MODULE: Run FastQC\n    //\n    FASTQC (\n        ch_samplesheet\n    )\n    ch_multiqc_files = ch_multiqc_files.mix(FASTQC.out.zip.collect{it[1]})\n    ch_versions = ch_versions.mix(FASTQC.out.versions.first())\n\n    //\n    // Collate and save software versions\n    //\n    softwareVersionsToYAML(ch_versions)\n        .collectFile(\n            storeDir: \"${params.outdir}/pipeline_info\",\n            name: 'nf_core_'  +  'customrnaseq_software_'  + 'mqc_'  + 'versions.yml',\n            sort: true,\n            newLine: true\n        ).set { ch_collated_versions }\n\n\n    //\n    // MODULE: MultiQC\n    //\n    ch_multiqc_config        = Channel.fromPath(\n        \"$projectDir/assets/multiqc_config.yml\", checkIfExists: true)\n    ch_multiqc_custom_config = params.multiqc_config ?\n        Channel.fromPath(params.multiqc_config, checkIfExists: true) :\n        Channel.empty()\n    ch_multiqc_logo          = params.multiqc_logo ?\n        Channel.fromPath(params.multiqc_logo, checkIfExists: true) :\n        Channel.empty()\n\n    summary_params      = paramsSummaryMap(\n        workflow, parameters_schema: \"nextflow_schema.json\")\n    ch_workflow_summary = Channel.value(paramsSummaryMultiqc(summary_params))\n    ch_multiqc_files = ch_multiqc_files.mix(\n        ch_workflow_summary.collectFile(name: 'workflow_summary_mqc.yaml'))\n    ch_multiqc_custom_methods_description = params.multiqc_methods_description ?\n        file(params.multiqc_methods_description, checkIfExists: true) :\n        file(\"$projectDir/assets/methods_description_template.yml\", checkIfExists: true)\n    ch_methods_description                = Channel.value(\n        methodsDescriptionText(ch_multiqc_custom_methods_description))\n\n    ch_multiqc_files = ch_multiqc_files.mix(ch_collated_versions)\n    ch_multiqc_files = ch_multiqc_files.mix(\n        ch_methods_description.collectFile(\n            name: 'methods_description_mqc.yaml',\n            sort: true\n        )\n    )\n\n    MULTIQC (\n        ch_multiqc_files.collect(),\n        ch_multiqc_config.toList(),\n        ch_multiqc_custom_config.toList(),\n        ch_multiqc_logo.toList(),\n        [],\n        []\n    )\n\n    emit:multiqc_report = MULTIQC.out.report.toList() // channel: /path/to/multiqc_report.html\n    versions       = ch_versions                 // channel: [ path(versions.yml) ]\n\n}\n\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    THE END\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n*/\n\n\n\nThis initial pipeline will use the samples defined in the samplesheet, run FastQC and MultiQC, and save any software versions used.\n\n\n4.4.3 Samplesheet assets/samplesheet.csv\nBefore we can run the pipeline, we have to create a samplesheet with our data. The file ./nf-core-customrnaseq/assets/samplesheet.csv contains an example samplesheet which we can adapt.\nsample,fastq_1,fastq_2\nSAMPLE_PAIRED_END,/path/to/fastq/files/AEG588A1_S1_L002_R1_001.fastq.gz,/path/to/fastq/files/AEG588A1_S1_L002_R2_001.fastq.gz\nSAMPLE_SINGLE_END,/path/to/fastq/files/AEG588A4_S4_L003_R1_001.fastq.gz,\nCopy this samplesheet into your folder, one level higher than the pipeline folder ./nf-core-customrnaseq\ncp ./nf-core-customrnaseq/assets/samplesheet.csv .\nAlso download the following test data to your folder:\ngit clone -b rnaseq https://github.com/nf-core/test-datasets.git\nThis test dataset consists of two paired-end samples gut, liver, and lung, as well as a transcriptome file.\ndata\n├── gut_1.fastq.gz\n├── gut_2.fastq.gz\n├── liver_1.fastq.gz\n├── liver_2.fastq.gz\n├── lung_1.fastq.gz\n├── lung_2.fastq.gz\n└-- transcriptome.fa\nYour samplesheet and data folder should now sit outside of your pipeline folder.\n.\n├── nf-core-customrnaseq \n├── samplesheet.csv \n└── data \nEdit the samplesheet to use our test data. Replace &lt;full_path&gt; with the path to your current working directory.\nsample,fastq_1,fastq_2\ngut,&lt;full_path&gt;/data/gut_1.fastq.gz,&lt;full_path&gt;/data/gut_2.fastq.gz\nliver,&lt;full_path&gt;/data/liver_1.fastq.gz,&lt;full_path&gt;/data/liver_2.fastq.gz\nlung,&lt;full_path&gt;/data/lung_1.fastq.gz,&lt;full_path&gt;/data/lung_2.fastq.gz\n\n\n4.4.4 Running the pipeline\nPreviously, we used a singularity profile, which allowed nextflow to store downloaded containers in a shared location. This location was defined using the bash command below:\n\n\n\n\n\n\nImportant\n\n\n\nTO DO\nCache directory location\n\n\nexport NXF_SINGULARITY_CACHEDIR=&lt;path_to_cache&gt; \nLet’s run our pipeline by specifying the singularity profile, input samplesheet, and output location.:\nnextflow run ./nf-core-customrnaseq/main.nf -profile singularity --input ./samplesheet.csv --outdir output\nNote that we are launching the wrapper workflow file, which contains the PIPELINE_INITIALISATION, PIPELINE_COMPLETION, and our analysis workflow CUSTOMRNASEQ.\nAs expected, the modules that have completed successfully are FASTQC and MULTIQC\n\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `./nf-core-customrnaseq/main.nf` [amazing_varahamihira] DSL2 - revision: d93ca14588\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/customrnaseq 1.0.0dev\n------------------------------------------------------\nInput/output options\n  input              : ./samplesheet.csv\n  outdir             : output\n\nGeneric options\n  trace_report_suffix: 2025-05-01_16-19-14\n\nCore Nextflow options\n  runName            : amazing_varahamihira\n  containerEngine    : singularity\n  launchDir          : /scratch/users/sli/workshop/pipeline\n  workDir            : /scratch/users/sli/workshop/pipeline/work\n  projectDir         : /scratch/users/sli/workshop/pipeline/nf-core-customrnaseq\n  userName           : sli\n  profile            : singularity\n  configFiles        : /scratch/users/sli/workshop/pipeline/nf-core-customrnaseq/nextflow.config\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\n* The nf-core framework\n    https://doi.org/10.1038/s41587-020-0439-x\n\n* Software dependencies\n    https://github.com/nf-core/customrnaseq/blob/main/CITATIONS.md\n\nexecutor &gt;  local (4)\n[27/5fb1e5] NFCORE_CUSTOMRNASEQ:CUSTOMRNASEQ:FASTQC (liver) | 3 of 3 ✔\n[4c/9df475] NFCORE_CUSTOMRNASEQ:CUSTOMRNASEQ:MULTIQC        | 1 of 1 ✔\n-[nf-core/customrnaseq] Pipeline completed successfully-\nNow that we’ve verified our pipeline template is working, we can add our own modules to the workflow."
  },
  {
    "objectID": "workshops/5.3_metadata_propagation_CP.html",
    "href": "workshops/5.3_metadata_propagation_CP.html",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of how to use nf-core modules in a workflow script\nManipulate and proprogate sample metadata throughout the workflow\nCreate a custom nf-core module"
  },
  {
    "objectID": "workshops/5.3_metadata_propagation_CP.html#samplesheet-parsing",
    "href": "workshops/5.3_metadata_propagation_CP.html#samplesheet-parsing",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "6.1 Samplesheet parsing",
    "text": "6.1 Samplesheet parsing\nIn the ./nf-core-customrnaseq/main.nf script, the PIPELINE_INITIALISATION subworkflow created by default from the nf-core template will output a channel that contains the parsed --input samplesheet. This channel is then input into NFCORE_CUSTOMRNASEQ, which launches our analysis workflow containing the newly included modules salmon/quant and salmon/quant.\n...\n\ninclude { PIPELINE_INITIALISATION } from './subworkflows/local/utils_nfcore_customrnaseq_pipeline'\n\n...\n\nworkflow {\n\n    main:\n\n    ...\n\n    //\n    // WORKFLOW: Run main workflow\n    //\n    NFCORE_CUSTOMRNASEQ (\n        PIPELINE_INITIALISATION.out.samplesheet\n    )\n\n    ...\n\n}\nHow does the PIPELINE_INITIALISATION parse the samplesheet?\n\n\n\n\n\n\n./nf-core-customrnaseq/subworkflows/local/utils_nfcore_customrnaseq_pipeline/main.nf\n\n\n\n\n\n//\n// Create channel from input file provided through params.input\n//\nworkflow PIPELINE_INITIALISATION {\n\n    take:\n    version           // boolean: Display version and exit\n    validate_params   // boolean: Boolean whether to validate parameters against the schema at runtime\n    monochrome_logs   // boolean: Do not use coloured log outputs\n    nextflow_cli_args //   array: List of positional nextflow CLI args\n    outdir            //  string: The output directory where the results will be saved\n    input             //  string: Path to input samplesheet\n\n    main:\n\n    ...\n\n    Channel\n        .fromList(samplesheetToList(params.input, \"${projectDir}/assets/schema_input.json\"))\n        .map {\n            meta, fastq_1, fastq_2 -&gt;\n                if (!fastq_2) {\n                    return [ meta.id, meta + [ single_end:true ], [ fastq_1 ] ]\n                } else {\n                    return [ meta.id, meta + [ single_end:false ], [ fastq_1, fastq_2 ] ]\n                }\n        }\n        .groupTuple()\n        .map { samplesheet -&gt;\n            validateInputSamplesheet(samplesheet)\n        }\n        .map {\n            meta, fastqs -&gt;\n                return [ meta, fastqs.flatten() ]\n        }\n        .set { ch_samplesheet }\n\n    emit:\n    samplesheet = ch_samplesheet\n    versions    = ch_versions\n}\n\n\n\nThe important pieces of information that we will explore further in this section are:\n\nThe use of a schema_input.json to validate the samplesheet metadata\nThe use of .map { } and .groupTuple() functions to manipulate sample metadata\n\n\n6.1.1 Default samplesheet channel\nThe samplesheet is automatically parsed, resulting in a channel that contains all relevant datta specified in the --input. What does this channel contain?\nOpen the analysis workflow file workflows/customrnaseq.nf. Use the .view() function inside the workflow scope to view the ch_samplesheet that has been input to the pipeline:\nworkflow CUSTOMRNASEQ {\n\n    take:\n    ch_samplesheet // channel: samplesheet read in from --input\n    main:\n\n    ch_samplesheet.view()\n\n    ch_versions = Channel.empty()\n    ch_multiqc_files = Channel.empty()\n\n    ...\n}\nNow, rerun the pipeline, ensuring -resume is specified in the nextflow run command. Note ebsure you are no longer inside your pipeline folder.\n\n\n\n\n\n\nImportant\n\n\n\nTO DO\nChange CONTAINER\n\n\nnextflow run ./nf-core-customrnaseq/main.nf -resume -profile CONTAINER --input ./samplesheet.csv --outdir output\nThe channel should have the following structure:\n[[id:gut, single_end:false], [/.../data/gut_1.fastq.gz, /.../data/gut_2.fastq.gz]]\n[[id:liver, single_end:false], [/.../data/liver_1.fastq.gz, /.../data/liver_2.fastq.gz]]\n[[id:lung, single_end:false], [/.../data/lung_1.fastq.gz, /.../data/lung_2.fastq.gz]]\nThis channel contains three elements, one for each sample type. The first element is a tuple, where the first element is a list that represents the sample metadata. This metadata contains the sample name, stored as id, and if the sample is single-ded, stored as single_end. The second element in this tuple contain the paths to the input FASTQ files.\nLet’s see how this relates to our samplesheet:\nsample,fastq_1,fastq_2\ngut,/.../data/gut_1.fastq.gz,/.../data/gut_2.fastq.gz\nliver,/.../data/liver_1.fastq.gz,/.../data/liver_2.fastq.gz\nlung,/.../data/lung_1.fastq.gz,/s.../data/lung_2.fastq.gz\nNotice that the value under the sample column has been assigned as id in the channel metadata. File paths in the fastq_1 and fastq_2 have been added as the second element in the tuple, which represents the read paths.\nThis is defined inside the assets/schema_input.json file. In this file, each “property” represents a column that can be present inside the --input samplesheet. Any required columns are also specified, as the \"required\" item.\n\n\n\n\n\n\nassets/schema_input.json\n\n\n\n\n\n{\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n    \"$id\": \"https://raw.githubusercontent.com/nf-core/customrnaseq/main/assets/schema_input.json\",\n    \"title\": \"nf-core/customrnaseq pipeline - params.input schema\",\n    \"description\": \"Schema for the file provided with params.input\",\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"sample\": {\n                \"type\": \"string\",\n                \"pattern\": \"^\\\\S+$\",\n                \"errorMessage\": \"Sample name must be provided and cannot contain spaces\",\n                \"meta\": [\"id\"]\n            },\n            \"fastq_1\": {\n                \"type\": \"string\",\n                \"format\": \"file-path\",\n                \"exists\": true,\n                \"pattern\": \"^\\\\S+\\\\.f(ast)?q\\\\.gz$\",\n                \"errorMessage\": \"FastQ file for reads 1 must be provided, cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'\"\n            },\n            \"fastq_2\": {\n                \"type\": \"string\",\n                \"format\": \"file-path\",\n                \"exists\": true,\n                \"pattern\": \"^\\\\S+\\\\.f(ast)?q\\\\.gz$\",\n                \"errorMessage\": \"FastQ file for reads 2 cannot contain spaces and must have extension '.fq.gz' or '.fastq.gz'\"\n            }\n        },\n        \"required\": [\"sample\", \"fastq_1\"]\n    }\n}\n\n\n\nInside the \"sample\" property, the \"meta\" has been set to [\"id\"]. This is the value in the channel metadata that the sample name will be assigned to. For example, if the following was specified: \"meta\": [\"name\"]\nThe parsed channel would have the following structure, where id is replaced with name:\n[[name:gut, single_end:false], [/.../data/gut_1.fastq.gz, /.../data/gut_2.fastq.gz]]\n[[name:liver, single_end:false], [/.../data/liver_1.fastq.gz, /.../data/liver_2.fastq.gz]]\n[[name:lung, single_end:false], [/.../data/lung_1.fastq.gz, /.../data/lung_2.fastq.gz]]\nTherefore, if you wish to specify an additional column in the sampleshet (ie. adding sample metadata), the schema_input.json should also be changed to allow for this. We will investigate this later in the session.\n\n\n\n\n\n\nTip\n\n\n\nMany existing nf-core nodules rely on the input metadata having at least the id value – it is not recommended to change this name from the default.\n\n\n\n\n6.1.2 Input channels to an nf-core module"
  },
  {
    "objectID": "workshops/5.3_metadata_propagation_CP.html#metadata-parsing",
    "href": "workshops/5.3_metadata_propagation_CP.html#metadata-parsing",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "7.1 Metadata Parsing",
    "text": "7.1 Metadata Parsing\nWe have covered a few different methods of metadata parsing.\n\n7.1.1 First Pass: .fromFilePairs\nA first pass attempt at pulling these files into Nextflow might use the fromFilePairs method:\nworkflow {\n    Channel.fromFilePairs(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/reads/*/*_R{1,2}.fastq.gz\")\n    .view\n}\nNextflow will pull out the first part of the fastq filename and returned us a channel of tuple elements where the first element is the filename-derived ID and the second element is a list of two fastq files.\nThe id is stored as a simple string. We’d like to move to using a map of key-value pairs because we have more than one piece of metadata to track. In this example, we have sample, replicate, tumor/normal, and treatment. We could add extra elements to the tuple, but this changes the ‘cardinality’ of the elements in the channel and adding extra elements would require updating all downstream processes. A map is a single object and is passed through Nextflow channels as one value, so adding extra metadata fields will not require us to change the cardinality of the downstream processes.\nThere are a couple of different ways we can pull out the metadata\nWe can use the tokenize method to split our id. To sanity-check, I just pipe the result directly into the view operator.\nworkflow {\n    Channel.fromFilePairs(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/reads/*/*_R{1,2}.fastq.gz\")\n    .map { id, reads -&gt;\n        tokens = id.tokenize(\"_\")\n    }\n    .view\n}\nIf we are confident about the stability of the naming scheme, we can destructure the list returned by tokenize and assign them to variables directly:\nmap { id, reads -&gt;\n    (sample, replicate, type) = id.tokenize(\"_\")\n    meta = [sample:sample, replicate:replicate, type:type]\n    [meta, reads]\n}\n\n\n\n\n\n\nNote\n\n\n\nMake sure that you're using a tuple with parentheses e.g. (one, two) rather than a List e.g. [one, two]\n\n\nIf we move back to the previous method, but decided that the ‘rep’ prefix on the replicate should be removed, we can use regular expressions to simply “subtract” pieces of a string. Here we remove a ‘rep’ prefix from the replicate variable if the prefix is present:\nmap { id, reads -&gt;\n    (sample, replicate, type) = id.tokenize(\"_\")\n    replicate -= ~/^rep/\n    meta = [sample:sample, replicate:replicate, type:type]\n    [meta, reads]\n}\nBy setting up our the “meta”, in our tuple with the format above, allows us to access the values in “sample” throughout our modules/configs as ${meta.sample}."
  },
  {
    "objectID": "workshops/5.3_metadata_propagation_CP.html#second-parse-.splitcsv",
    "href": "workshops/5.3_metadata_propagation_CP.html#second-parse-.splitcsv",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "Second Parse: .splitCsv",
    "text": "Second Parse: .splitCsv\nWe have briefly touched on .splitCsv in the first week.\nAs a quick overview\nAssuming we have the samplesheet\nsample_name,fastq1,fastq2\ngut_sample,/.../training/nf-training/data/ggal/gut_1.fq,/.../training/nf-training/data/ggal/gut_2.fq\nliver_sample,/.../training/nf-training/data/ggal/liver_1.fq,/.../training/nf-training/data/ggal/liver_2.fq\nlung_sample,/.../training/nf-training/data/ggal/lung_1.fq,/.../training/nf-training/data/ggal/lung_2.fq\nWe can set up a workflow to read in these files as:\nparams.reads = \"/.../rnaseq_samplesheet.csv\"\n\nreads_ch = Channel.fromPath(params.reads)\nreads_ch.view()\nreads_ch = reads_ch.splitCsv(header:true)\nreads_ch.view()\n\n\n\n\n\n\nChallenge\n\n\n\nUsing .splitCsv and .map read in the samplesheet below: /home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/samplesheet.csv\nSet the meta to contain the following keys from the header id, repeat and type\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nparams.input = \"/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/samplesheet.csv\"\n\nch_sheet = Channel.fromPath(params.input)\n\nch_sheet.splitCsv(header:true)\n    .map {\n        it -&gt;\n            [[it.id, it.repeat, it.type], it.fastq_1, it.fastq_2]\n    }.view()"
  },
  {
    "objectID": "workshops/5.3_metadata_propagation_CP.html#manipulating-metadata-and-channels",
    "href": "workshops/5.3_metadata_propagation_CP.html#manipulating-metadata-and-channels",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "7.2 Manipulating Metadata and Channels",
    "text": "7.2 Manipulating Metadata and Channels\nThere are a number of use cases where we will be interested in manipulating our metadata and channels.\nHere we will look at 2 use cases.\n\n7.2.1 Matching input channels\nAs we have seen in examples/challenges in the operators section, it is important to ensure that the format of the channels that you provide as inputs match the process definition.\nparams.reads = \"/home/Shared/For_NF_Workshop/training/nf-training/data/ggal/*_{1,2}.fq\"\n\nprocess printNumLines {\n    input:\n    path(reads)\n\n    output:\n    path(\"*txt\")\n\n    script:\n    \"\"\"\n    wc -l ${reads}\n    \"\"\"\n}\n\nworkflow {\n    ch_input = Channel.fromFilePairs(\"$params.reads\")\n    printNumLines( ch_input )\n}\nAs if the format does not match you will see and error similar to below:\n[myeung@papr-res-compute204 lesson7.1test]$ nextflow run test.nf \nN E X T F L O W  ~  version 23.04.1\nLaunching `test.nf` [agitated_faggin] DSL2 - revision: c210080493\n[-        ] process &gt; printNumLines -\nor if using nf-core template\nERROR ~ Error executing process &gt; 'PMCCCGTRC_UMIHYBCAP:UMIHYBCAP:PREPARE_GENOME:BEDTOOLS_SLOP'\n\nCaused by:\n  Not a valid path value type: java.util.LinkedHashMap ([id:genome_size])\n\n\nTip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`\n\n -- Check '.nextflow.log' file for details\nWhen encountering these errors there are two methods to correct this:\n\nChange the input definition in the process\nUse variations of the channel operators to correct the format of your channel\n\nThere are cases where changing the input definition is impractical (i.e. when using nf-core modules/subworkflows).\nLet’s take a look at some select modules.\nBEDTOOLS_SLOP\nBEDTOOLS_INTERSECT\n\n\n\n\n\n\nChallenge\n\n\n\nAssuming that you have the following inputs\nch_target = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals.bed\")\nch_bait = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals2.bed\").map { fn -&gt; [ [id: fn.baseName ], fn ] }\nch_sizes = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/genome.sizes\")\nWrite a mini workflow that:\n\nTakes the ch_target bedfile and extends the bed by 20bp on both sides using BEDTOOLS_SLOP (You can use the config definition below as a helper, or write your own as an additional challenge)\nTake the output from BEDTOOLS_SLOP and input this output with the ch_baits to BEDTOOLS_INTERSECT\n\nHINT: The modules can be imported from this location: /home/Shared/For_NF_Workshop/training/pmcc-test/modules/nf-core/bedtools\nHINT: You will need need the following operators to achieve this .map and .combine\n\n\n\n\n\n\n\n\nConfig\n\n\n\n\n\n\nprocess {\n    withName: 'BEDTOOLS_SLOP' {\n        ext.args = \"-b 20\"\n        ext.prefix = \"extended.bed\"\n    }\n\n    withName: 'BEDTOOLS_INTERSECT' {\n        ext.prefix = \"intersect.bed\"\n    }\n}\n:::\n\n:::{.callout-caution collapse=\"true\"}\n## **Solution**\n```default\ninclude { BEDTOOLS_SLOP } from '/home/Shared/For_NF_Workshop/training/pmcc-test/modules/nf-core/bedtools/slop/main'\ninclude { BEDTOOLS_INTERSECT } from '/home/Shared/For_NF_Workshop/training/pmcc-test/modules/nf-core/bedtools/intersect/main'\n\n\nch_target = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals.bed\")\nch_bait = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals2.bed\").map { fn -&gt; [ [id: fn.baseName ], fn ] }\nch_sizes = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/genome.sizes\")\n\nworkflow {\n    BEDTOOLS_SLOP ( ch_target.map{ fn -&gt; [ [id:fn.baseName], fn ]}, ch_sizes)\n\n    target_bait_bed = BEDTOOLS_SLOP.out.bed.combine( ch_bait )\n    BEDTOOLS_INTERSECT( target_bait_bed, ch_sizes.map{ fn -&gt; [ [id: fn.baseName], fn]} )\n}\nnextflow run nfcoretest.nf -profile singularity -c test2.config --outdir nfcoretest"
  },
  {
    "objectID": "workshops/5.3_metadata_propagation_CP.html#grouping-with-metadata",
    "href": "workshops/5.3_metadata_propagation_CP.html#grouping-with-metadata",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "7.3 Grouping with Metadata",
    "text": "7.3 Grouping with Metadata\nEarlier we introduced the function groupTuple\n\nch_reads = Channel.fromFilePairs(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/reads/*/*_R{1,2}.fastq.gz\")\n    .map { id, reads -&gt;\n        (sample, replicate, type) = id.tokenize(\"_\")\n        replicate -= ~/^rep/\n        meta = [sample:sample, replicate:replicate, type:type]\n    [meta, reads]\n}\n\n## Assume that we want to drop replicate from the meta and combine fastqs\n\nch_reads.map {\n    meta, reads -&gt; \n        [ meta - meta.subMap('replicate') + [data_type: 'fastq'], reads ]\n    }\n    .groupTuple().view()"
  },
  {
    "objectID": "workshops/4.1_modules.html",
    "href": "workshops/4.1_modules.html",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of Nextflow modules and subworkflows\nGain an understanding of Nextflow workflow structures\nExplore some groovy functions and libraries\nSetup config, profile, and some test data"
  },
  {
    "objectID": "workshops/4.1_modules.html#environment-setup",
    "href": "workshops/4.1_modules.html#environment-setup",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "Environment Setup",
    "text": "Environment Setup\nSet up an interactive shell to run our Nextflow workflow:\nsrun --pty -p prod_short --mem 8GB --mincpus 2 -t 0-2:00 bash\nLoad the required modules to run Nextflow:\nmodule load nextflow/23.04.1\nmodule load singularity/3.7.3\nSet the singularity cache environment variable:\nexport NXF_SINGULARITY_CACHEDIR=/config/binaries/singularity/containers_devel/nextflow\nSingularity images downloaded by workflow executions will now be stored in this directory.\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session. A complete list of environment variables can be found here."
  },
  {
    "objectID": "workshops/4.1_modules.html#modularization",
    "href": "workshops/4.1_modules.html#modularization",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5. Modularization",
    "text": "5. Modularization\nThe definition of module libraries simplifies the writing of complex data analysis workflows and makes re-use of processes much easier.\nUsing the rnaseq.nf example from previous section, you can convert the workflow’s processes into modules, then call them within the workflow scope.\n#!/usr/bin/env nextflow\n\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\nprocess INDEX {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nprocess FASTQC {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QUANTIFICATION(index_ch, reads_ch)\n  quant_ch.view()\n\n  fastqc_ch = FASTQC(reads_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}"
  },
  {
    "objectID": "workshops/4.1_modules.html#modules",
    "href": "workshops/4.1_modules.html#modules",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1 Modules",
    "text": "5.1 Modules\nNextflow DSL2 allows for the definition of stand-alone module scripts that can be included and shared across multiple workflows. Each module can contain its own process or workflow definition."
  },
  {
    "objectID": "workshops/4.1_modules.html#importing-modules",
    "href": "workshops/4.1_modules.html#importing-modules",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1.1. Importing modules",
    "text": "5.1.1. Importing modules\nComponents defined in the module script can be imported into other Nextflow scripts using the include statement. This allows you to store these components in one or more file(s) that they can be re-used in multiple workflows.\nUsing the rnaseq.nf example, you can achieve this by:\nCreating a file called modules.nf in the top-level directory. Copying and pasting all process definitions for INDEX, QUANTIFICATION, FASTQC and MULTIQC into modules.nf. Removing the process definitions in the rnaseq.nf script. Importing the processes from modules.nf within the rnaseq.nf script anywhere above the workflow definition:\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION } from './modules.nf'\ninclude { FASTQC } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\n\n\n\n\n\n\nTip\n\n\n\nIn general, you would use relative paths to define the location of the module scripts using the ./prefix.\n\n\nExercise\nCreate a modules.nf file with the INDEX, QUANTIFICATION, FASTQC and MULTIQC from rnaseq.nf. Then remove these processes from rnaseq.nf and include them in the workflow using the include definitions shown above.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe rnaseq.nf script should look similar to this:\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION } from './modules.nf'\ninclude { FASTQC } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QUANTIFICATION(index_ch, reads_ch)\n  quant_ch.view()\n\n  fastqc_ch = FASTQC(reads_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\n\n\nRun the pipeline to check if the module import is successful\nnextflow run rnaseq.nf --outdir \"results\" -resume\n\n\n\n\n\n\nChallenge\nTry modularising the modules.nf even further to achieve a setup of one tool per module (can be one or more processes), similar to the setup used by most nf-core pipelines\nnfcore/rna-seq\n  | modules\n    | local\n      | multiqc\n      | deseq2_qc\n    | nf-core\n      | fastqc\n      | salmon\n        | index\n          | main.nf\n        | quant\n          | main.nf"
  },
  {
    "objectID": "workshops/4.1_modules.html#multiple-imports",
    "href": "workshops/4.1_modules.html#multiple-imports",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1.2. Multiple imports",
    "text": "5.1.2. Multiple imports\nIf a Nextflow module script contains multiple process definitions they can also be imported using a single include statement as shown in the example below:\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX; QUANTIFICATION; FASTQC; MULTIQC } from './modules.nf'\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QUANTIFICATION(index_ch, reads_ch)\n  fastqc_ch = FASTQC(reads_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}"
  },
  {
    "objectID": "workshops/4.1_modules.html#module-aliases",
    "href": "workshops/4.1_modules.html#module-aliases",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1.3 Module aliases",
    "text": "5.1.3 Module aliases\nWhen including a module component it is possible to specify a name alias using the as declaration. This allows the inclusion and the invocation of the same component multiple times using different names:\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n\n  input:\n    tuple val(sample_id), path(reads)\n  \n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n\n}\nNote how the QUANTIFICATION process is now being refer to as QT, and FASTQC process is imported twice, each time with a different alias, and how these aliases are used to invoke the processes.\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [sharp_meitner] DSL2 - revision: 6afd5bf37c\nexecutor &gt;  local (16)\n[c7/56160a] process &gt; INDEX          [100%] 1 of 1 ✔\n[75/cb99dd] process &gt; QT (3)         [100%] 3 of 3 ✔\n[d9/e298c6] process &gt; FASTQC_one (3) [100%] 3 of 3 ✔\n[5e/7ccc39] process &gt; TRIMGALORE (3) [100%] 3 of 3 ✔\n[a3/3a1e2e] process &gt; FASTQC_two (3) [100%] 3 of 3 ✔\n[e1/411323] process &gt; MULTIQC (3)    [100%] 3 of 3 ✔\n\n\n\n\n\n\nWarning\n\n\n\nWhat do you think will happen if FASTQC is imported only once without alias, but used twice within the workflow?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nProcess 'FASTQC' has been already used -- If you need to reuse the same component, include it with a different name or include it in a different workflow context"
  },
  {
    "objectID": "workshops/4.1_modules.html#workflow-definition",
    "href": "workshops/4.1_modules.html#workflow-definition",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2 Workflow definition",
    "text": "5.2 Workflow definition\nThe workflow scope allows the definition of components that define the invocation of one or more processes or operators:\n\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow my_workflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\nworkflow {\n  my_workflow()\n}\nFor example, the snippet above defines a workflow named my_workflow, that is invoked via another workflow definition."
  },
  {
    "objectID": "workshops/4.1_modules.html#workflow-inputs",
    "href": "workshops/4.1_modules.html#workflow-inputs",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2.1 Workflow inputs",
    "text": "5.2.1 Workflow inputs\nA workflow component can declare one or more input channels using the take statement. When the take statement is used, the workflow definition needs to be declared within the main block.\nFor example:\n\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow my_workflow {\n  take:\n  transcriptome_file\n  reads_ch\n\n  main:\n  index_ch = INDEX(transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\nThe input for the workflowcan then be specified as an argument:\nworkflow {\n  my_workflow(Channel.of(params.transcriptome_file), reads_ch)\n}"
  },
  {
    "objectID": "workshops/4.1_modules.html#workflow-outputs",
    "href": "workshops/4.1_modules.html#workflow-outputs",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2.2 Workflow outputs",
    "text": "5.2.2 Workflow outputs\nA workflow can declare one or more output channels using the emit statement. For example:\n\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow my_workflow {\n  take:\n  transcriptome_file\n  reads_ch\n\n  main:\n  index_ch = INDEX(transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n\n  emit:\n  quant_ch\n\n}\n\nworkflow {\n  my_workflow(Channel.of(params.transcriptome_file), reads_ch)\n  my_workflow.out.view()\n}\nAs a result, you can use the my_workflow.out notation to access the outputs of my_workflow in the invoking workflow.\nYou can also declare named outputs within the emit block.\n  emit:\n  my_wf_output = quant_ch\nworkflow {\n  my_workflow(Channel.of(params.transcriptome_file), reads_ch)\n  my_workflow.out.my_wf_output.view()\n}\nThe result of the above snippet can then be accessed using my_workflow.out.my_wf_output."
  },
  {
    "objectID": "workshops/4.1_modules.html#calling-named-workflows",
    "href": "workshops/4.1_modules.html#calling-named-workflows",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2.3 Calling named workflows",
    "text": "5.2.3 Calling named workflows\nWithin a main.nf script (called rnaseq.nf in our example) you can also have multiple workflows. In which case you may want to call a specific workflow when running the code. For this you could use the entrypoint call -entry &lt;workflow_name&gt;.\nThe following snippet has two named workflows (quant_wf and qc_wf):\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow quant_wf {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n}\n\nworkflow qc_wf {\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\nworkflow {\n  quant_wf(Channel.of(params.transcriptome_file), reads_ch)\n  qc_wf(reads_ch, quant_wf.out)\n}\nBy default, running the main.nf (called rnaseq.nf in our example) will execute the main workflow block.\nnextflow run runseq.nf --outdir \"results\"\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq4.nf` [goofy_mahavira] DSL2 - revision: 2125d44217\nexecutor &gt;  local (12)\n[38/e34e41] process &gt; quant_wf:INDEX (1)   [100%] 1 of 1 ✔\n[9e/afc9e0] process &gt; quant_wf:QT (1)      [100%] 1 of 1 ✔\n[c1/dc84fe] process &gt; qc_wf:FASTQC_one (3) [100%] 3 of 3 ✔\n[2b/48680f] process &gt; qc_wf:TRIMGALORE (3) [100%] 3 of 3 ✔\n[13/71e240] process &gt; qc_wf:FASTQC_two (3) [100%] 3 of 3 ✔\n[07/cf203f] process &gt; qc_wf:MULTIQC (1)    [100%] 1 of 1 ✔\nNote that the process is now annotated with &lt;workflow-name&gt;:&lt;process-name&gt;\nBut you can choose which workflow to run by using the entry flag:\nnextflow run runseq.nf --outdir \"results\" -entry quant_wf\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq5.nf` [magical_picasso] DSL2 - revision: 4ddb8eaa12\nexecutor &gt;  local (4)\n[a7/152090] process &gt; quant_wf:INDEX  [100%] 1 of 1 ✔\n[cd/612b4a] process &gt; quant_wf:QT (1) [100%] 3 of 3 ✔"
  },
  {
    "objectID": "workshops/4.1_modules.html#importing-subworkflows",
    "href": "workshops/4.1_modules.html#importing-subworkflows",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2.4 Importing Subworkflows",
    "text": "5.2.4 Importing Subworkflows\nSimilar to module script, workflow or sub-workflow can also be imported into other Nextflow scripts using the include statement. This allows you to store these components in one or more file(s) that they can be re-used in multiple workflows.\nAgain using the rnaseq.nf example, you can achieve this by:\nCreating a file called subworkflows.nf in the top-level directory. Copying and pasting all workflow definitions for quant_wf and qc_wf into subworkflows.nf. Removing the workflow definitions in the rnaseq.nf script. Importing the sub-workflows from subworkflows.nf within the rnaseq.nf script anywhere above the workflow definition:\ninclude { QUANT_WF } from './subworkflows.nf'\ninclude { QC_WF } from './subworkflows.nf'\nExercise\nCreate a subworkflows.nf file with the QUANT_WF, and QC_WF from the previous sections. Then remove these processes from rnaseq.nf and include them in the workflow using the include definitions shown above.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe rnaseq.nf script should look similar to this:\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { QUANT_WF; QC_WF } from './subworkflows.nf'\n\nworkflow {\n  QUANT_WF(Channel.of(params.transcriptome_file), reads_ch)\n  QC_WF(reads_ch, QUANT_WF.out)\n}\nand the subworkflows.nf script should look similar to this:\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow QUANT_WF{\n  take:\n  transcriptome_file\n  reads_ch\n\n  main:\n  index_ch = INDEX(transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n\n  emit:\n  quant_ch\n}\n\nworkflow QC_WF{\n  take:\n  reads_ch\n  quant_ch\n\n  main:\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n\n  emit:\n  multiqc_ch\n}\n\n\n\nRun the pipeline to check if the workflow import is successful\nnextflow run rnaseq.nf --outdir \"results\" -resume\n\n\n\n\n\n\nChallenge\nStructure modules and subworkflows similar to the setup used by most nf-core pipelines (e.g. nf-core/rnaseq)"
  },
  {
    "objectID": "workshops/4.1_modules.html#workflow-structure",
    "href": "workshops/4.1_modules.html#workflow-structure",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.3 Workflow Structure",
    "text": "5.3 Workflow Structure\nThere are three directories in a Nextflow workflow repository that have a special purpose:"
  },
  {
    "objectID": "workshops/4.1_modules.html#bin",
    "href": "workshops/4.1_modules.html#bin",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.3.1 ./bin",
    "text": "5.3.1 ./bin\nThe bin directory (if it exists) is always added to the $PATH for all tasks. If the tasks are performed on a remote machine, the directory is copied across to the new machine before the task begins. This Nextflow feature is designed to make it easy to include accessory scripts directly in the workflow without having to commit those scripts into the container. This feature also ensures that the scripts used inside of the workflow move on the same revision schedule as the workflow itself.\nIt is important to know that Nextflow will take care of updating $PATH and ensuring the files are available wherever the task is running, but will not change the permissions of any files in that directory. If a file is called by a task as an executable, the workflow developer must ensure that the file has the correct permissions to be executed.\nFor example, let’s say we have a small R script that produces a csv and a tsv:\n\n#!/usr/bin/env Rscript\nlibrary(tidyverse)\n\nplot &lt;- ggplot(mpg, aes(displ, hwy, colour = class)) + geom_point()\nmtcars |&gt; write_tsv(\"cars.tsv\")\nggsave(\"cars.png\", plot = plot)\nWe’d like to use this script in a simple workflow car.nf:\nprocess PlotCars {\n    // container 'rocker/tidyverse:latest'\n    container '/config/binaries/singularity/containers_devel/nextflow/r-dinoflow_0.1.1.sif'\n\n    output:\n    path(\"*.png\"), emit: \"plot\"\n    path(\"*.tsv\"), emit: \"table\"\n\n    script:\n    \"\"\"\n    cars.R\n    \"\"\"\n}\n\nworkflow {\n    PlotCars()\n\n    PlotCars.out.table | view { \"Found a tsv: $it\" }\n    PlotCars.out.plot | view { \"Found a png: $it\" }\n}\nTo do this, we can create the bin directory, write our R script into the directory. Finally, and crucially, we make the script executable:\nchmod +x bin/cars.R\n\n\n\n\n\n\nWarning\n\n\n\nAlways ensure that your scripts are executable. The scripts will not be available to your Nextflow processes without this step.\nYou will get the following error if permission is not set correctly.\nERROR ~ Error executing process &gt; 'PlotCars'\n\nCaused by:\n  Process `PlotCars` terminated with an error exit status (126)\n\nCommand executed:\n\n  cars.R\n\nCommand exit status:\n  126\n\nCommand output:\n  (empty)\n\nCommand error:\n  .command.sh: line 2: /scratch/users/.../bin/cars.R: Permission denied\n\nWork dir:\n  /scratch/users/.../work/6b/86d3d0060266b1ca515cc851d23890\n\nTip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`\n\n -- Check '.nextflow.log' file for details\n\n\nLet’s run the script and see what Nextflow is doing for us behind the scenes:\nnextflow run car.nf\nand then inspect the .command.run file that Nextflow has generated\nYou’ll notice a nxf_container_env bash function that appends our bin directory to $PATH:\nnxf_container_env() {\ncat &lt;&lt; EOF\nexport PATH=\"\\$PATH:/scratch/users/&lt;your-user-name&gt;/.../bin\"\nEOF\n}\nWhen working on the cloud, Nextflow will also ensure that the bin directory is copied onto the virtual machine running your task in addition to the modification of $PATH."
  },
  {
    "objectID": "workshops/4.1_modules.html#templates",
    "href": "workshops/4.1_modules.html#templates",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.3.2 ./templates",
    "text": "5.3.2 ./templates\nIf a process script block is becoming too long, it can be moved to a template file. The template file can then be imported into the process script block using the template method. This is useful for keeping the process block tidy and readable. Nextflow’s use of $ to indicate variables also allows for directly testing the template file by running it as a script.\nFor example:\n# cat templates/my_script.sh\n\n#!/bin/bash\necho \"process started at `date`\"\necho $name\necho \"process completed\"\nprocess SayHiTemplate {\n    debug true\n    input: \n      val(name)\n\n    script: \n      template 'my_script.sh'\n}\n\nworkflow {\n    SayHiTemplate(\"Hello World\")\n}\nBy default, Nextflow looks for the my_script.sh template file in the templates directory located alongside the Nextflow script and/or the module script in which the process is defined. Any other location can be specified by using an absolute template path."
  },
  {
    "objectID": "workshops/4.1_modules.html#lib",
    "href": "workshops/4.1_modules.html#lib",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.3.3 ./lib",
    "text": "5.3.3 ./lib\nIn the next chapter, we will start looking into adding small helper Groovy functions to the main.nf file. It may at times be helpful to bundle functionality into a new Groovy class. Any classes defined in the lib directory are available for use in the workflow - both main.nf and any imported modules.\nClasses defined in lib directory can be used for a variety of purposes. For example, the nf-core/rnaseq workflow uses five custom classes:\n\nNfcoreSchema.groovy for parsing the schema.json file and validating the workflow parameters.\nNfcoreTemplate.groovy for email templating and nf-core utility functions.\nUtils.groovy for provision of a single checkCondaChannels method.\nWorkflowMain.groovy for workflow setup and to call the NfcoreTemplate class.\nWorkflowRnaseq.groovy for the workflow-specific functions.\n\nThe classes listed above all provide utility executed at the beginning of a workflow, and are generally used to “set up” the workflow. However, classes defined in lib can also be used to provide functionality to the workflow itself."
  },
  {
    "objectID": "workshops/4.1_modules.html#groovy-functions-and-libraries",
    "href": "workshops/4.1_modules.html#groovy-functions-and-libraries",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6. Groovy Functions and Libraries",
    "text": "6. Groovy Functions and Libraries\nNextflow is a domain specific language (DSL) implemented on top of the Groovy programming language, which in turn is a super-set of the Java programming language. This means that Nextflow can run any Groovy or Java code.\nYou have already been using some Groovy code in the previous sections, but now it’s time to learn more about it."
  },
  {
    "objectID": "workshops/4.1_modules.html#some-useful-groovy-introduction",
    "href": "workshops/4.1_modules.html#some-useful-groovy-introduction",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6.1 Some useful groovy introduction",
    "text": "6.1 Some useful groovy introduction"
  },
  {
    "objectID": "workshops/4.1_modules.html#variables",
    "href": "workshops/4.1_modules.html#variables",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6.1.1 Variables",
    "text": "6.1.1 Variables\nTo define a variable, simply assign a value to it:\nx = 1\nprintln x\n\nx = new java.util.Date()\nprintln x\n\nx = -3.1499392\nprintln x\n\nx = false\nprintln x\n\nx = \"Hi\"\nprintln x\n&gt;&gt; nextflow run variable.nf\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `variable.nf` [trusting_moriondo] DSL2 - revision: ee74c86d04\n1\nWed Jun 05 03:45:19 AEST 2024\n-3.1499392\nfalse\nHi\nLocal variables are defined using the def keyword:\ndef x = 'foo'\nThe def should be always used when defining variables local to a function or a closure."
  },
  {
    "objectID": "workshops/4.1_modules.html#maps",
    "href": "workshops/4.1_modules.html#maps",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6.1.2 Maps",
    "text": "6.1.2 Maps\nMaps are like lists that have an arbitrary key instead of an integer (allow key-value pair).\nmap = [a: 0, b: 1, c: 2]\nMaps can be accessed in a conventional square-bracket syntax or as if the key was a property of the map.\nmap = [a: 0, b: 1, c: 2]\n\nassert map['a'] == 0 \nassert map.b == 1 \nassert map.get('c') == 2 \nTo add data or to modify a map, the syntax is similar to adding values to a list:\nmap = [a: 0, b: 1, c: 2]\n\nmap['a'] = 'x' \nmap.b = 'y' \nmap.put('c', 'z') \nassert map == [a: 'x', b: 'y', c: 'z']\nMap objects implement all methods provided by the java.util.Map interface, plus the extension methods provided by Groovy."
  },
  {
    "objectID": "workshops/4.1_modules.html#if-statement",
    "href": "workshops/4.1_modules.html#if-statement",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6.1.3 If statement",
    "text": "6.1.3 If statement\nThe if statement uses the same syntax common in other programming languages, such as Java, C, and JavaScript.\nif (&lt; boolean expression &gt;) {\n    // true branch\n}\nelse {\n    // false branch\n}\nThe else branch is optional. Also, the curly brackets are optional when the branch defines just a single statement.\nx = 1\nif (x &gt; 10)\n    println 'Hello'\nIn some cases it can be useful to replace the if statement with a ternary expression (aka a conditional expression):\nprintln list ? list : 'The list is empty'\nThe previous statement can be further simplified using the Elvis operator:\nprintln list ?: 'The list is empty'\nExercise\nWe are going to turn the rnaseq.nf into a conditional workflow with an additional params.qc_enabled to set an on/off trigger for the QC parts of the workflow.\nparams.qc_enabled = false\n\nworkflow {\n  QUANT_WF(Channel.of(params.transcriptome_file), reads_ch)\n\n  if (params.qc_enabled) {\n    QC_WF(reads_ch, QUANT_WF.out)\n  }\n}\nRun the workflow again:\nnextflow run rnaseq.nf --outdir \"results\"\nWe should only see the following two stages being executed.\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [hopeful_gautier] DSL2 - revision: 7c50056656\nexecutor &gt;  local (2)\n[c3/91f695] process &gt; QUANT_WF:INDEX (1) [100%] 1 of 1 ✔\n[1d/fac0d9] process &gt; QUANT_WF:QT (1)    [100%] 1 of 1 ✔\nThe params.qc_enabled can be turn on during execution.\nnextflow run rnaseq.nf --outdir \"results\" --qc_enabled true\n\n\n\n\n\n\nChallenge\nThe trimgalore currently only supports paired-end read. How do we update this so the same process can be used for both single-end and paired-end?\nFor reference, the (simplified) command that we can use for single-end can be as follow:\n  trim_galore \\\\\n    --gzip \\\\\n    $reads"
  },
  {
    "objectID": "workshops/4.1_modules.html#functions",
    "href": "workshops/4.1_modules.html#functions",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6.1.4 Functions",
    "text": "6.1.4 Functions\nIt is possible to define a custom function into a script:\ndef fib(int n) {\n    return n &lt; 2 ? 1 : fib(n - 1) + fib(n - 2)\n}\n\nassert fib(10)==89\nA function can take multiple arguments separating them with a comma.\nThe return keyword can be omitted and the function implicitly returns the value of the last evaluated expression. Also, explicit types can be omitted, though not recommended:\ndef fact(n) {\n    n &gt; 1 ? n * fact(n - 1) : 1\n}\n\nassert fact(5) == 120"
  },
  {
    "objectID": "workshops/4.1_modules.html#testing",
    "href": "workshops/4.1_modules.html#testing",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "7. Testing",
    "text": "7. Testing"
  },
  {
    "objectID": "workshops/4.1_modules.html#stub",
    "href": "workshops/4.1_modules.html#stub",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "7.1 Stub",
    "text": "7.1 Stub\nYou can define a command stub, which replaces the actual process command when the -stub-run or -stub command-line option is enabled:\n\nprocess INDEX {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n\n    stub:\n    \"\"\"\n    mkdir salmon_idx\n    touch salmon_idx/seq.bin\n    touch salmon_idx/info.json\n    touch salmon_idx/refseq.bin\n    \"\"\"\n}\nThe stub block can be defined before or after the script block. When the pipeline is executed with the -stub-run option and a process’s stub is not defined, the script block is executed.\nThis feature makes it easier to quickly prototype the workflow logic without using the real commands. The developer can use it to provide a dummy script that mimics the execution of the real one in a quicker manner. In other words, it is a way to perform a dry-run.\nExercise\nTry modifying modules.nf to add stub for the INDEX process.\n    \"\"\"\n    mkdir salmon_idx\n    touch salmon_idx/seq.bin\n    touch salmon_idx/info.json\n    touch salmon_idx/refseq.bin\n    \"\"\"\nLet’s keep the workflow to only run the INDEX process, as a new rnaseq_stub.nf\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n}\nAnd run the rnaseq_stub.nf with -stub-run\nnextflow run rnaseq_stub.nf -stub-run\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [lonely_albattani] DSL2 - revision: 11fb1399f0\nexecutor &gt;  local (1)\n[a9/7d3084] process &gt; INDEX [100%] 1 of 1 ✔\nThe process should look like it is running as normal. But if we inspect the work folder a9/7d3084, you will notice that the salmon_idx folder actually consists of three empty files that we touch as part of stub.\nls -la work/a9/7d3084636d95cba6b81a9ce8125289/salmon_idx/\ntotal 1\ndrwxrwxr-x 2 rlupat rlupat 4096 Jun  5 11:05 .\ndrwxrwxr-x 3 rlupat rlupat 4096 Jun  5 11:05 ..\n-rw-rw-r-- 1 rlupat rlupat    0 Jun  5 11:05 info.json\n-rw-rw-r-- 1 rlupat rlupat    0 Jun  5 11:05 refseq.bin\n-rw-rw-r-- 1 rlupat rlupat    0 Jun  5 11:05 seq.bin\n\n\n\n\n\n\nChallenge\nAdd stubs to all modules in modules.nf and try running the full workflow in a stub."
  },
  {
    "objectID": "workshops/4.1_modules.html#nf-test",
    "href": "workshops/4.1_modules.html#nf-test",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "7.2. nf-test",
    "text": "7.2. nf-test\nIt is critical for reproducibility and long-term maintenance to have a way to systematically test that every part of your workflow is doing what it’s supposed to do. To that end, people often focus on top-level tests, in which the workflow is un on some test data from start to finish. This is useful but unfortunately incomplete. You should also implement module-level tests (equivalent to what is called ‘unit tests’ in general software engineering) to verify the functionality of individual components of your workflow, ensuring that each module performs as expected under different conditions and inputs.\nThe nf-test package provides a testing framework that integrates well with Nextflow and makes it straightforward to add both module-level and workflow-level tests to your pipeline. For more background information, read the blog post about nf-test on the nf-core blog.\nSee this tutorial for some examples.\n\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, and Nextflow Patterns materials from Nextflow and nf-core"
  },
  {
    "objectID": "workshops/5.4_pipeline_testing.html",
    "href": "workshops/5.4_pipeline_testing.html",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of how version control is utilised throughout the pipeline\nUse nf-core lint to lint the pipeline"
  },
  {
    "objectID": "workshops/5.4_pipeline_testing.html#version-control",
    "href": "workshops/5.4_pipeline_testing.html#version-control",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "7.1 Version control",
    "text": "7.1 Version control\nIn every nf-core module, a versions.yml file has been emitted as output. Let’s collect all these files together into one channel ch_versions, which will contain the versions used for every tool in the pipeline.\nFor the SALMON_INDEX process, this can be done by using the .out attribute, along with versions. This file is then added to the list of files already present in ch_versions, using the .mix operator.\n    SALMON_INDEX ( \n        ch_genome_fasta,\n        ch_transcript_fasta\n    )\n\n    ch_versions = ch_versions.mix(SALMON_INDEX.out.versions)\nExercise: Add the versions for SALMON_QUANT to ch_versions\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo add the version file for SALMON_QUANT to ch_versions, the .out.versions attribute can be used. This is then added to ch_versions using the .mix operator:\n    ch_versions = ch_versions.mix(SALMON_QUANT.out.versions)\n\n\n\nIn nf-core, the existing softwareVersionsToYAML function will take all .yml files inside ch_versions, creating one large file that traks all software versions.\n    //\n    // Collate and save software versions\n    //\n    softwareVersionsToYAML(ch_versions)\n        .collectFile(\n            storeDir: \"${params.outdir}/pipeline_info\",\n            name: 'nf_core_'  +  'customrnaseq_software_'  + 'mqc_'  + 'versions.yml',\n            sort: true,\n            newLine: true\n        ).set { ch_collated_versions }\nThis file is saved in the pipeline_info folder of the output directory:\noutput/pipeline_info/nf_core_customrnaseq_software_mqc_versions.yml\nIn addition to tool versions used, the Nextflow version, and pipeline version is also recorded\nFASTQC:\n  fastqc: 0.12.1\nSALMON_INDEX:\n  salmon: 1.10.3\nSALMON_QUANT:\n  salmon: 1.10.3\nWorkflow:\n    nf-core/customrnaseq: v1.0.0dev\n    Nextflow: 24.10.5"
  },
  {
    "objectID": "workshops/5.4_pipeline_testing.html#other-resources",
    "href": "workshops/5.4_pipeline_testing.html#other-resources",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "8.1 Other resources",
    "text": "8.1 Other resources\n\n8.1.1 Pipeline linting\nThe nf-core pipelines lint command can be used to check that a given pipeline follow all nf-core community guidelines. This is the same test that is used on the automated continuous integration tests, and is important if you would like to contribute to nf-core.\nTo contribute your pipeline to nf-core, full documentation is available.\n\n\n8.1.2 Pipeline test profiles\nAnother important feature of nf-core pipelines are their test profiles. Pipeline level tests can facilitate more reliable and reproducible pipelines by ensuring identical results are produced at every run. More documentation from nf-core is available here and here\n\n\n8.1.2 Pushing to GitHub\nCurrently, we have developed our pipeline locally. However, creating a remote repository can further improve the continuous integration process and streamline work if multiple people are working on the same pipeline. See documentation available here.\n\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, Nextflow Patterns materials from Nextflow, nf-core nf-core tools documentation and nf-validation –&gt;"
  },
  {
    "objectID": "workshops/1.2_intro_nf_core.html",
    "href": "workshops/1.2_intro_nf_core.html",
    "title": "Introduction to nf-core",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn the core features and concepts of nf-core.\nLearn nf-core terminology.\nUse Nextflow to pull and run the nf-core/testpipeline workflow\nGain an understanding of pipeline template structure for nf-core/rnaseq\n\n\n\n\n1.2.1. What is nf-core?\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nnf-core provides a standardized set of best practices, guidelines, and templates for building and sharing bioinformatics workflows. These workflows are designed to be modular, scalable, and portable, allowing researchers to easily adapt and execute them using their own data and compute resources.\nThe community is a diverse group of bioinformaticians, developers, and researchers from around the world who collaborate on developing and maintaining a growing collection of high-quality workflows. These workflows cover a range of applications, including transcriptomics, proteomics, and metagenomics.\nOne of the key benefits of nf-core is that it promotes open development, testing, and peer review, ensuring that the workflows are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries.\nnf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276–278 (2020). Nature Biotechnology\nKey Features of nf-core workflows\n\nDocumentation\n\nnf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won’t be left in the dark.\n\nStable Releases\n\nnf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible.\n\nPackaged software\n\nPipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations.\n\nPortable and reproducible\n\nnf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to execute.\n\nCloud-ready\n\nnf-core workflows are tested on AWS\n\n\n\n\n1.2.2. Executing an nf-core workflow\nThe nf-core website has a full list of workflows and asssociated documentation to be explored.\nEach workflow has a dedicated page that includes expansive documentation that is split into 7 sections:\n\nIntroduction: an introduction and overview of the workflow\nResults: Example output files generated from the full test dataset\nUsage docs: Descriptions of how to execute the workflow\nParameters: Grouped workflow parameters with descriptions\nOutput docs: Descriptions and examples of the expected output files\nReleases & Statistics: Workflow version history and statistics\n\nSince nf-core is a community development project, the code for a pipeline can change at any time. To ensure that you are using a specific version of a pipeline, you can use Nextflow’s built-in functionality to pull a workflow. The following command can be used to download and cache workflows from GitHub repositories:\nnextflow pull nf-core/&lt;pipeline&gt;\nNextflow run will also automatically pull the workflow if it was not already available locally:\nnextflow run nf-core/&lt;pipeline&gt;\nNextflow will pull the default git branch if a workflow version is not specified. This will be the master branch for nf-core workflows with a stable release. nf-core workflows use GitHub releases to tag stable versions of the code and software. You will always be able to execute a previous version of a workflow once it is released using the -revision or -r flag.\nFor this section of the workshop we will be using the nf-core/testpipeline as an example. Navigate back to your work directory and create a separate directory for this section.\nmkdir ./nf_testpipeline && cd $_\nExercise: Pull the nf-core/testpipeline pipeline\n\n\n\n\n\n\nSolution\n\n\n\n\n\nnextflow pull nf-core/testpipeline\n\n\n\n\n\n\n\n\n\nRunning on your institutional compute\n\n\n\n\n\nWhen running on your institutional compute remember to follow the requirements of your HPC, such as:\n\nNot running on the login node of the HPC\nLoading your required software (i.e. nextflow/singularity/apptainer)\n\n\n\n\n\n\n1.2.3. Workflow structure\nnf-core workflows start from a common template and follow the same structure. Although you won’t need to edit code in the workflow project directory, having a basic understanding of the project structure and some core terminology will help you understand how to configure its execution.\nLet’s take a look at the code for the nf-core/rnaseq pipeline.\n\nNotice that the typical nf-core workflow doesn’t contain just one .nf file, but instead contains multiple folders that categorise different elements of the workflow. Nf-core workflows typically consists of a main workflow file, main.nf, which launches the analysis workflow file workflows/&lt;workflow&gt;.nf that invokes the modules and subworkflows.\n\n\n1.2.4. Modules\nThe modules folder contain both local and nf-core processes, each contained within their own tool folder inside a script called main.nf.\n\nLook through the contents of modules/nf-core/bedtools/genomecov/main.nf. Notice that it only consists of a single process that will execute a single tool.\nRecall the following components of a process:\n\ninputs: declares any inputs to the process, along with the variable qualifier\noutputs: declares any outputs of the process, along with the variable qualifier\nscript: executes commands\n\nCustom processes that are specific to a particular pipeline are stored inside the local folder. Nf-core modules that have already been tested and developed by the nf-core community are stored inside a separate nf-core folder. A full list of available modules can be seen here or on the nf-core modules GitHub repository.\nThese processes/modules can then be imported into an overarching analysis workflow file that chains each step together.\nExercise: How many modules are in nf-core/testpipeline? Are they local to the pipeline or are they stored in the nf-core modules GitHub repository? What are the names of the modules?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are two modules in nf-core/testpipeline, fastqc and multiqc. Since the modules are stored inside the folder nf-core, they can are available on the nf-core modules GitHub repository.\n\n\n\n\n\n1.2.5. Subworkflows\nA subworkflow contains groups of modules that are used in combination with each other and have a common purpose. Subworkflows improve workflow readability and help with the reuse of modules within a workflow. The nf-core community also shares subworkflows in the nf-core subworkflows GitHub repository. Local subworkflows are workflow specific, and are not shared in the nf-core subworkflows repository.\nLet’s take a look at the BAM_STATS_SAMTOOLS subworkflow:\n//\n// Run SAMtools stats, flagstat and idxstats\n//\n\ninclude { SAMTOOLS_STATS    } from '../../../modules/nf-core/samtools/stats/main'\ninclude { SAMTOOLS_IDXSTATS } from '../../../modules/nf-core/samtools/idxstats/main'\ninclude { SAMTOOLS_FLAGSTAT } from '../../../modules/nf-core/samtools/flagstat/main'\n\nworkflow BAM_STATS_SAMTOOLS {\n    take:\n    ch_bam_bai // channel: [ val(meta), path(bam), path(bai) ]\n    ch_fasta   // channel: [ path(fasta) ]\n\n    main:\n    ch_versions = Channel.empty()\n\n    SAMTOOLS_STATS ( ch_bam_bai, ch_fasta )\n    ch_versions = ch_versions.mix(SAMTOOLS_STATS.out.versions)\n\n    SAMTOOLS_FLAGSTAT ( ch_bam_bai )\n    ch_versions = ch_versions.mix(SAMTOOLS_FLAGSTAT.out.versions)\n\n    SAMTOOLS_IDXSTATS ( ch_bam_bai )\n    ch_versions = ch_versions.mix(SAMTOOLS_IDXSTATS.out.versions)\n\n    emit:\n    stats    = SAMTOOLS_STATS.out.stats       // channel: [ val(meta), path(stats) ]\n    flagstat = SAMTOOLS_FLAGSTAT.out.flagstat // channel: [ val(meta), path(flagstat) ]\n    idxstats = SAMTOOLS_IDXSTATS.out.idxstats // channel: [ val(meta), path(idxstats) ]\n\n    versions = ch_versions                    // channel: [ path(versions.yml) ]\n}\nThis subworkflow is comprised of the following modules:\n\nSAMTOOLS_STATS\nSAMTOOLS_IDXSTATS\nSAMTOOLS_FLAGSTAT\n\nThese modules are imported using include, which specifies the name of the process to import, and the path to the module file.\nThe first input to this subworkflow (as indicated by take) is a tuple consisting of three elements – sample metadata, BAM file, BAM index file. The second input is a single fasta file. In this subworkflow, all processes are executed independently. SAMTOOLS_STATS, SAMTOOLS_FLAGSTAT, and SAMTOOLS_IDXSTATS can all be ran in parallel, since no process input is dependant on the output of a process output.\nAlso note that while we have a process and workflow scope definition, there is no subworkflow definition – it is simply defined using workflow.\n\n\n1.2.6. Viewing workflow parameters\nEvery nf-core workflow has a full list of parameters on the nf-core website, in the Parameters tab. When viewing these parameters online, you will also be shown a description and the parameter type. Some parameters will have additional text to help you understand when and how a parameter should be used.\n\nPipeline parameters and their descriptions can also be viewed on the command line using the run command with the --help:\nnextflow run -r 3.18.0 nf-core/rnaseq --help\nIn the example above, the parameters for rnaseq pipeline version 3.14.0 is being viewed.\nExercise: View the parameters for version 2.0 of the nf-core/testpipeline workflow using the command line\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe nf-core/testpipeline workflow parameters for version 2.0can be printed using the nextflow run command along with the --help option:\nnextflow run -r 2.0 nf-core/testpipeline --help\n N E X T F L O W   ~  version 24.10.5\n\nLaunching `https://github.com/nf-core/testpipeline` [angry_aryabhata] DSL2 - revision: 28e764e281 [2.0]\n\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/testpipeline v2.0-g28e764e\n------------------------------------------------------\nTypical pipeline command:\n\n  nextflow run nf-core/testpipeline --input samplesheet.csv --genome GRCh37 -profile docker\n\nInput/output options\n  --input                            [string]  Path to comma-separated file containing information about the samples in the experiment.\n  --outdir                           [string]  The output directory where the results will be saved. You have to use absolute paths to storage on Cloud \n                                               infrastructure. \n  --email                            [string]  Email address for completion summary.\n  --multiqc_title                    [string]  MultiQC report title. Printed as page header, used for filename if not otherwise specified.\n\nReference genome options\n  --genome                           [string]  Name of iGenomes reference.\n  --fasta                            [string]  Path to FASTA genome file.\n\nGeneric options\n  --multiqc_methods_description      [string]  Custom MultiQC yaml file containing HTML including a methods description.\n\n\n\n\n\n1.2.7. Default parameters and configuration fiiles\nMost parameters in a Nextflow pipeline will have a default setting that is defined inside the nextflow.config file in the workflow project directory. By default, most parameters are set to null or false and are only activated by a profile or configuration file.\nLook inside the nextflow.config file for rnaseq. Note that default params are defined inside the params { } scope.\nThere are also several includeConfig statements in the nextflow.config file that are used to load additional .config files from the conf/ folder. For example:\n// Load base.config by default for all pipelines\nincludeConfig 'conf/base.config'\nEach additional .config file contains categorized configuration information for your workflow execution, some of which can be optionally included:\n\nbase.config\n\nIncluded by the workflow by default.\nGenerates resource allocations such as CPU and memory using process labels.\nDoes not specify any method for software management and expects software to be available (or specified elsewhere).\n\nigenomes.config\n\nOptionally included by the workflow.\nDefault configuration to access reference files stored on AWS iGenomes.\n\n\nNotably, the nextflow.config file can also contain the definition of one or more profiles, via the profiles { } scope. A profile is a set of configuration attributes that can be activated when launching a workflow by using the Nextflow -profile command option. For example, to use the apptainer profile, the following command can be used:\nnextflow run nf-core/&lt;pipeline&gt; -profile apptainer\nBy specifying apptainer as the Nextflow profile, the following options will be enabled:\nprofiles {\n    apptainer {\n        apptainer.enabled       = true\n        apptainer.autoMounts    = true\n        conda.enabled           = false\n        docker.enabled          = false\n        singularity.enabled     = false\n        podman.enabled          = false\n        shifter.enabled         = false\n        charliecloud.enabled    = false\n    }\n}\nProfiles used by nf-core workflows include:\n\nSoftware management profiles\n\nProfiles for the management of software using software management tools, e.g., docker, apptainer, singularity, and conda.\n\nTest profiles\n\nProfiles to execute a workflow with a standardized set of test data and parameters, e.g., test and test_full.\n\n\nMultiple profiles can be specified in a comma-separated (,) list when you execute your command. The order of profiles is important as they will be read from left to right:\nnextflow run nf-core/&lt;pipeline&gt; test,apptainer\nThe use of either software containers (Docker, Apptainer, or Singularity), or Conda environments can be activated using Nextflow profiles.\n\n\n\n\n\n\nTip\n\n\n\nIf your computer has internet access and one of Conda, Singularity, Apptainer, or Docker installed, you should be able to run any nf-core workflow with the test profile and the respective software management profile ‘out of the box’. The test data profile will pull small test files directly from the nf-core/test-datasets GitHub repository and run it on your local system. The test profile is an important control to check the workflow is working as expected and is a great way to trial a workflow. Some workflows have multiple test profiles for you to test.\n\n\n\n\n1.2.8. Parameters in the command line\nParameters can be customized using the command line. Any parameter can be configured on the command line by prefixing the parameter name with a double dash (--):\nnextflow run -r 2.0 nf-core/testpipeline -resume -profile test,apptainer --outdir 'output_testpipeline'\nIn the above example, the Nextflow option -resume is used to cache any previously completed processes, allowing the pipeline to be resumed from the last incompleted process. For any previosuly successfully completed processes, they will not need to be re-executed. Another Nextflow option, -profile, is used to specify the test and the apptainer software profile to use when executing the pipeline. Finally, the output directory name is set as output_testpipeline using the workflow parameter --outdir.\n\n\n\n\n\n\nTip\n\n\n\nNextflow options are prefixed with a single dash (-) and workflow parameters are prefixed with a double dash (--).\n\n\nExercise: Use the command line to run the nf-core/testpipeline pipeline with the test and apptainer profile. Set the MultiQC report tile as your favourite animal.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the following command to check what parameter can be used to change the MultiQC title:\nnextflow run -r 2.0 nf-core/testpipeline --help\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAdd the --multiqc_title flag to your command and execute it. Use the -resume option to save time:\nnextflow run -r 2.0 nf-core/testpipeline -resume -profile test,apptainer --outdir 'output_testpipeline' --multiqc_title 'koala'\n\n\n\nYou can check your parameter has been applied by listing the files created in the output folder (output_testpipeline):\nls output_testpipeline/multiqc/\n\n\n1.2.9. Custom configuration files\nConfiguration .config files that contain various workflow properties that can also be specified in the command-line, via the -c option:\nnextflow run nf-core/&lt;pipeline&gt; -c &lt;path/to/custom.config&gt;\nMultiple custom .config files can be included at execution by separating them with a comma (,).\nCustom configuration files follow the same structure as the configuration file included in the workflow directory. Configuration properties are organized into scopes by grouping the properties in the same scope using the curly brackets notation. Scopes allow you to quickly configure settings required to deploy a workflow on different infrastructures, using different software management.\nFor example, the executor scope can be used to provide settings for the deployment of a workflow on a HPC cluster. This example sets the maximum number of parallel tasks to be 100.\nexecutor {\n  queueSize         = 100\n}\nSimilarly, the apptainer scope controls how apptainer containers are executed by Nextflow. In this example, the apptainer container software management has been enabled.\napptainer {\n    enabled = true\n}\nMultiple scopes can be included in the same .config file using a mix of dot prefixes and curly brackets. A full list of scopes is described in detail here.\nExercise: Navigate through the full scopes list and determine which scope can be used to set the MultiQC tile in your configuration file. Then, set the MultiQC tile as your favourite colour, specifying the custom custom.config file in the nextflow run command.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo change set a parameter inside your custom.config file, a params { } scope has to be used.\nNote that you are no longer required to specify the multiqc_title via the command line.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreate a custom custom.config file that contains your favourite colour, e.g., blue:\nparams {\n    multiqc_title = \"blue\"\n}\nInclude the custom .config file in your execution command with the -c option:\nnextflow run -r 2.0 nf-core/testpipeline -resume -profile test,apptainer --outdir 'output_testpipeline' -c custom.config\n\n\n\nCheck if it has been applied:\nls output_testpipeline/multiqc/\n\n\n\n\n\n\nWarning\n\n\n\n\n\nWhy did this fail?\nYou can not use the params scope in custom configuration files. Parameters can only be configured using the -params-file option and the command line. While the parameter is listed as a parameter on the STDOUT, it was not applied to the executed command.\nWe will revisit this at the end of the module\n\n\n\n\n\n1.2.10 Parameter files\nParameter files are used to define the params options for a pipeline, generally written in the YAML format. They are added to a pipeline with the flag -params-file\nExample YAML:\n\"number\": 1\n\"greeting\": \"hello\"\n\"mark_duplicates\": true\nExercise: Since the multiqc_title parameter could not be set using the custom.config file, create a custom_params.yml file that sets multiqc_title to your favourite colour. Then, rerun the pipeline specifying your parameter file.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSet up custom_params.yml:\nmultiqc_title: \"black\"\nnextflow run -r 2.0 nf-core/testpipeline -resume -profile test,apptainer --outdir 'output_testpipeline' -params-file custom_params.yml \n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nNextflow can be used to pull nf-core workflows, or run nf-core workflows.\nnf-core workflows follow similar template structures\nnf-core workflows can be configured using parameters and profiles\n\n\n\n\n\n\nNext Chapter: Customising and running nf-core pipelines\n\n\nThis workshop is adapted from various nextflow training materials, including:\n\nNextflow Training Materials\nCustomising Nf-Core Workshop\nHello Nextflow Workshop"
  },
  {
    "objectID": "workshops/00_setup.html",
    "href": "workshops/00_setup.html",
    "title": "Essential Workshop Preparation",
    "section": "",
    "text": "In this workshop, we will be using an HPC system to run nextflow and nf-core workflows.\nBefore joining the workshop, please complete the following checklist:"
  },
  {
    "objectID": "workshops/00_setup.html#install-and-set-up-visual-studio-code",
    "href": "workshops/00_setup.html#install-and-set-up-visual-studio-code",
    "title": "Essential Workshop Preparation",
    "section": "Install and set up Visual Studio Code",
    "text": "Install and set up Visual Studio Code\nWe recommend Visual Studio Code as a source code editor because it is lightweight and has rich support for extensions and syntax highlighting available across various popular operating system.\nDownload Visual Studio Code on your computer and follow the instructions for your specific Operating System as required:\n\nmacOS\nWindows\nLinux\n\nOnce installed, open VS Code on your computer.\n\n\nInstall the Nextflow Extension\nThe Nextflow extension provides syntax highlighting and quick shortcuts for common code snippets.\nClick on the extensions button (four blocks) on the left side bar. Search for “Nextflow” in the extensions search bar, then click on the blue “Install” button.\n\n\n\nInstall the Remote-SSH Extension\nRemote-SSH allows us to use any remote machine with a SSH server as your development environment. This lets us work directly on the our cluster’s storage.\nClick on the extensions button (four blocks) on the left side bar. Search for “Remote - SSH” in the extensions search bar, then click on the blue “Install” button.\n\n\n\nLogin via Visual Studio Code\nConnect to your instance with VS code by adding the host details to your .ssh config file (if you have not done this previously)\n\nIn a new VS code window, type Ctrl+Shift+P if you’re on a Windows machine or Cmd+Shift+P for MacOS to open the command palette\nSelect Remote-SSH: Open SSH configuration file and select your .ssh config file\nAdd a new entry with your details to login to cluster, and save your .ssh config file:\n\nHost pmac-workshop\n    HostName &lt;your-ip-name&gt;\n    User &lt;your-user-name&gt;\n\nType Ctrl+Shift+P and select Remote-SSH: Connect to Host; and pmac-cluster (or whatever you name your host above)\nWhen prompted, select Linux as the platform of the remote host from the dropdown menu\nType in your password and hit enter\n\nHaving successfully logged in, you should see a small blue or green box in the bottom left corner of your screen:\n\nTo set up your VS Code window for the workshop:\n\nOpen a new folder in the file explorer panel on the left side of the screen by typing Ctrl + K, Ctrl + O if you’re running Windows or Cmd+K+ Cmd + O for MacOS\nSelect /home/&lt;your-user-name&gt;/nfWorkshop to open our working directory. If you encountered an error that the directory does not exist, you would need to ssh in to the cluster and create that directory first before attempting this step.\nWhen prompted, select the box for Trust the authors of all files in the parent folder ‘home’ then click Yes, I trust the authors\nYou can dismiss the warning message saying our cluster’s git version is outdated\nTo open a terminal, type Ctrl+J if you’re on a Windows machine or Cmd+J on MacOS\n\n\nThis setup instruction is adapted from Customising Nf-Core Workshop materials from Sydney Informatics Hub"
  },
  {
    "objectID": "workshops/2.3_tips_and_tricks.html",
    "href": "workshops/2.3_tips_and_tricks.html",
    "title": "Best practise, tips and tricks",
    "section": "",
    "text": "3.3.1. Running Nextflow Pipelines on a HPC \nNextflow, by default, launches mulitple parallel tasks that can be ran concurrently. Recall previously that we ran these tasks locally. We can however, use the process and executor scope to run these tasks using an HPC job scheduler such as SLURM, submitting the desired number of concurrent jobs.\nprocess {\n    executor = 'slurm'\n    queue = 'PARTITION'\n}\n\nexecutor {\n    queueSize = 4\n}\nBy specifying the executor as slurm, Nexflow will submit each process as a separate job using the sbatch command. All jobs will be submittd to the PARTITION partition.\nInside the process { } scope, we can also define resources such as cpus, time, memory, and queue.\nprocess {\n    executor = 'slurm'\n    queue = 'PARTITION'\n\n    cpus = 1\n    time = '2h'\n    memory = '4.GB'\n}\n\nexecutor {\n    queueSize = 4\n}\nNow each individual job will be executed using 1 CPU, 4GB of memory, and a maximum time limit of 2 hours. Since we didn’t specify a process label or a process name, this setting will apply for all processes within the pipeline.\n\nRun processes on different partitions\nPreviously, we used the withLabel and withName process selectors to specify the cpus, time, memory for a group of processes, or a particular process. We can also use those process selectors to change what partition the job will be submitted to.\nFor example, suppose we have one process that requires the use of GPUs. If we change the queue to our GPU partition gpu_partition, this means all process jobs, even ones that don’t require GPU, will be ran on that partition.\nprocess {\n    executor = 'slurm'\n    queue = 'gpu_partition'\n\n    cpus = 1\n    time = '2h'\n    memory = '4.GB'\n}\n\nexecutor {\n    queueSize = 4\n}\nInstead, we can use the withName process selector to send the job execution for that process to a GPU-speicifc partition. This means we won’t unnecessarily use GPU partition resources.\nprocess {\n    executor = 'slurm'\n    queue = 'PARTITION'\n\n    cpus = 1\n    time = '2h'\n    memory = '4.GB'\n\n    withName: 'GPU_PROCESS' {\n      queue = 'gpu_queue'\n    }\n}\n\nexecutor {\n    queueSize = 4\n}\n\n\nSpecify infrastructure-specific directives for your jobs\nAdjusting the custom configuration file above, we can define any native configuration options using the clusterOptions process directive, used to specify resources not already available in Nextflow.\nFor example, if you are running your pipeline on an HPC system that is billed, you have the option to specify what project the resource usage is billed to.\nFor example if you typically submit a job using the following command, where --account is used to specify the project number resource usage is billed to:\nsbatch --acount=PROJECT1 script.sh\nBy default, this account option is not a supported directive in Nextflow. Therefore, we cannot use the following config:\nprocess {\n    executor = 'slurm'\n    queue = 'PARTITION'\n    account = 'PROJECT1\n\n    cpus = 1\n    time = '2h'\n    memory = '4.GB'\n\n    withName: 'GPU_PROCESS' {\n      queue = 'gpu_queue'\n    }\n}\n\nexecutor {\n    queueSize = 4\n}\nInstead, this can be specified using clusterOptions, as below:\nprocess {\n    executor = 'slurm'\n    queue = 'PARTITION'\n    clusterOptions = \"--account=PROJECT1\"\n\n    cpus = 1\n    time = '2h'\n    memory = '4.GB'\n\n    withName: 'GPU_PROCESS' {\n      queue = 'gpu_queue'\n    }\n}\n\nexecutor {\n    queueSize = 4\n}\n\n\n\n\n\n\nCaution\n\n\n\nOn certain HPC systems, you may not be able to submit new jobs from another job (such as an interactive session). In this case, you may get the following error:\nsbatch: error: Batch job submission failed: Access/permission denied\nTo overcome this, use login-node (and exit your interactive session) when running your workflow.\n\n\n\n\n\n3.3.2. Clean your work directory\nYour work directory can get very big, very quickly (especially if you are using full sized datasets). It is good practise to clean your work directory regularly. Rather than removing the work folder with all of it’s contents, the Nextflow clean function allows you to selectively remove data associated with specific runs.\nnextflow clean -help\nClean up project cache and work directories\nUsage: clean [options] \n  Options:\n    -after\n       Clean up runs executed after the specified one\n    -before\n       Clean up runs executed before the specified one\n    -but\n       Clean up all runs except the specified one\n    -n, -dry-run\n       Print names of file to be removed without deleting them\n       Default: false\n    -f, -force\n       Force clean command\n       Default: false\n    -h, -help\n       Print the command usage\n       Default: false\n    -k, -keep-logs\n       Removes only temporary files but retains execution log entries and\n       metadata\n       Default: false\n    -q, -quiet\n       Do not print names of files removed\n       Default: false\nThe -after, -before, and -but options are all very useful to select specific runs to clean. The -dry-run option is also very useful to see which files will be removed if you were to -force the clean command.\nExercise: Use Nextflow to clean your work directory of all staged files, but keep your execution logs.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the Nextflow clean command with the -k and -f options:\nnextflow clean -k -f\n\n\n\n\n\n3.3.3. Change the default Nextflow cache strategy\nSometimes, a wrkflow execution is not resumed as expected. The default behaviour of Nextflow cache keys is to index the input file meta-data information. Reducing the cache stringency to lenient means the file cache keys are based only on filesize and path, and can help to avoid unexpectedly re-running certain processes when -resume is in use.\nTo apply lenient cache strategy to all of your runs, you could add the following to a custom configuration file:\nprocess {\n    cache = 'lenient'\n}\nAgain, you can specify different cache stategies for different processes by using withName or withLabel.\n\n\n3.3.4. Access private GitHub repositories\nTo interact with private repositories on GitHub, you can provide Nextflow with access to GitHub by specifying your GitHub user name and a Personal Access Token in the scm configuration file inside your specified .nextflow/ directory:\nproviders {\n\n  github {\n    user = 'rlupat'\n    password = 'my-personal-access-token'\n  }\n\n}\nReplace 'my-personal-access-token' with your personal access token.\n\n\n3.3.5. Additional resources \nHere are some useful resources to help you get started with running and developing nf-core pipelines:\n\nNextflow tutorials\nnf-core pipeline tutorials\nNextflow patterns\nHPC tips and tricks\nNextflow coding best practice recommendations\nThe Nextflow blog"
  },
  {
    "objectID": "workshops/5.2_nf_core_modules_CP.html",
    "href": "workshops/5.2_nf_core_modules_CP.html",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "",
    "text": "Objectives\n\n\n\n\nDevelop a basic Nextflow workflow with nf-core templates\nTest and set up profiles for a Nextflow workflow\nCreate conditional processes, and conditional scripts within a processs\nRead data of different types into a Nextflow workflow"
  },
  {
    "objectID": "workshops/5.2_nf_core_modules_CP.html#nf-core-modules",
    "href": "workshops/5.2_nf_core_modules_CP.html#nf-core-modules",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1 nf-core modules",
    "text": "5.1 nf-core modules\nPreviously, we used the nf-core pipelines command to create our initial pipeline template. We can now add modules to this template using nf-core modules. This command also contains subcommands, which can be used to manage modules.\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n           \n                                                                                             \n Usage: nf-core modules [OPTIONS] COMMAND [ARGS]...                                          \n                                                                                             \n Commands to manage Nextflow DSL2 modules (tool wrappers).                                   \n                                                                                             \n╭─ For pipelines ───────────────────────────────────────────────────────────────────────────╮\n│ list         List modules in a local pipeline or remote repository.                       │\n│ info         Show developer usage information about a given module.                       │\n│ install      Install DSL2 modules within a pipeline.                                      │\n│ update       Update DSL2 modules within a pipeline.                                       │\n│ remove       Remove a module from a pipeline.                                             │\n│ patch        Create a patch file for minor changes in a module                            │\n╰───────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Developing new modules ──────────────────────────────────────────────────────────────────╮\n│ create         Create a new DSL2 module from the nf-core template.                        │\n│ lint           Lint one or more modules in a directory.                                   │\n│ test           Run nf-test for a module.                                                  │\n│ bump-versions  Bump versions for one or more modules in a clone of the nf-core/modules    │\n│                repo.                                                                      │\n╰───────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Options ─────────────────────────────────────────────────────────────────────────────────╮\n│ --git-remote  -g  TEXT  Remote git repo to fetch files from                               │\n│ --branch      -b  TEXT  Branch of git repository hosting modules.                         │\n│ --no-pull     -N        Do not pull in latest changes to local clone of modules           │\n│                         repository.                                                       │\n│ --help        -h        Show this message and exit.                                       │\n╰───────────────────────────────────────────────────────────────────────────────────────────╯\nTo check what modules have been added into our pipeline run nf-core modules list local within the pipeline folder.\ncd ./nf-core-customrnaseq\n\nnf-core modules list local\nThis command will output each module that is part of the pipeline, the source repository, version, message at last commit, and the date it was last modified.\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n    There is a new version of nf-core/tools available! (3.2.1)\n\n\nINFO     Repository type: pipeline                                                           \nINFO     Modules installed in '.':                                                           \n                                                                                             \n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name ┃ Repository      ┃ Version SHA ┃ Message                        ┃ Date       ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc      │ nf-core/modules │ 0810805     │ use detailed profiles in       │ 2024-12-13 │\n│             │                 │             │ tests/nf-test.config (#7207)   │            │\n│ multiqc     │ nf-core/modules │ f0719ae     │ bump multiqc 1.26 to 1.27      │ 2025-01-27 │\n│             │                 │             │ (#7364)                        │            │\n└─────────────┴─────────────────┴─────────────┴────────────────────────────────┴────────────┘\nTo obtain a list of all available nf-core modules available, nf-core modules list remote can be used.\n\n5.1.1 Installing modules nf-core modules install\nPreviously, we created a simple workflow that indexed the transcriptome file (INDEX), performed quantification (QUANTIFICATION) and FastQC (FASTQC) on the sample FASTQ files. Finally, perfomed MultiQC on both the outputs of QUANTIFICATION and FASTQC.\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [sad_jennings] DSL2 - revision: cfae7ccc0e\nexecutor &gt;  local (7)\n[b5/6bece3] process &gt; INDEX              [100%] 1 of 1 ✔\n[32/46f20b] process &gt; QUANTIFICATION (3) [100%] 3 of 3 ✔\n[44/27aa8d] process &gt; FASTQC (2)         [100%] 3 of 3 ✔\nWe will be recreating these steps, using the nf-core template as a guide.\nRecall that when we created each process, we manually defined the input(s) and their structure, the output(s) and their structure, the process script, and any containers required to execute the process. This can become time consuming, especially when creating pipelines with many processes.\nBefore creating a module ourselves, we should always first check the nf-core modules page to see if the module we are interested in exists.\nFor our INDEX and QUANTIFICATION processes, let’s check if are already modules that use salmon index and salmon quant.\n\n\n\n\n\n\nImportant\n\n\n\nTO DO Change CONTAINER\n\n\nprocess INDEX {\n    container \"&lt;CACHE DIRECTORY&gt;/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\nprocess QUANTIFICATION {\n    container \"&lt;CACHE DIRECTORY&gt;/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\nSimilar to nf-core pipelines, nf-core modules also have documentation that specifies the inputs to the process, the outputs to the process, and how to install the module to our pipeline.\nTo install a module, navigate inside your nf-core pipeline folder. Then, the module can be installed. First, install the SALMON_INDEX module.\ncd ./nf-core-customrnaseq\n\nnf-core modules install salmon/index\nIf the module has been installed successfully, you will get the following message, which also includes how the module can be included in the analysis script workflows/customrnaseq.nf\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n    There is a new version of nf-core/tools available! (3.2.1)\n\n\nINFO     Installing 'salmon/index'                                                           \nINFO     Use the following statement to include this module:                                 \n                                                                                             \n include { SALMON_INDEX } from '../modules/nf-core/salmon/index/main'                        \n                                                                  \nBy default, the example include command provided by running nf-core modules install will provide the relative output path from the analysis workflow file. Therefore, this can be copied directly into workflows/customrnaseq.nf. Recall that if you are importing the module from within a subworkflow file in the subworkflows folder, you will need to change the relative file path to match the new structure.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nIf you are installing the module outside of the nf-core pipeline folder, you may see the below message:\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nWARNING  'repository_type' not defined in .nf-core.yml                                       \n? Is this repository a pipeline or a modules repository? (Use arrow keys)\n » Pipeline\n   Modules repository\nCancel your nf-core modules install command, navigate inside your pipeline folder, then retry.\n\n\n\nNote that this module has been installed inside the modules folder in the pipeline, inside the nf-core subfolder.\nmodules\n└── nf-core\n    ├── fastqc\n    |   ├── ...\n    ├── multiqc\n    |   ├── ...\n    └── salmon\n        └── index\n            ├── environment.yml\n            ├── main.nf\n            ├── meta.yml\n            └── tests\n                ├── main.nf.test\n                └── main.nf.test.snap\nExercise: Using the nf-core modules page, find a module that performs salmon quant. Install it to your pipeline, and check what location it has been added. Then, include the module in your analysis workflow script workflows/customrnaseq.nf.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe nf-core SALMON_QUANT module can perform salmon quant.\nTo install this module, the following command can be ran inside my pipeline folder nf-core-customrnaseq:\nnf-core modules install salmon/quant\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Installing 'salmon/quant'                                                           \nINFO     Use the following statement to include this module:                                 \n                                                                                             \n include { SALMON_QUANT } from '../modules/nf-core/salmon/quant/main'                        \n                                                                       \nThis module has been successfully installed in the modules folder, inside nf-core.\nmodules\n└── nf-core\n    ├── ...\n    └── salmon\n        ├── index\n        │   ├── ...\n        └── quant\n            ├── environment.yml\n            ├── main.nf\n            ├── meta.yml\n            └── tests\n                ├── main.nf.test\n                ├── main.nf.test.snap\n                └── nextflow.config\nSince the example include command provided by running nf-core modules install will provide the relative output path from the analysis workflow file, the following can be copied directly into workflows/customrnaseq.nf.\ninclude { SALMON_QUANT } from '../modules/nf-core/salmon/quant/main'  \n\n\n\nWe can now list all the modules installed in our pipeline:\nnf-core modules list local\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Repository type: pipeline                                                                                                                                                                                       \nINFO     Modules installed in '.':                                                                                                                                                                                       \n                                                                                                                                                                                                                         \n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name  ┃ Repository      ┃ Version SHA ┃ Message                                                                     ┃ Date       ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc       │ nf-core/modules │ 0810805     │ use detailed profiles in tests/nf-test.config (#7207)                       │ 2024-12-13 │\n│ multiqc      │ nf-core/modules │ f0719ae     │ bump multiqc 1.26 to 1.27 (#7364)                                           │ 2025-01-27 │\n│ salmon/index │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative path in antismash/antismashlite tests (#8116) │ 2025-03-26 │\n│ salmon/quant │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative path in antismash/antismashlite tests (#8116) │ 2025-03-26 │\n└──────────────┴─────────────────┴─────────────┴─────────────────────────────────────────────────────────────────────────────┴────────────┘\n\nModule list: modules.json\nThe modules.json contains detailed information of all the modules installed in the pipeline, where it was installed from, and the version of the module.\n\n\n\n\n\n\nmodules.json\n\n\n\n\n\n{\n    \"name\": \"nf-core/customrnaseq\",\n    \"homePage\": \"https://github.com/nf-core/customrnaseq\",\n    \"repos\": {\n        \"https://github.com/nf-core/modules.git\": {\n            \"modules\": {\n                \"nf-core\": {\n                    ...\n\n                    \"salmon/index\": {\n                        \"branch\": \"master\",\n                        \"git_sha\": \"05954dab2ff481bcb999f24455da29a5828af08d\",\n                        \"installed_by\": [\"modules\"]\n                    },\n                    \"salmon/quant\": {\n                        \"branch\": \"master\",\n                        \"git_sha\": \"05954dab2ff481bcb999f24455da29a5828af08d\",\n                        \"installed_by\": [\"modules\"]\n                    }\n                }\n            },\n            \"subworkflows\": {\n                \"nf-core\": {\n                    \"utils_nextflow_pipeline\": {\n                        \"branch\": \"master\",\n                        \"git_sha\": \"c2b22d85f30a706a3073387f30380704fcae013b\",\n                        \"installed_by\": [\"subworkflows\"]\n                    },\n\n                    ...\n\n                    }\n    ...\n}\n\n\n\nWhat if we remove the module folder for salmon/quant?\nrm -r modules/nf-core/salmon/quant\n\nls modules/nf-core/salmon/quant\nls: cannot access 'modules/nf-core/salmon/quant': No such file or directory\nLet’s now rerun the following command:\nnf-core modules list local\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\nINFO     Repository type: pipeline                                                           \nINFO     Reinstalling modules found in 'modules.json' but missing from directory:            \n         'modules/nf-core/salmon/quant'                                                      \nINFO     Modules installed in '.':                                                           \n                                                                                             \n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name  ┃ Repository      ┃ Version SHA ┃ Message                       ┃ Date       ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc       │ nf-core/modules │ 0810805     │ use detailed profiles in      │ 2024-12-13 │\n│              │                 │             │ tests/nf-test.config (#7207)  │            │\n│ multiqc      │ nf-core/modules │ f0719ae     │ bump multiqc 1.26 to 1.27     │ 2025-01-27 │\n│              │                 │             │ (#7364)                       │            │\n│ salmon/index │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative │ 2025-03-26 │\n│              │                 │             │ path in                       │            │\n│              │                 │             │ antismash/antismashlite tests │            │\n│              │                 │             │ (#8116)                       │            │\n│ salmon/quant │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative │ 2025-03-26 │\n│              │                 │             │ path in                       │            │\n│              │                 │             │ antismash/antismashlite tests │            │\n│              │                 │             │ (#8116)                       │            │\n└──────────────┴─────────────────┴─────────────┴───────────────────────────────┴────────────┘\nIn the command output, we see that our deleted module is automatically reinstalled, based on the list in modules.json\nls modules/nf-core/salmon/quant\nenvironment.yml  main.nf  meta.yml  tests\n\n\nInstalling specific module versions\nInside the modules.json file, the git_sha is listed, which corresponds to a specific version of the module. To obtain the SHA for a module, navigate to the module Github page, and click History.\n\nThe full SHA path can then be copied.\n\nIf there is a specific module version we would like to install, this can be done using nf-core modules. Run the following command:\nnf-core modules install -h\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\n                                                                                             \n Usage: nf-core modules install [OPTIONS] &lt;tool&gt; or &lt;tool/subtool&gt;                           \n                                                                                             \n Install DSL2 modules within a pipeline.                                                     \n                                                                                             \n╭─ Options ─────────────────────────────────────────────────────────────────────────────────╮\n│ --dir     -d  PATH          Pipeline directory. [default: current working directory]      │\n│ --prompt  -p                Prompt for the version of the module                          │\n│ --force   -f                Force reinstallation of module if it already exists           │\n│ --sha     -s  &lt;commit sha&gt;  Install module at commit SHA                                  │\n│ --help    -h                Show this message and exit.                                   │\n╰───────────────────────────────────────────────────────────────────────────────────────────╯\nThe SHA version can be specified using the --sha parameter. Since we have already installed the module, the --force parameter will also be required.\nExercise: Install the salmon/quant module, for SHA version 85b5f8a0d9df9ce7587af50e2ee75b37c97515c6. Use either nf-core modules list local or the modules.json file to verify the correct version was installed.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe following command can be used to install salmon/quant, SHA version 85b5f8a0d9df9ce7587af50e2ee75b37c97515c6.\nnf-core modules install --sha 85b5f8a0d9df9ce7587af50e2ee75b37c97515c6 --force salmon/quant\nInside the modules.json, the git_sha has been updated to the correct version.\n\"salmon/quant\": {\n    \"branch\": \"master\",\n    \"git_sha\": \"85b5f8a0d9df9ce7587af50e2ee75b37c97515c6\",\n    \"installed_by\": [\"modules\"]\n}\nUsing nf-core modules list local, the Version SHA has also been updated\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n    There is a new version of nf-core/tools available! (3.2.1)\n\n\nINFO     Repository type: pipeline                                                           \nINFO     Modules installed in '.':                                                           \n                                                                                             \n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name  ┃ Repository      ┃ Version SHA ┃ Message                       ┃ Date       ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc       │ nf-core/modules │ 0810805     │ use detailed profiles in      │ 2024-12-13 │\n│              │                 │             │ tests/nf-test.config (#7207)  │            │\n│ multiqc      │ nf-core/modules │ f0719ae     │ bump multiqc 1.26 to 1.27     │ 2025-01-27 │\n│              │                 │             │ (#7364)                       │            │\n│ salmon/index │ nf-core/modules │ 05954da     │ Delete all tag.yml + relative │ 2025-03-26 │\n│              │                 │             │ path in                       │            │\n│              │                 │             │ antismash/antismashlite tests │            │\n│              │                 │             │ (#8116)                       │            │\n│ salmon/quant │ nf-core/modules │ 85b5f8a     │ Fix language server error in  │ 2025-03-18 │\n│              │                 │             │ salmon/quant (#7843)          │            │\n└──────────────┴─────────────────┴─────────────┴───────────────────────────────┴────────────┘\n\n\n\n\n\n\n5.1.2 Updating modules: nf-core modules update\nA limitation of nf-core modules install is that it doesn’t explicitly print what has been changed within the module. The nf-core modules update command is an alternative that can do this.\nLet’s update salmon/quant\nnf-core modules update salmon/quant\nThe command will then prompt you if you wish to view the differences between the current installation and the update.\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n    There is a new version of nf-core/tools available! (3.2.1)\n\n\n? Do you want to view diffs of the proposed changes? (Use arrow keys)\n » No previews, just update everything\n   Preview diff in terminal, choose whether to update files\n   Just write diffs to a patch file\nUse the up/down arrow keys can to choose Preview diff in terminal, choose whether to update files and press Enter.\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\ \n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 3.2.0 - https://nf-co.re\n\n\n? Do you want to view diffs of the proposed changes? Preview diff in terminal, choose whether to update files\nINFO     Changes in component 'nf-core/salmon/quant' between (85b5f8a0d9df9ce7587af50e2ee75b37c97515c6) and (05954dab2ff481bcb999f24455da29a5828af08d)                                                                   \nINFO     'modules/nf-core/salmon/quant/environment.yml' is unchanged                                                                                                                                                     \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/meta.yml' is unchanged                                                                                                                                                            \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/main.nf' is unchanged                                                                                                                                                             \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/tests/nextflow.config' is unchanged                                                                                                                                               \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/tests/main.nf.test' is unchanged                                                                                                                                                  \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/tests/main.nf.test.snap' is unchanged                                                                                                                                             \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\nINFO     'modules/nf-core/salmon/quant/tests/tags.yml' was removed                                                                                                                                                       \n┏━ salmon/quant ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n? Update module 'salmon/quant'? (y/N)\nWe see that the only change between the two module versions is that tags.yml was removed. Since these changes are acceptable to us, we can now press y. Your module should now be updated.\nINFO     Updating 'nf-core/salmon/quant'                                                                                                                                                                                 \nINFO     Updates complete ✨   \n\n\n5.1.3 Removing modules\nPreviously, we saw that by simply removing the module folder didn’t delete the module from the pipeline. The pipeline automatically searches for missing module files and redownloads them, based on the SHA version specified in modules.json.\nTo remove a module from a pipeline, the following can be used:\nFor nf-core to no longer register the module is to be distributed with your pipeline you need to use:\nnf-core modules remove\nAs an exercise, we are going to install the samtools/sort module\nnf-core modules install samtools/sort\nQuickly view the modules.json or use nf-core modules list local to view the changes from installing the module.\nNow remove the samtools/sort module\nnf-core modules remove samtools/sort\n\n\n\n\n\n\nOverall Challenge\n\n\n\nNow add the include module statements to the our workflows/myrnaseq.nf\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\ninclude { FASTQC as FASTQC_one } from '../modules/nf-core/fastq/main' \ninclude { FASTQC as FASTQC_two } from '../modules/nf-core/fastq/main' \n\ninclude { TRIMGALORE } from '../modules/nf-core/trimgalore/main'\n\n\n\n\n\n5.1.4 Writing modules with nf-core template\nFor this section we are going to refer to the nf-core guidelines for modules.\nWhile these are the full guidelines for contributing back to nf-core, there are still some general components that are good practice even if you are NOT planning to contribute.\n\n\n\n\n\n\nSummary of guidelines\n\n\n\n\nAll required and optional input files must be included in the input as a path variable\nThe command should run without any additional argument, any required flag values should be included as an input val variable\ntask.ext.args must be provided as a variable\nWhere possible all input and output files should be compressed (i.e. fastq.gz and .bam)\nA versions.yml file is output\nNaming conventions include using all lowercase without puntuation and follows the convention of software/tool (i.e. bwa/mem)\nAll outputs must include an emit definition\n\n\n\nWe are going to write up our own samtools/view module.\nnf-core modules create \n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\n\nINFO     Repository type: pipeline\nINFO     Press enter to use default values (shown in brackets) or type your own responses. ctrl+click underlined text to open links.\nName of tool/subtool: samtools/view\nINFO     Using Bioconda package: 'bioconda::samtools=1.20'\nINFO     Could not find a Docker/Singularity container (Unexpected response code `500` for https://api.biocontainers.pro/ga4gh/trs/v2/tools/samtools/versions/samtools-1.20) ## Cluster\nGitHub Username: (@author): @mmyeung\nINFO     Provide an appropriate resource label for the process, taken from the nf-core pipeline template.\n         For example: process_single, process_low, process_medium, process_high, process_long\n? Process resource label: process_low\nINFO     Where applicable all sample-specific information e.g. 'id', 'single_end', 'read_group' MUST be provided as an input via a Groovy Map called\n         'meta'. This information may not be required in some instances, for example indexing reference genome files.\nWill the module require a meta map of sample information? [y/n] (y): y\nINFO     Created component template: 'samtools/view'\nINFO     Created following files:\n           modules/local/samtools/view.nf\nAs we progressed through the interactive prompt, you will have noticed that nf-core always attempts to locate the corresponding bioconda package and singularity/Docker container.\n\n\n\n\n\n\nWhat happens when there is no bioconda package or container?\n\n\n\n\n\nnf-core modules create --author @mmyeung --label process_single --meta testscript\nThe command will indicate that the there is no bioconda package with the software name, and prompt you for a package name you might wish to use.\nINFO     Repository type: pipeline\nINFO     Press enter to use default values (shown in brackets) or type your own responses. ctrl+click underlined text to open links.\nWARNING  Could not find Conda dependency using the Anaconda API: 'testscript'\nDo you want to enter a different Bioconda package name? [y/n]: n\nWARNING  Could not find Conda dependency using the Anaconda API: 'testscript'\n         Building module without tool software and meta, you will need to enter this information manually.\nINFO     Created component template: 'testscript'\nINFO     Created following files:\n           modules/local/testscript.nf      \nwithin the module .nf script you will note that the definitions for the conda and container are incomplete for the tool.\n    conda \"${moduleDir}/environment.yml\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/YOUR-TOOL-HERE':\n        'biocontainers/YOUR-TOOL-HERE' }\"\nnf-core has a large cache of containers here. Though you can also provide a simple path to docker hub.\n    container \"mmyeung/trccustomunix:0.0.1\"\n\n\n\nThe resource labels, are those as defined in conf/base.config\n\n\n\n\n\n\nChallenge\n\n\n\nWrite up the inputs, outputs and script for samtools/view.\nAssume that all the inputs will be .bam and the outputs will also be .bam.\nFor reference look at the documentation for samtools/view\nAre there optional flags that take file inputs? What options need to set to ensure that the command runs without error?\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda \"${moduleDir}/environment.yml\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.20--h50ea8bc_0' :\n        'biocontainers/samtools:1.20--h50ea8bc_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(index)\n    tuple val(meta2), path(fasta)\n    path bed\n    path qname\n\n    output:\n    tuple val(meta), path(\"*.bam\"),  emit: bam\n    path  \"versions.yml\",            emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    def readnames = qname ? \"--qname-file ${qname}\": \"\"\n    def regions = bed ? \"-L ${bed}\": \"\"\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        view \\\\\n        -hb \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${readnames} \\\\\n        ${regions} \\\\\n        $args \\\\\n        -o ${prefix}.bam \\\\\n        $input \\\\\n        $args2\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2&gt;&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = args.contains(\"--output-fmt sam\") ? \"sam\" :\n                    args.contains(\"--output-fmt bam\") ? \"bam\" :\n                    args.contains(\"--output-fmt cram\") ? \"cram\" :\n                    input.getExtension()\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n\n    def index = args.contains(\"--write-index\") ? \"touch ${prefix}.csi\" : \"\"\n\n    \"\"\"\n    touch ${prefix}.${file_type}\n    ${index}\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2&gt;&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n\n\nSimilar to nf-core create you can minimise a the number of prompts by using optional flags.\n\n\n\n\n\n\nOverall Challenge\n\n\n\nWrite up the short workflow as discussed above\nFASTQC -&gt; trimgalore -&gt; FASTQC -&gt; MULTIQC"
  },
  {
    "objectID": "workshops/5.2_nf_core_modules_CP.html#nf-core-subworkflow",
    "href": "workshops/5.2_nf_core_modules_CP.html#nf-core-subworkflow",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.4 Nf-core subworkflow",
    "text": "5.4 Nf-core subworkflow\nnf-core subworkflows\nor with\nnf-core subworkflows list remote\n\n5.4.1 Installing nf-core subworkflows\nSubworkflows can be updated/removed like modules\n\n\n\n\n\n\nChallenge\n\n\n\nInstall the subworkflow fastq_subsample_fq_salmon into the workflow\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nnf-core subworkflows install fastq_subsample_fq_salmon\n\n\n\n\n\n5.4.2 Writing subworkflows with nf-core template\n\n\n\n\n\n\nChallenge\n\n\n\nWrite up the QC_WF subworkflow from last week."
  },
  {
    "objectID": "workshops/5.2_nf_core_modules_CP.html#nf-core-schema-and-input-validation",
    "href": "workshops/5.2_nf_core_modules_CP.html#nf-core-schema-and-input-validation",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1 Nf-core schema and input validation",
    "text": "5.1 Nf-core schema and input validation\nRelies on plugins written by nf-core community\nIn particular nf-validation\nnextflow_schmea.json is for pipeline parameters\nnf-core schema build\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\n\nINFO     [✓] Default parameters match schema validation\nINFO     [✓] Pipeline schema looks valid (found 32 params)\nINFO     Writing schema with 32 params: 'nextflow_schema.json'\n🚀  Launch web builder for customisation and editing? [y/n]: y\nINFO     Opening URL: https://nf-co.re/pipeline_schema_builder?id=1718112529_0841fa08f86f\nINFO     Waiting for form to be completed in the browser. Remember to click Finished when you're done.\n⢿ Use ctrl+c to stop waiting and force exit.\nRecommend writing in web browser\njson format details additional reading\n\n\n\n\n\n\nChallenge\n\n\n\nWe are going add the input parameter for the transcript.fa\nThen install salmon/index and write up quant_wf subworkflow from last week.git\n\n\n\n5.1.2 Nf-core inputs\nnested in this schema is the input or samplesheet schema. unfortunately there isn’t a nice interface to help you write this schema yet.\n\nmeta: Allows you to predesignate the “key” with in the “meta”\nrequired: value must be included\ndependency: value is dependant on other value existing in samplesheet (i.e. fastq_2 must imply there is a fastq_1)\n\n\n\n5.6 Nf-core tools for launching\ncreate-params-file\n\n\n5.7 Nf-core for pipeline management\nbump-version ==&gt; good software management to note down versions\nAs you take look through the files created you will see many comments through the files starting with // TODO nf-core. These are pointers from nf-core towards areas of the pipeline that you may be intersted in changing.\nThey are also the “key word” used by the nf-core lint."
  },
  {
    "objectID": "workshops/5.2_nf_core_modules_CP.html#test-profile",
    "href": "workshops/5.2_nf_core_modules_CP.html#test-profile",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1 Test Profile",
    "text": "5.1 Test Profile\nnf-core tries to encourage software engineering concepts such as minimal test sets, this can be set up using the conf/test.config and conf/test_full.config\nFor the duration of this workshop we will be making use of the conf/test.config, to test our pipeline.\nLet’s take a look at what is currently in the conf/test.config.\ncat pmcc-myrnaseq/conf/test.config\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Nextflow config file for running minimal tests\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Defines input files and everything required to run a fast and simple pipeline test.\n\n    Use as follows:\n        nextflow run pmcc/myrnaseq -profile test,&lt;docker/singularity&gt; --outdir &lt;OUTDIR&gt;\n\n----------------------------------------------------------------------------------------\n*/\n\nparams {\n    config_profile_name        = 'Test profile'\n    config_profile_description = 'Minimal test dataset to check pipeline function'\n\n    // Limit resources so that this can run on GitHub Actions\n    max_cpus   = 2\n    max_memory = '6.GB'\n    max_time   = '6.h'\n\n    // Input data\n    // TODO nf-core: Specify the paths to your test data on nf-core/test-datasets\n    // TODO nf-core: Give any required params for the test so that command line flags are not needed\n    input  = params.pipelines_testdata_base_path + 'viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv'\n\n    // Genome references\n    genome = 'R64-1-1'\n}\nFrom this, we can see that this config uses the params scope to define:\n\nMaximal values for resources\nDirects the input parameter to a sample sheet hosted in the nf-core/testdata github\nSets the genome to “R64-1-1”\n\n\n\n\n\n\n\nHow does setting the parameter genome set all the genome references?\n\n\n\nThis is possible due to us using the igenomes configs from nf-core.\nYou can see in the conf/igenomes.config how nested within each genome definition are paths to various reference files.\nTo find out more about the igenomes project here\n\n\nFor the duration of this workshop we are going to use the data from nf-training that was cloned in the first workshop. We are also going to update our test.config to contain the igenomes_base parameter, as we have a local cache on the cluster.\ninput = \"/home/Shared/For_NF_Workshop/training/nf-training/data/ggal/samplesheet.csv\"\noutdir = \"/scratch/users/${USER}/myrnaseqtest\"\n\n// genome references\ngenome = \"GRCh38\"\nigenomes_base = \"/data/janis/nextflow/references/genomes/ngi-igenomes\"\nAlso, we will need to change the value, custom_config_base to null, in nextflow.config\ncustom_config_base         = null\nLet’s quickly check that our pipeline runs with the test profile.\ncd ..\nnextflow run ./pmcc-myrnaseq -profile test,singularity\n\n\n\n\n\n\nWhat’s the difference between the test.config and the test_full.config\n\n\n\nTypically the test.config contains the minimal test example, while the test_full.config contains at least one full sized example data."
  },
  {
    "objectID": "sessions/1_intro_run_nf.html",
    "href": "sessions/1_intro_run_nf.html",
    "title": "Introduction to Nextflow and running nf-core workflows",
    "section": "",
    "text": "This workshop is designed to provide participants with a foundational understanding of Nextflow and nf-core pipelines, with a focus on running existing pipelines efficiently. Participants are expected to have prior experience with the command-line interface and working with cluster systems like Slurm. The primary goal of the workshop is to equip researchers with the skills needed to use nf-core pipelines for their research data.\n\nCourse Presenters\n\nSong Li, Bioinformatics Core Facility, Peter Mac\nRichard Lupat, Bioinformatics Core Facility, Peter Mac\n\n\n\nCourse Helpers\n\nEmma Gail, Melbourne Bioinformatics\nSanduni Rajapaksa, Research Computing Facility, Peter Mac\nAdam Taranto, WEHI\n\n\n\nPrerequisites\n\nExperience with command line interface and cluster/slurm\nFamiliarity with the basic concept of workflows\nAccess to a slurm cluster\n\n\n\nLearning Objectives:\nBy the end of this workshop, participants should be able to:\n\nGain exposure to key concepts and terminology in Nextflow and nf-core pipelines.\nUnderstand the foundational knowledge required to navigate and customize the code base of nf-core pipelines.\nDevelop basic troubleshooting and customization skills necessary for responsibly applying nf-core pipelines to your own research data\nDevelop basic nextflow workflow\n\n\n\nSet up requirements\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP.\n\n\nWorkshop schedule\n\n\n\nLesson\nOverview\nTime\n\n\n\n\nSetup\nFollow these instructions to install VS Code and setup your workspace\nPrior to workshop\n\n\nSession kick off\nSession kick off: Discuss learning outcomes and finalising workspace setup\n10:00 - 10.10\n\n\nIntroduction to Nextflow\nIntroduction to Nextflow: Introduce nextflow’s core features and concepts; including CLI and how to run it on cluster\n10:10 - 10:25\n\n\nIntroduction to nf-core\nKey nf-core features and concepts, structures, tools, and example nf-core pipelines\n10:25 - 10:50\n\n\nBreak\nBreak\n10:50 - 11:00\n\n\nCustomising and running nf-core pipelines\nCustomising & running nf-core pipelines: Discuss pipelines’ required inputs, optional inputs, outputs, parameters file and configurations files\n11:00 - 11:30\n\n\nTroubleshooting a Nextflow run\nTroubleshooting nextflow run: Discuss Nextflow logging, caching, task execution directory, dependencies, and manual troubleshooting\n11:30 - 12:00\n\n\nLunch Break\nBreak\n12:00 - 12:45\n\n\nIntroduction to processes and channels\nIntroduction to Nextflow channels types and process structure\n12:45 - 13:00\n\n\nCreating a basic Nextflow workflow\nIntroduction to nextflow channels, processes, data types and workflows\n13.00 - 13.45\n\n\nBest practise and Q&A\nBest practise, tips & tricks for running nextflow pipelines\n13.45 - 14:00\n\n\n\n\n\nCredits and acknowledgement\nThis workshop is adapted from various nextflow training materials, including:\n\nNextflow Training Materials\nCustomising Nf-Core Workshop\nHello Nextflow Workshop"
  }
]